[
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/frequency/",
	"title": "Häufigkeiten",
	"tags": [],
	"description": "",
	"content": "Kategoriale (nominale und ordinale) Variablen werden in Häufigkeitstabellen zusammengefasst. Dabei wird für jede Ausprägung die Anzahl der Beobachtungen gezählt:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Age Range\u0026#39;].value_counts()    Mit der Funktion value_counts() können Sie sich absolute Häufigkeitstabellen ausgeben lassen. Mit dem zusätzlichen Argumentaufruf normalize=True werden relative Häufigkeiten berechnet:\ndf[\u0026#39;Age Range\u0026#39;].value_counts(normalize=True)    Der Modus ist dabei die Merkmalsausprägung, die die meisten Beobachtungen besitzen: age_mode = df[\u0026#39;Age Range\u0026#39;].mode() age_mode[0]     Erstellen Sie eine Häufigkeitsverteilung für die Variable 'Year Patron Registered'. Wie viel Prozent der Kunden wurden 2013 im System registriert? Wie viele in den kommenden Jahren? Was fällt Ihnen auf? Wie viel Prozent der Kunden sind zwischen 25 und 34 Jahren? Ersetzen Sie die fehlenden Werte in der Spalte Age Range durch den Modus dieser Spalte. Denken Sie, es handelt sich dabei um eine gute Methode, fehlende Werte zu ersetzen? Welche anderen Strategien fallen Ihnen ein?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/solutions/",
	"title": "Lösungen zu den Kursaufgaben",
	"tags": [],
	"description": "",
	"content": "Im Laufe des Kurses werden hier die Lösungen zu den einzelnen Aufgaben hochgeladen. Die Projektaufgaben werden am Präsenztag gesammelt besprochen.\nKursorganisation und Vorbereitung Quiz  Strg+Enter siehe hier: https://docs.anaconda.com/anaconda/packages/pkg-docs/ 423448, len(df) siehe (unter sns.set()): https://seaborn.pydata.org/introduction.html  Jan gilt.), diskret, `object` - `temp`: metrisch, stetig, `int` - `below_zero`: nominal, diskret, `boolean` #### Datenrundreise   Lösungen   solutions_datenrundreise.ipynb  (35 ko)    #### Exkurs: Einlesen von Daten - In Linux kann z.B. mit dem `free` Kommando der freie Speicherplatz ermittelt werden. Bei 16 Gigabyte Arbeitsspeicher werden ca. 3 Gigabyte vom System verbraucht. - 12 Gigabyte sind 12 000 000 000 Bytes. Somit können theoretisch 1 500 000 000 Zahlen vom Typ `int64` eingelesen werden. - Eine Tabelle mit 100 Variablen kann somit 120 Millionen Beobachtungen enthalten. - Der Library Datensatz verbraucht ca. 220 Megabyte im Arbeitsspeicher (`df.info(memory_usage='deep')`). #### Spalten und Zeilen   Lösungen   solutions_selection.ipynb  (6 ko)    #### Fehlende Werte - `None` ist ein spezieller Datentyp in Python der fehlende Objekte oder Variablen bezeichnet. - `5` ist eine ganzzahlige Zahl vom Typ `int` - `True` ist eine binäre Zahl vom Typ `boolean` - Die entsprechenden Ausdrücke in `\"\"` repräsentieren jeweils einen **Text** vom Typ `str` (oder `object` in pandas)   Weitere Lösungen   solutions_na.ipynb  (4 ko)    #### Quiz - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html - `len(df)` oder `df.shape[0]` - `df['Age Range'].isna().sum()` - `len(df[(df['Age Range'] == '60 to 64 years') \u0026 (df['Circulation Active Year'] == '2016')])` ### Deskriptive Statistik und Visualisierungen #### Häufigkeiten   Lösungen    #### Lagemaße Der Median ist robust gegenüber Ausreißern, da er nicht auf den absoluten sondern nur auf der relativen Reihung der Beobachtungen basiert. Wird beispielsweise der größte Wert einer Messreihe um den Faktor 1000 tausend vergrößert, so ändert sicht der Median nicht. Der Mittelwert hingegen basiert auf den absoluten Werten. Da die Variable `Total Checkouts` einige wenige sehr große Ausreißer enthält, ist der Mittelwert hier viel größer. #### Streuungsmaße   Lösungen    #### Symmetrie und Schiefe Von oben links nach unten rechts: - Bimodal, Symmetrisch - Unimodal, Linksschief/ Rechtssteil - Unimodal, Linksschief/ Rechtssteil - Unimodal, Rechtsschief/ Linkssteil - Kein Modus, Symmetrisch, Gleichverteilung - Unimodal, Linksschief/ Rechtssteil - Unimodal, Rechtsschief/ Linkssteil - Unimodal, Rechtsschief/ Linkssteil - Unimodal, Symmetrisch #### Symmetrie: Fallstudie   Lösungen   solutions_case_study.ipynb  (49 ko)    #### Korrelation: Anscombe-Quartett   Lösungen   solutions_anscombe.ipynb  (37 ko)    #### Fox News Die Balkendiagramme beginnen nicht im Nullpunkt. Somit werden die relativen Unterschiede viel größer dargestellt, als sie in Wahrheit sind. #### Quiz - rechtsschief - `df['Provided Email Address'][df['Age Range'] == '0 to 9 years'].sum()` - `df['Total Checkouts'].quantile(0.60)` - `df['Age Range'].mode()` - `df['Total Renewals'].quantile([0.25, 0.75]).diff()` ### Inferenzstatistik   Lösungen   solutions_inference.ipynb  (12 ko)      Alle Lösungen   solutions_anscombe.ipynb  (37 ko)   solutions_case_study.ipynb  (49 ko)   solutions_datenrundreise.ipynb  (35 ko)   solutions_inference.ipynb  (12 ko)   solutions_na.ipynb  (4 ko)   solutions_selection.ipynb  (6 ko)    -- "
},
{
	"uri": "https://bonartm.github.io/data-librarian/",
	"title": "Modul 3",
	"tags": [],
	"description": "",
	"content": "Daten analysieren und verstehen Herzlichen Willkommen zum dritten Modul Daten analysieren und darstellen des Data Librarian Zertifikationskurs. In diesem Modul möchten wir Ihnen einen praktischen Einblick in die Datenanalyse mit der Programmiersprache Python geben.\nNachdem Sie im ersten Modul schon die grundlegenden Werkzeuge und Programmiertechniken kennen gelernt haben, werden Sie sich in den kommenden Wochen anhand von praktischen Beispielen und Aufgaben Grundlagen der deskriptiven Statistik, der Datenvisualisierung und des Maschinellen Lernens aneignen. Dabei können Sie Ihre Programmier- und Datenanalysekenntnisse in Python verbessern und im bibliothekarischen Kontext anwenden.\n Die Kurseinheiten bauen aufeinander auf. Wir empfehlen Ihnen deswegen durch die Inhalte dieses Moduls mit den Pfeiltasten zu navigieren. In der linken Navigationsleiste wird Ihr Fortschritt gespeichert. Diese Webseite ist für alle Endgeräte optimiert. Sie können deswegen auch Ihr Smartphone oder Tablet zum Lesen nutzen. Starten Sie nun mit der ersten Kurseinheit indem Sie auf   klicken oder mit den Pfeiltasten  navigieren.    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/numpy/",
	"title": "numpy",
	"tags": [],
	"description": "Effizientes Handling und Bearbeitung von numerischen Arrays.",
	"content": "numpy bietet den array als zentrale Datenstruktur. Mit ihm lassen sich numerische Daten effizient im Arbeitsspeicher (RAM) erstellen, ein- und auslesen, bearbeiten und aggregieren.\nNumpy bietet neben dem array viele Funktionen an, mit denen sich effizient Berechnungen auf diesen durchführen lassen können. Außerdem wird die klassische Matrizenrechnung unterstützt.\n# import the library and give it a shorter name \u0026#39;np\u0026#39; import numpy as np # create 100 randomly distributed numbers X = np.normal.random(size=100) # transform X into a 2-dimensional array of size 20x5 X.reshape((20, 5)) # calculate the matrix dot product: X*X\u0026#39;, where X\u0026#39; is the transformation of X X.dot(X.T)    Beispielsweise kann ein Bild als dreidimensionales numpy array dargestellt werden: Die ersten zwei Dimensionen beschreiben die Farbintensität der Pixel auf einer zweidimensionalen Fläche. Die dritte Dimension speichert die jeweiligen Pixelwerte für die Farbkanäle rot, grün und blau.\n  https://www.oreilly.com/library/view/elegant-scipy/9781491922927/ch01.html\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/series/",
	"title": "Series und DataFrames",
	"tags": [],
	"description": "",
	"content": "Series und DataFrames sind die zentralen Datenstrukturen in Pandas. Series sind wie standardmäßige Listen in Python, mit dem wichtigen Unterschied, dass Series nur Werte eines einzelnen Datentyps enthalten können.\nimport pandas as pd x = pd.Series([34, 12, 23, 45]) print(x) x.dtype    Ein Datentyp ist die grundlegende Einheit, in der einzelne Werte in einer Programmiersprache vom Computer gespeichert und verarbeitet werden können. Beispiele für Datentypen in pandas sind: float für Gleitkommazahlen, int für Ganzzahlen, bool für binäre True, False Werte oder datetime für Datumswerte. Text wird im pandas-spezifischen Format object abgespeichert. Für einen DataFrame der beispielsweise in einer Variable mit dem Namen df gepeichert ist, können Sie sich die Datentypen jeder Spalte mit df.dtypes ausgeben lassen.\n  Ein DataFrame fasst mehrere Series gleicher Länge zu einer Datentabelle mit Zeilen (Beobachtungen), Spalten (Variablen) und Spaltennamen (Variablennamen) zusammen.\nDataFrames können aus Daten in Form von Python Dictionaries konstruiert werden:\nimport pandas as pd data = {\u0026#39;month\u0026#39;: [\u0026#39;Jan\u0026#39;, \u0026#39;Feb\u0026#39;, \u0026#39;Mar\u0026#39;], \u0026#39;temp\u0026#39;: [-5, 2, 3], \u0026#39;below_zero\u0026#39;: [True, False, False]} df = pd.DataFrame(data) print(df)    Skalenniveau und Datentypen (10 Min)  Welches Skalenniveau besitzen die Variablen im obigen Beispiel? Sind die Variablen stetig oder diskret? Was ist der Datentyp jeder einzelnen Spalte?    In der praktischen Datenanalyse werden Sie nur selten DataFrames oder Series manuell erstellen, sondern im Computer abgespeicherte Datentabellen aus anderen Formaten, wie Excel oder .csv einlesen.\n"
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/ml/statistic/",
	"title": "Statistik",
	"tags": [],
	"description": "",
	"content": " Justus Perthes (1838): Rhein, Elbe und Oder   Statistik ist die traditionelle Wissenschaft von der Erhebung und Analyse von Daten. Sie verfügt über eine großes theoretisches und mathematisches Fundament und lässt sich in die Teilgebiete deskriptive (Beschreiben), explorative (Suchen) und schließende (Induktion) Statistik unterteilen.\nLesen Sie mehr über die Grundlagen der Statistik im Kapitel Grundbegriffe der Statistik.\n"
},
{
	"uri": "https://bonartm.github.io/data-librarian/inference/overview/",
	"title": "Statistische Inferenz",
	"tags": [],
	"description": "",
	"content": "Bisher haben Sie vorliegende Daten einer Stichprobe mit Visualisierungen und Statistiken beschrieben und zusammengefasst. Von Interesse sind aber in der Regel, die Zusammenhänge und Statistiken in der Gesamtpopulation.\nBeispiel Wahlumfrage: Sie ziehen zufällig $n=100$ Personen aus dem Wahlregister und befragen Sie nach ihren Parteipräferenzen. Sie können dann beispielsweise den relativen Anteil der Personen in Ihrer Stichprobe, die eine bestimmte Partei favorisieren, bestimmen. Damit haben Sie einen Schätzwert für den tatsächlichen Wert, wenn Sie alle Personen des Wahlregisters befragt hätten.\nZiehen Sie eine weitere Stichprobe, so werden die neuen Schätzwerte nicht genau mit denen aus der vorherigen Stichprobe übereinstimmen. Ziehen Sie noch eine Stichprobe, so wird auch hier der Mittelwert wieder geringfügig anders sein. Wollen Sie deswegen eine Aussage über die tatsächlichen Anteile in der Gesamtpopulation treffen, so ist diese Aussage immer mit Unsicherheit behaftet.\nDer Mittelwert/ relative Anteil ändert sich mit jeder Stichprobe, die Sie ziehen. Damit können die auf einer Stichprobe berechneten Schätzerte als statistische Variablen betrachtet werden. Wie für andere Variablen auch, kann diese Stichprobenverteilung deskriptiv beschrieben werden.\nIn der Realität wird aber in der Regel nur eine einzige Stichprobe der Größe $n$ gezogen. Wie können Sie von Mittelwerten einer einzelnen Stichprobe auf den \u0026ldquo;wahren\u0026rdquo; zugrundeliegenden Wert in der Gesamtpopulation schließen? Wie können Sie die Unsicherheiten, die dabei auftreten quantifizieren? Mit diesen Fragen beschäftigt sich die Inferenzstatistik!\n  Der Stichprobenfehler gibt an, wie stark ein Schätzwert (z.B. Relativer Anteil Personen mit Präferenz für Partei A) von Stichprobe zu Stichprobe schwankt. Damit wird also die Varianz eines Schätzers angegeben. Für viele Statistiken, wie das arithmetische Mittel, kann dessen Verteilung theoretisch hergeleitet werden.\nBesitzt beispielsweise die Stichprobenverteilung für den Mittelwert eine hohe Varianz (d.h. mit jeder weiteren Stichprobe würden die berechneten Mittelwerte stark schwanken), dann lässt sich der tatsächliche Mittelwert in der Population nur schlecht eingrenzen. In der Regeln verringert sich die Varianz eines Schätzers je größer die Stichprobe ist.\n    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/io/",
	"title": "Ein- und Ausgabe",
	"tags": [],
	"description": "",
	"content": "Die Funktionen zur Ein- und Ausgabe von Daten in pandas sind umfangreich aber systematisch organisiert. Um beispielsweise eine .csv Datei einzulesen und in einer Variable zu speichern verwenden Sie die Funktion read_csv:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df.head()    Um einen eingelesenen Datensatz beispielsweise im .json Textformat zu speichern verwenden Sie die Funktion to_json:\ndf.to_json(\u0026#34;../data/Library_Usage.json\u0026#34;)     Manche Funktion aus dem pandas Paket sind statische Funktionen: Sie sind an kein konkretes Objekt wie ein DataFrame gebunden, sondern werden über den Bibliotheksnamen pd aufgerufen. Beispiele: pd.read_csv, pd.to_numeric, pd.crosstab. Andere Funktionen sind an ein bestimmtes Objekt, welches mit einer Variable referenziert wird, gebunden. In der Regel ist dies ein DataFrame oder eine Series. Beispiele: df.to_csv, df.corr, df.head, x.mean. Machen Sie sich mit dem Unterschied vertraut: Was bedeuten pd und df und x in den Beispielen?    Exkurs: Datenrundreise (20 Min)  Informieren Sie sich hier über die verschiedenen Funktionen zur Ein- und Ausgabe. Lesen Sie den Datensatz \u0026quot;../data/Library_Usage_Small.csv\u0026quot; ein (Download hier). Er enthält nur die ersten 10 Zeilen des originalen Datensatzes (aus Performancegründen). Speichern sie den DataFrame als .json ab. Lesen Sie die .json ein und speichern Sie den DataFrame als .html Tabelle ab (Die .html Datei lässt sich auch mit einem Browser öffnen). Lesen Sie dann die .html Datei ein (Achtung: read_html gibt eine Liste von DataFrames zurück!) und speichern Sie den DataFrame als .xlsx Datei ab (Die .xlsx Datei lässt sich auch mit Excel öffnen). Lesen Sie nun die .xlsx Datei ein und speichern Sie den DataFrame wieder als .csv ab. Achten Sie darauf, den ursprünglichen originalen Datensatz nicht zu überschreiben. Vergleichen Sie die originale .csv Version mit der Version, nach der Datenrundreise. Ist alles gleich geblieben?    Exkurs: Einlesen von Daten Die Festplatte des Computers dient zur persistenten Speicherung von Dateien. Auch wenn der Strom weg ist, bleiben diese darauf erhalten. Die hohe Speicherfähigkeit hat ihren Preis: Die Zugriffszeiten, d.h. die Zeit die die Festplatte benötigt um z.B. Zeilen einer Textdatei zu lesen und die Werte an den Prozessor zu übergeben, sind hoch.\nDeswegen gibt es neben dem Festplattenspeicher auch noch den Arbeitsspeicher (RAM). Dessen Zugriffszeiten sind wesentlich schneller, die Daten sind jedoch nicht persistent. Wenn Sie z.B. eine Tabelle mit Excel öffnen, dann werden die Daten von der Festplatte in den Arbeitsspeicher geladen. Das gleiche, nur ohne graphische Oberfläche, passiert, wenn Sie Daten mit dem pandas Paket einlesen.\nDa normalerweise der Datensatz komplett in den Arbeitsspeicher geladen werden muss, können prinzipiell nicht beliebig große Datenmengen bearbeitet werden.\nArbeitsspeicher (30 Min)  Finden Sie heraus, wie viel freier Arbeitsspeicher Ihr Computer hat (Das Betriebssystem und Hintergrundprogramme verbrauchen auch RAM). Wie viele int64 Werte, also Zahlen, die 8 Byte (=64 Bit) Speicher benötigen, können Sie damit theoretisch in den Arbeitsspeicher laden? Tip: Nutzen sie Google zum Umrechnen. Wie viele Beobachtungen kann eine Tabelle mit 100 numerischen Variablen damit maximal theoretisch haben, damit Sie diese noch bearbeiten können? Nutzen Sie die Funktion memory_usage um sich den tatsächlich benötigten Speicher eines DataFrames oder einer Series anzeigen zu lassen. Mit dem Funktionsargument deep=True wird der Wert genau ermittelt und nicht nur geschätzt.    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/ml/ml/",
	"title": "Machine Learning",
	"tags": [],
	"description": "",
	"content": "Seit der Erfindung des Personal Computers und des Internets werden statistische Probleme immer komplexer und größer. Die Datenmengen erfordern neue effiziente Strukturen zum Speichern und Auffinden der Informationen.\nMaschinelles Lernen (Machine Learning oder Statistical Learning) bedeutet in diesem Kontext relevante und signifikante Muster und Trends aus den Daten zu extrahieren um die Daten \u0026ldquo;zu verstehen\u0026rdquo;. Dabei spielen Computer und deren wachsende Rechenpower eine immer größere Rolle. Sie haben die klassische angewandte Statistik revolutioniert und es sind vor allem Ingenieure und Informatiker, die die Weiterentwicklung der Disziplin heutzutage vorantreiben.1\nIm Maschinellen Lernen steht insbesondere die Vorhersagekraft und Generalisierbarkeit von statistischen Methoden und Algorithmen im Vordergrund. Ziel ist es, möglichst gute Prognosen, beispielsweise bei der Gesichtserkennung, zu machen. Die klassische Statistik dagegen ist stärker an den kausalen Zusammenhängen und der Stärke von signifikanten Einflüssen einzelner Faktoren auf ein Resultat interessiert.\nTeachable Machine von Google ermöglicht das Trainieren von Machine Learning Modellen im eigenen Web-Browser ohne das Programmiercode geschrieben werden muss. Somit wird das Grundprinzip des Machinellen Lernens (hier: Supervised Learning) auch ohne Vorkenntnisse erfahrbar und man bekommt ein gutes Gespür für die Möglichkeiten und Grenzen der Methode.\nExperimentieren Sie zum Beispiel mit dem Bild-Klassifikator.\n  Maschinelles Lernen und die angewandte Statistik besitzen große Überschneidungen und beide Gebiete bauen auf Erkenntnissen der Wahrscheinlichkeitstheorie auf. Machine Learning ist dabei auch ein Teilgebiet der Künstlichen Intelligenz, die als die Automatisierung von intellektuelle Aufgaben, die normalerweise von Menschen durchgeführt werden verstanden wird.2\nKI kann auch allein mit durch Programmierer fest eingebauten Regeln entstehen. Diese Regeln legen fest, wie eine Eingabe (z.B Pixelwerte eines Bilds oder die Anzahl der gestrigen Sonnenstunden) in eine Ausgabe (Wahrscheinlichkeit für ein Katzenbild oder heutige Regenwahrscheinlichkeit) transformiert wird. Damit diese Art der KI erfolgreich ist, braucht es ein großes Vorwissen und spezielle Expertise. Algorithmen des Maschinellen Lernens werden hingegen trainiert, d.h. sie lernen selbstständig die optimalen Regeln,die von einer Eingabe zu einer Ausgabe schließen lassen. Damit diese Transformation erfolg hat benötigt das System viele Beispiele, für die die Ausgabe bekannt ist.2\n  Hastie, Tibshirani, Friedman (2017): The Elements of Statistical Learning, Springer. \u0026#x21a9;\u0026#xfe0e;\n Francois Chollet (2018): Deep Learning with Python, Manning. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/columns/",
	"title": "Auswahl und Erstellung von Spalten",
	"tags": [],
	"description": "",
	"content": "Die Spalten eines DataFrames werden über einen Spaltenindex referenziert. Üblicherweise besteht der Spaltenindex aus Spaltennamen in Textform:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df.columns    Einzelne Spalten können wie bei einem Python Dictionary mit df[\u0026lt;name\u0026gt;] ausgewählt werden. Mehre Spalten mit df[[\u0026lt;name1\u0026gt;, \u0026lt;name2]]. Wenn Sie mehrere Spalten auswählen erhalten Sie einen DataFrame zurück. Bei einer Spalte, eine Series. Das Ergebnis der Auswahl können Sie bei Bedarf wieder in einer Variablen abspeichern:\nx = df[\u0026#39;Total Renewals\u0026#39;] df[[\u0026#39;Total Renewals\u0026#39;, \u0026#39;Total Checkouts\u0026#39;]] filter = [\u0026#39;Total Renewals\u0026#39;, \u0026#39;Total Checkouts\u0026#39;] # auxiliary variable subset = df[filter] print(x) print(subset)    Spalten können mit einer Zuweisung (=) überschrieben oder neu erstellt werden:\ndf[\u0026#39;dummy_variable\u0026#39;] = 5    Bei der Auswahl von Spalten und Zeilen wird keine Kopie des DataFrames oder der Series erstellt, sondern nur eine Referenz auf die ursprüngliche Tabelle. Wenn Sie Daten in der ursprünglichen Tabelle ändern, so ändert sich auch die Referenz:\nx = df[\u0026#39;Total Renewals\u0026#39;] df[\u0026#39;Total Renewals\u0026#39;] = 5 x    Berechnungen auf schon bestehenden Variablen können auch direkt einer neuen Spalte zugeordnet werden:\nimport numpy as np df[\u0026#39;is_adult\u0026#39;] = df[\u0026#39;Patron Type Definition\u0026#39;] == \u0026#39;ADULT\u0026#39; df[\u0026#39;log_renewals\u0026#39;] = np.log(df[\u0026#39;Total Renewals\u0026#39;])    Im ersten Beispiel wurde zuerst die Anweisung df['Patron Type Definition'] == 'ADULT' durchgeführt. Das implizite Ergebnis dieser Anweisung ist eine Series mit booleschen Werten (True oder False). Die neu erstellte Series wird dann in einer neuen Spalte is_adult dem DataFrame angehängt.\nIm zweiten Beispiel wurde der Logarithmus auf den Werten der Spalte Total Renewals berechnet und einer neuen Spalte log_renewals zugewiesen.\nFallstudie: Feature Engineering (20 Min) Ziel ist es, eine neue Variable Membership Duration zu erstellen, die für jeden Kunden die aktive Mitgliedschaft in Monaten seit der Registrierung misst. Die aktive Mitgliedschaft wird definiert als:\n\u0026#39;Membership Duration\u0026#39; = (\u0026#39;Circulation Active Year\u0026#39; - \u0026#39;Year Patron Registered\u0026#39;)*12 + \u0026#39;Circulation Active Month\u0026#39;  Die Spalte Circulation Active Year ist als Text und nicht als Zahl abgespeichert! Konvertieren Sie die Spalte in ein numerisches Format. Überschreiben Sie die ursprüngliche Variable mit den neuen Werten. Nutzen Sie dieses Codesnippet:  pd.to_numeric( df[\u0026#39;Circulation Active Year\u0026#39;], errors=\u0026#39;coerce\u0026#39; ) Die Spalte Circulation Active Month enthält die Monatsnamen als Text. Für die Berechnung muss diese in ein numerisches Format konvertiert werden. Die Umcodierung kann händisch erfolgen. Sie können aber auch dieses Codesnippet nutzen::  df[\u0026#39;Circulation Active Month\u0026#39;] = pd.to_datetime( df[\u0026#39;Circulation Active Month\u0026#39;], errors=\u0026#39;coerce\u0026#39;, format=\u0026#34;%B\u0026#34; ) df[\u0026#39;Circulation Active Month\u0026#39;].dt.month  Berechnen Sie nun die aktive Mitgliedschaftsdauer in Monaten wie oben definiert.\n  Nehmen Sie an, dass Einträge mit fehlenden Werten eine aktive Mitgliedschaft von Null Monaten bedeuten. Ersetzen Sie dazu alle NaN values in der neuen Variable mit der Zahl 0. Nutzen Sie dieses Codesnippet:\n  df[\u0026#39;Membership Duration\u0026#39;].fillna(0, inplace=True)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/ml/data-science/",
	"title": "Data Science",
	"tags": [],
	"description": "",
	"content": "Ein Data Scientist ist eine Person, die oder der Wissen und Erkenntnisse aus strukturierten und unstrukturierten Daten gewinnt. Data Science ist eine interdisziplinäre Disziplin und liegt irgendwo in der Schnittmenge von angewandter Statistik, angewandter Informatik (Hacking skills) und spezielles Fachwissen (domain knowledge/ substantive expertise).\nAufgrund der stark angewachsenen Mengen an unstrukturierten Daten aus heterogenen Datenquellen (Text, Bilder, Sensoren, Netzwerke, Videos, \u0026hellip;) reichen die Methoden und Fertigkeiten, die die Statistik traditionellerweise liefert und vermittelt, nicht mehr aus, um diese Daten effizient zu strukturieren, aggregieren, kombinieren, analysieren und visualisieren zu können:\n A Data Scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician 1.\n Quelle: http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram\n  Joel Grus (2019): Data Science from Scratch, O'Reilly. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/rows/",
	"title": "Auswahl von Zeilen",
	"tags": [],
	"description": "",
	"content": "Die Zeilen eines DataFrames werden über einen Zeilenindex (loc[]), über die aufsteigenden Zeilennummern (iloc[]) oder über logische Ausdrücke ([] oder loc[]) referenziert.\nHier wird nur der wichtigste letzte Fall näher betrachtet:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[df[\u0026#39;Total Checkouts\u0026#39;] \u0026gt; 10000]    Der Ausdruck df['Total Checkouts'] \u0026gt; 10000 wird zuerst ausgewertet und ergibt eine boolesche Series mit Einträgen True wenn die Beobachtung mehr als 1000 Ausleihen getätigt hat und False sonst.\nMit einer booleschen Series lassen sich dann die Zeilen des DataFrame auswählen: Es werden genau die Zeilen zurückgegeben, bei denen die Series True Werte enthält.\nAnstatt alles in einer Zeile zu schreiben, können wir auch eine Hilfsvariable erstellen, die den booleschen Vektor zwischenspeichert:\nfilter = df[\u0026#39;Total Checkouts\u0026#39;] \u0026gt; 10000 df[filter]    Für den booleschen Zeilenfilter können komplexe logische Ausdrücke unter Zuhilfenahme der Operatoren \u0026lt;, \u0026gt;, \u0026amp;, |, == u.s.w. gebildet werden. Welche Zeilen werden hier gefiltert?\nfilter = (df[\u0026#39;Patron Type Definition\u0026#39;] == \u0026#39;SENIOR\u0026#39;) \u0026amp; (df[\u0026#39;Notice Preference Definition\u0026#39;] == \u0026#39;email\u0026#39;) df[filter]    Logische Operatoren    Ausdruck Beschreibung     \u0026lt;/ \u0026lt;= kleiner/ kleiner gleich   \u0026gt; / \u0026gt;= größer/ größer gleich   == gleich   != ungleich   \u0026amp; elementweises logisches und (True und True ergeben True, sonst False)   | elementweises logisches oder (False und False ergeben False, sonst True)   ~ elementweise logische negation (True ergibt False und umgekehrt)      Nützlich ist auch die Funktion Series.between(left, right), mit der eine boolesche Series erstellt wird, die True ist wenn der Wert der ursprünglichen Series zwischen oder auf left und right liegt:\nfilter1 = (df[\u0026#39;Total Checkouts\u0026#39;] \u0026gt;= 20) \u0026amp; (df[\u0026#39;Total Checkouts\u0026#39;] \u0026lt;= 80) filter2 = df[\u0026#39;Total Checkous\u0026#39;].between(20, 80) all(filter1 == filter2)    Filtern (30 Min)  Filter Sie den Datensatz nach Kindern unter 10 Jahren. Wie viele Einträge erhalten Sie? Gibt es Personen mit mehr als 20000 Ausleihen? Wie viele Personen stammen aus dem Norden San Franciscos (Supervisor Districts 1, 2 und 3)? Nutzen Sie die Funktion Series.isin(). Wie viele Prozent der Beobachtungen haben eine Membership Duration von Null Monaten?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/na/",
	"title": "Exkurs: Fehlende Werte",
	"tags": [],
	"description": "",
	"content": "Real erhobene Daten sind meistens unsauber und fehlerhaft. Ein häufiges Problem dabei sind fehlende Werte, also Beobachtungen für die manche Merkmale nicht erhoben wurden. In jedem Datensatz werden fehlende Werte anders gekennzeichnet, aber man findet oft diese Kodierungen wieder: \u0026quot;-999\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;NULL\u0026quot;, \u0026quot;#N/A\u0026quot;.\nWenn beispielsweise der Mittelwert einer statistischen Variable berechnet wird, so muss entschieden werden, wie mit fehlenden Werten umgegangen werden soll: Sollen die Werte entfernt werden? Sollen die fehlenden Werte durch einen bestimmten Wert ersetzt werden?\nIn DataFrames werden fehlende Werte durch das Schlüsselwort NaN (\u0026quot;Not a Number\u0026quot;) angezeigt. Beim Einlesen von Daten (siehe z.B. die read_csv Funktion) können mit dem Argument na_values zusätzliche Kodierungen für fehlerhafte Werte mit angegeben werden.\nFallbeispiel Der Library Usage Datensatz enthält die Kodierung \u0026quot;None\u0026quot; für fehlende Werte. Diese werden von pandas beim Einlesen von numerischen Spalten nicht richtig erkannt:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Circulation Active Year\u0026#39;] Obwohl die Spalte 'Circulation Active Year' numerisch ist, wird Sie von pandas als Text abgespeichert, da \u0026quot;None\u0026quot; nicht als Zahl erkannt wird. Möchten Sie z.B. 2019 - df['Circulation Active Year'] berechnen, so werden Sie eine Fehlermeldung erhalten, da für Text-Werte keine Substraktionen durchgeführt werden können.\nUm das Problem zu beheben können Sie auf zwei Arten vorgehen. Sie können schon beim Einlesen, die Kodierung für fehlende Werte mit angeben:\ndf = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;, na_values=\u0026#34;None\u0026#34;) df[\u0026#39;Circulation Active Year\u0026#39;] Oder Sie führen nach dem Einlesen eine explizite Umwandlung des Datentyps durch:\ndf = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;, na_values=\u0026#34;None\u0026#34;) df[\u0026#39;Circulation Active Year\u0026#39;] = pd.to_numeric(df[\u0026#39;Circulation Active Year\u0026#39;], errors=\u0026#39;coerce\u0026#39;) df[\u0026#39;Circulation Active Year\u0026#39;]  Was unterscheidet den Wert None vom Wert \u0026quot;None\u0026quot;? Was den Wert 5 vom Wert \u0026quot;5\u0026quot;? Was den Wert \u0026quot;NaN\u0026quot; vom Wert NaN? Ist True und \u0026quot;True\u0026quot; das gleiche?\n  Behandlung von Fehlenden Werten Pandas bietet für Series und DataFrames die nützlichen Funktionen isna(), notna(), dropna() und fillna() an um fehlende Werte zu identifizieren, zu entfernen oder mit anderen Werten zu ersetzen.\nFilter Die Funktionen isna (notna) geben eine boolesche Series zurück, die True ist, wenn an der Stelle ein fehlender Wert steht. Damit pandas fehlende Werte korrekt erkennt, müssen diese vorher erst in das interne Format NaN umgewandelt werden (siehe oben).\ndf[df[\u0026#39;Age Range\u0026#39;].isna()] df[df[\u0026#39;Age Range\u0026#39;].notna()]    Mit diesem nützlichen Befehl können Sie sich schnell die Anzahl fehlender Werte in jeder Spalte ausgeben lassen:\ndf.isna().sum()    Dies funktioniert, da Python bei Bedarf einen booleschen Wert implizit in ein numerisches Format konvertiert. True wird zu 1 konvertiert und False zu 0.\nEntfernen # drops all rows that contain at least one missing values df.dropna() # drops all missing values in this series df[\u0026#39;Age Range\u0026#39;].dropna()    Ersetzen df[\u0026#39;Age Range\u0026#39;].fillna(\u0026#34;keine Angabe\u0026#34;)    Standardmäßig werden bei den Operationen fillna oder dropna neue Series oder DataFrames zurückgegeben. Die originale Variable bleibt dabei unangetastet. Mit dem Argument inplace=True werden die originalen Objekte direkt überschrieben.\n  Fehlende Werte (20 Min)  Welche Spalten enthalten alles fehlende Werte? Lesen Sie den Datensatz ein und erstellen Sie einen DataFrame der keine Beobachtungen mit fehlenden Werten mehr enthält. Speicher Sie diesen unter dem Namen Library_Usage_Clean.csv ab. Wie viele Beobachtungen wurden dabei entfernt?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/",
	"title": "Kursorganisation und Vorbereitungen",
	"tags": [],
	"description": "",
	"content": "21.01 – 26.01 Kursorganisation und Vorbereitungen Diese Einheit gibt einen Überblick über die Kursinhalte, wichtige Termine und die benötigte Software und Python-Pakete. Viele der hier besprochenen Dinge werden Ihnen schon bekannt vorkommen und die Software haben Sie voraussichtlich auch schon auf Ihrem Rechner installiert.\nZiele  Installieren Sie Anaconda mit Python 3.7 auf Ihrem Rechner Erstellen Sie einen Projektordner und fügen Sie einen Datensatz Ihrem Projektordner hinzu Stellen Sie sicher, dass Python Notebooks lokal ausgeführt werden können Wiederholen Sie die grundlegenden Python Programmier-Basics aus dem ersten Modul  Dieses Cheat-Sheet gibt einen guten Überblick über die wichtigsten Befehle in Python.\nWeitere Ressourcen  Interaktive Python Online-Tutorials auf learnpython.org Einführung in Python auf kaggle Python Data Analaysis Tutorials Pandas Tutorial auf kaagle    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/functions/",
	"title": "Überblick: Funktionen in Pandas",
	"tags": [],
	"description": "",
	"content": "Mit df.head() können Sie sich die ersten $n$ Zeilen eines DataFrames anzeigen lassen:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df.head()    Analog dazu funktioniert die Funktion df.tail().\nPandas Funktionen (5 Min)\nSchauen Sie sich die Dokumentation für die Funktion head() hier an. Wie können Sie sich die ersten $100$ Zeilen anzeigen lassen?\n  Mit df.info() erhalten Sie speicherbezogene Informationen über das Objekt. Mit df.describe() werden nützliche deskriptive Statistiken für alle numerischen Spalten eines Datensatzes ausgegeben. Um alle Spalten miteinzubeziehen nutzen sie das Funktionsargument include='all':\ndf.describe(include=\u0026#39;all\u0026#39;)    Viele Funktionen funktionieren für DataFrames und Series gleichermaßen:\nprint(df.min()) print(df[\u0026#39;Total Renewals\u0026#39;].min())    Mit der Funktion sum() werden die Werte einer Spalte aufaddiert:\ndf[\u0026#39;Total Renewals\u0026#39;].sum() df[\u0026#39;Total Renewals\u0026#39;].between(100, 200).sum()    DataFrames besitzen drei wichtige Attribute, die Informationen über die Spalten, die Datentypen und die Anzahl der Elemente geben:\ndf.columns df.dtypes df.shape "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/modules/",
	"title": "Kurseinheiten",
	"tags": [],
	"description": "",
	"content": "Wir haben das Modul in wöchentliche Einheiten, die jeweils ein Gebiet aufgreifen und vertiefen, unterteilt. Sie können sich die Zeit für die Bearbeitung der Einheiten selber aufteilen, sollten aber jede Einheit am Ende der jeweiligen Woche abgeschlossen haben. Am Ende einer Woche wird die nächste Einheit auf dieser Webeite freigeschaltet.\nJede Einheit umfasst kleine praktische Projektaufgaben, welche Sie in Form eines Jupyter Notebooks bearbeiten und aufbereiten. Zu jeder Einheit werden viele verschiedene Aufgaben mit unterschiedlichem Schwierigkeitsgrad angeboten. Wenn Sie mit einer Aufgabe nicht weiterkommen oder zu viel Zeit aufwenden müssen, können Sie diese am Präsenztag mit den Betreuern besprechen.\nDer erste Teil des Moduls (21-01 - 15.02) wird von Malte Bonart betreut und behandelt grundlegende klassische Konzepte der angewandten Statistik. Der zweite Teil des Moduls (16.02 - 04.03) wird von Konrad Förstner betreut und gibt einen Überblick über Themen des Maschinellen Lernens.\nAm Präsenztag, der am 05.03.2020 stattfindet, werden wir im voraus gesammelte Fragen gemeinsam beantworten und diskutieren. Sie werden Zeit haben, an einem persönlichen Datenanalyseprojekt zu arbeiten. Die Kursleiter werden Sie dabei unterstützen und individuell betreuen. Am Ende des Präsenztages stellen alle KursteilnehmerInnen ihre Ergebnisse in einer Kurzpräsentation vor.\n"
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/schedule/",
	"title": "Termine",
	"tags": [],
	"description": "",
	"content": "Hier finden Sie einen Überblick über die einzelnen Moduleinheiten.\n   Datum Titel Ziele     21.01 – 26.01 Vorbereitung Installieren Sie die benötigte Software Laden Sie die Kursmaterialien und Datensätze herunter Stellen Sie sicher, dass Python Notebooks lokal ausgeführt werden können   27.01 – 02.02 Grundlagen Beschreiben Sie Datensätze mit dem statistischen Grundvokabular Lesen Sie Datensätze als DataFrames in Python ein Filtern Sie DataFrames nach Spalten oder Zeilen Erstellen Sie neue Variablen   03.02 – 09.02 Deskriptive Statistik und Visualisierung Berechnen Sie grundlegende Lage- und Streuungsmaße Berechnen Sie Statistiken für bivariate Verteilungen Erstellen Sie einfache Visualisierungen   10.02 – 16.02 Inferenzstatistik / Maschinelles Lernen I Führen Sie einen Zwei-Stichproben Mittelwerttest durch Beschreiben Sie die Unterschiede zwischen Supervised und Unsupervised Learning   17.02 – 23.02 Maschinelles Lernen II Beschreiben Sie grundlegende Funktionsweisen und Konzepte von scikit-learn Führen Sie eine Regression, Klassifikation oder Clustering mit scikit-learn durch   24.02 – 01.03 Maschinelles Lernen III Beschreiben Sie die Funktionsweise von Text-Analyse mit NLTK oder spaCy Formulieren Sie einfache quantitative Fragen für den Projekttag als Expose (max. 1 Seite Text)   02.03 – 04.03 Vorbereitung Präsenztag Suchen Sie nach geeigneten Daten für den Projekttag Schicken Sie Ihre inhaltlichen und fachlichen Fragen an die Kursleiter   5.03 Präsenztag Nehmen Sie an der Frage und Antwortrunde teil Finden Sie geeignete Daten zum Lösen der Fragen Beantworten Sie Ihre Frage mit den gelernten statistischen Tools Bereiten Sie die Ergebnisse in Form einer Visualisierung auf Stellen Sie die Ergebnisse in einer Kurzpräsentation in Ihrer Gruppe vor (\u0026lt; 5 Minuten)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/anaconda/",
	"title": "Conda und Anaconda",
	"tags": [],
	"description": "",
	"content": "Conda ist eine freie und offene Softwarepaketverwaltung für Python. Neben der Möglichkeit, Pakete (packages, libraries) für Python aus dem Internet zu installieren, können mit conda virtuelle Umgebungen (environments) angelegt werden. Diese Umgebungen beinhalten nur die Pakete und Python Versionen, die für ein spezifisches Projekt gebraucht werden. Umgebungen können mit anderen Personen geteilt werden, sodass sichergestellt ist, dass alle Programmierer mit den gleichen Paketen und Versionen arbeiten, auch wenn sie unterschiedliche Systeme (Windows, Linux, MacOS) verwenden.\nAnaconda basiert auf conda. Mit Anaconda werden eine Vielzahl von Paketen, die für die Datenanalyse gebraucht werden, schon vorinstalliert. Außerdem bietet Anaconda eine vorinstallierte Entwicklungsumgebung (Spyder IDE) und eine vorinstallierte Version von Jupyter, mit der Notebooks gestartet werden können.\nAnaconda (10 Min)  Wenn noch nicht geschehen, können Sie Anaconda hier für Ihr Betriebssystem herunterladen. Wir verwenden die Version für Python 3.7. Öffnen Sie den mit Anaconda installierten Anaconda Navigator und verschaffen Sie sich einen Überblick über die vorhandenen Programme.    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/notebooks/",
	"title": "Jupyter Notebooks",
	"tags": [],
	"description": "",
	"content": "Die Projekt-Aufgaben und Code-Beispiele in diesem Modul werden über Jupyter Notebooks erstellt und verteilt.\nJupyter Notebooks bieten eine browserbasierte graphische Schnittstelle zur Python Programmierumgebung. Deswegen können Notebooks auf jedem System gestartet werden, man benötigt dazu nur einen Web-Browser und eine lokale installierte Version von Python.\nDarüber hinaus bieten Notebooks die Möglichkeit Text, Visualisierungen und Code in einer integrierten Datei zu erstellen. Somit können einfach statistische Reports und Analysen erstellt werden. Die Replizierbarkeit der Ergebnisse ist auch gewährleistet, da jede Person, die Programmierschritte im Notebook auf dem eignen Rechner wiederholen kann.\nJupyter Notebook enthält einen Dateimanager mit dem Sie durch die Ordner und Dateien Ihres Systems navigieren können. Mit einem Klick auf eine Notebook-Datei öffnet sich ein neues Browser-Tab mit dem Notebook. Notebooks bestehen immer aus Text/ Markdown oder Code Zellen (cells). Der Python Code in den Zellen kann ausgeführt werden und das Ergebnis wird direkt im Notebook angezeigt.\nNotebooks (20 Min)  Laden Sie dieses Notebook herunter (Rechtsklick -\u0026gt; Ziel/Link speichern unter\u0026hellip;) Starten Sie Jupyter Notebook über die Kommandozeile oder über den Anaconda Navigator Navigieren Sie zu dem Speicherort des Notebooks und öffnen Sie es. Markieren Sie die Code-Zelle und führen Sie sie mit einem Klick auf den Run Button oder mit der Tastenkombination Strg+Enter aus Versuchen Sie, die Farbe der Punkte im Plot von Grün auf Rot zu ändern Fügen Sie das Datum und Ihren Namen der Text-Zelle hinzu     Notebook-Dateien erkennen Sie immer an der Dateiendung .ipynb. Diese Dateien können Sie in Jupyter mit dem integrierten Dateimanager öffnen. Jupyter starten Sie entweder über den Anaconda Navigator oder indem Sie den folgenden Befehl in Ihrer Kommandozeile ausführen (Die Kommandozeile danach nicht wieder schließen!):  jupyter notebook  Rufen Sie http://localhost:8890 in Ihrem Browser auf, um zur Oberfläche von Jupyter zu gelangen.      Jupyter Notebook   tutorial_jupyter.ipynb  (21 ko)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/dataset/",
	"title": "San Francisco Library Usage",
	"tags": [],
	"description": "",
	"content": "Im ersten Teil des Moduls werden Sie einen offenen Kundendatensatz der Bibliothek in San Francisco analysieren.\n The Integrated Library System (ILS) is composed of bibliographic records including inventoried items, and patron records including circulation data. The data is used in the daily operation of the library, including circulation, online public catalog, cataloging, acquisitions, collection development, processing, and serials control. This dataset represents the usage of inventoried items by patrons \u0026hellip; (Abstract taken from here)\n Anlegen eines Projektordners (15 Min)  Besuchen Sie das offene Daten-Portal der Stadt San Francisco und informieren Sie sich über den Datensatz Erstellen Sie einen Ordner auf Ihrem Computer. Dieser Ordner wird Ihr Projektordner für dieses Modul. Dort legen Sie alle Datensätze und Jupyter Notebooks ab. Erstellen Sie einen Unterordner ./data/ und einen Unterordner ./notebooks/ innerhalb Ihres Projektordners. Laden Sie den Datensatz Library_Usage.csv aus dem Internet herunter und speichern Sie ihn im Projektordner im Unter-Ordner ./data/ ab. Stellen Sie sicher, dass Ihr Projektordner die folgende Verzeichnisstruktur aufweist:  data-librarian-3 ├── data │ └── Library_Usage.csv ├── notebooks │ └── tutorial_jupyter.ipynb    In dieser Excel Tabelle finden Sie eine detallierte Erklärung der einzelnen Variablen des Datensatzes.\n    books by 1 brian is licesed under CC BY-NC-SA 2.0\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/",
	"title": "Python Pakete und Bibliothekten",
	"tags": [],
	"description": "",
	"content": "Die folgende Liste gibt einen kurzen Überblick über die wichtigsten Python Bibliotheken, von denen Sie manche im Modul näher kennenlernen werden. Im ersten Teil des Modules werden wir hauptsächlich mit pandas und seaborn arbeiten.\nProgramming Recap  Module, Paket, Library  Ein Python Skript mit der Endung .py wird Modul genannt. Eine Sammlung von Modulen in einem Ordner, wird Paket (package) genannt. Eine Sammlung von Paketen innerhalb eines größeren Projekts wird Bibliothek (library) genannt. Ein framework ist eine große grundlegende Bibliothek, mit einem bestimmten Zweck und mit vielen Paketen, die voneinander abhängen und aufeinander aufbauen. Die Begriffe werden aber nicht einheitlich benutzt und der Übergang ist oft fließend.   Kommentare stehen immer hinter dem # Zeichen. Text steht immer in Anführungszeichen, z.B \u0026quot;hallo\u0026quot; oder 'hi'. Mit dem import Befehl können externe Bibliotheken mit mehr Funktionalitäten geladen werden. Mit dem Zuweisungsoperator = können Objekte einem Variablennamen oder einem Funktionswert zugeordnet werden, z.B: x = 1, text = 'hallo'. Funktionen werden mit runden Klammern aufgerufen und können Funktionsargumente besitzen, z.B. sum([1, 2, 3]). Viele Funktionen sind Bestandteil von Bibliotheken und werden dann wie folgt aufgerufen: \u0026lt;paketname\u0026gt;.\u0026lt;funktionsname\u0026gt;(\u0026lt;funktionsargumente\u0026gt;)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/quiz_intro/",
	"title": "Recap: Quiz",
	"tags": [],
	"description": "",
	"content": " .quiz fieldset { border-color: black; border-width: 10px; margin-bottom: 1em; } .quiz legend { font-size: 105%; font-weight: 600; padding-left: 15px; padding-right: 15px; padding-top: 15px; } .quiz label { display: block; line-height: 1.75em; } .quiz input[type=\"radio\"] { margin-right: 10px; page-break-after: avoid; page-break-before: avoid; } .quiz input[type=\"submit\"] { background: black; color: white; display: block; font-size: 120%; font-weight: 600; height: 2.5em; margin-top: 2em; text-transform: uppercase; width: 100%; } .quiz table { color: white; font-weight: bold; margin: 1em auto 2em auto; width: 100%; } .quiz td { padding: 5px 15px; text-align: left; width: 60px; } .quiz td.missing-label, .quiz td.missing-score { background: #CECBC2; } .quiz td.right-label, .quiz td.right-score { background: #74b559; } .quiz td.wrong-label, .quiz td.wrong-score { background: #D01F3C; }    var choices = \"Strg+R,Enter,Strg+Enter\".split(\",\"); var id = \"vorbereitung_quiz\"; var question = \"Mit welcher Tastenkombination können sie Zellen in Jupyter Notebooks ausführen?\"; var answer = 3 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"weniger als 200,200-400,401-600,mehr als 600\".split(\",\"); var id = \"vorbereitung_quiz\"; var question = \"Wie viele Pakete sind in Anaconda unter der Linux-Python 3.7 Version verfügbar bzw. schon vorinstalliert?\"; var answer = 4 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"423448,423000,15,2103\".split(\",\"); var id = \"vorbereitung_quiz\"; var question = \"Wie viele Zeilen enthält der San Francisco Library Usage Datensatz?\"; var answer = 1 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"das Paket seaborn wird importiert,dem Paket wird der kürzere Name sns zugewiesen,das Standard-Design von mit matplotlib generierten Plots wird angepasst\".split(\",\"); var id = \"vorbereitung_quiz\"; var question = \"Was macht die Funktion sns.set() aus dem seaborn package?\"; var answer = 3 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var quiz = new Quiz(\"vorbereitung_quiz\", questions, {\"shuffle\": true});   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/ml/",
	"title": "Statistik, Data Science und Machine Learning",
	"tags": [],
	"description": "",
	"content": "Seit einigen Jahren sind Data Science und Machine Learning zu alltäglichen Begriffen geworden. Studiengänge im Bereich Data Science werden neu eingerichtet oder schon bestehende Abschlüsse umbenannt. Anwendungen des Maschinellen Lernens sind Thema in Massenmedien. Der Zertifikationskurs Data Librarian spiegelt diese Popularität wieder.\nUm die Zusammenhänge zwischen den Fachgebieten besser zu verstehen, wird im folgenden wird ein kurzer Überblick über die Begriffe gegeben. Für Interessierte gibt es Verweise zu weiteren Quellen.\n trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"data science\",\"geo\":\"DE\",\"time\":\"2010-01-01 2019-11-22\"},{\"keyword\":\"machine learning\",\"geo\":\"DE\",\"time\":\"2010-01-01 2019-11-22\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=2010-01-01%202019-11-22\u0026geo=DE\u0026q=data%20science,machine%20learning\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"});  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/basic_terms/",
	"title": "Grundbegriffe der Statistik",
	"tags": [],
	"description": "",
	"content": "In der Kursumgebung finden Sie das Einführungskapitel des Buchs Statistik: Der Weg zur Datenanalyse zum alleinigen persönlichen Gebrauch hier im Kurs hinterlegt.1\n Eine weitere sehr gute und modernere Einstiegsliteratur sind die englischen Lehrbücher von OpenIntro. Sie können kostenlos auf der Webseite heruntergeladen werden. Für diesen Kurs sind insbesondere die ersten zwei Kapitel des Buchs Introductory Statistics with Randomization and Simulation zu empfehlen. 2\n Der Text gibt einen Einstieg in die Aufgaben und Anwendungsbereiche der Statistik und erklärt die grundlegenden Begriffe, mit denen Daten und Datensätze charakterisiert werden können. Nach der Einheit sollten Sie die folgenden Fragen beantworten können:\n Was ist Statistik? Was macht Statistik? Welche grundlegenden Begriffe werden in der Statistik verwendet? Wie kann das Messniveau für Spalten eines Datensatzes bestimmt werden?  Grundbegriffe (45 Min) Beantworten und diskutieren Sie folgende Fragen konkret für den San Francisco Library Usage Datensatz. Halten Sie Ihre Ergebnisse in Stichpunkten in einem Jupyter Notebook fest. Nicht alle Fragen lassen sich klar beantworten.\n Wie viele Merkmale besitzt der Datensatz? Wie groß ist die Stichprobengröße des Datensatzes? Wer oder was sind die Merkmalsträger? Von wann bis wann wurden die Daten erhoben? Wie lässt sich die Grundgesamtheit beschreiben? Handelt es sich um eine Vollerhebung? Welche Merkmale sind stetig? Welche diskret? Welchem Skalenniveau entsprechen die einzelnen Merkmale (Nominal-, Ordinal- oder Metrische Skala)? Enthält der Datensatz fehlende Werte? Handelt es sich um Querschnitts-, Längsschnitss- oder Paneldaten?      Fahrmeir, Ludwig, Christian Heumann, Rita Künstler, Iris Pigeot, and Gerhard Tutz. Statistik: Der Weg zur Datenanalyse. Springer-Verlag, 2016, https://www.springer.com/de/book/9783662503713. \u0026#x21a9;\u0026#xfe0e;\n Diez, David, Christopher Barr, and Mine Cetinkaya-Rundel. Introductory Statistics with Randomization and Simulation, 2014, https://www.openintro.org/stat/textbook.php. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/",
	"title": "Univariate Verteilungen",
	"tags": [],
	"description": "",
	"content": "In der Statistik geben Verteilungen an, wie wahrscheinlich oder häufig eine bestimmte Merkmausausprägung oder Kombination von Merkmausausprägungen ist. Univariate Verteilung beschreiben dabei die Wahrscheinlichkeiten einer einzelnen statischen Variablen, während bivariate oder multivariate Verteilungen sich auf zwei oder mehr Variablen beziehen.\nEmpirische Verteilungen beziehen sich dabei auf die Häufigkeiten in konkreten Daten während theoretische Verteilungen als mathematischen Funktionen, die von einigen wenigen Parametern abhängen, vorliegen. Statistiken, wie der Mittelwert oder der Modus dienen zur Beschreibungen und Charakterisierung von Verteilungen durch aussagekräftige Kennzahlen. Dabei gibt es Statistiken, die oft nur auf Variablen mit einem bestimmten Skalenniveaus anwendbar sind.\nKategoriale (nominale und ordinale) Variablen können in Häufigkeitstabellen zusammengefasst werden. Wichtige charakteristische Eigenschaften für metrische Variablen sind die zentrale Lage, die Streuung und die Symmetrie.\nNach dieser Einheit sollten Sie die folgenden Fragen beantworten können:\n Wie erstelle und interpretiere ich eine (relative) Häufigkeitstabelle? Welche grundlegenden Statistiken kann ich mit Pandas Funktionen ausrechnen? Was ist der Unterschied zwischen dem Median und dem arithmetischem Mittel? Welche Funktionen gibt es, um die Streuung einer Variablen zu messen? Welche univariaten Verteilungstypen gibt es?  Im Folgenden werden mit $x = x_1, \\dots, x_n$ eine univariate Reihe von Beobachtungen beschrieben, mit $n$ die Anzahl der Beobachtungen. $x_i$ beschreibt die Beobachtung an der i-ten Stelle.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/mean/",
	"title": "Lagemaße",
	"tags": [],
	"description": "",
	"content": "Für metrische Variablen beschreiben Lagemaße die Zentralität einer Verteilung. Das bekannteste Lagemaß ist der empirische Mittelwert:\n$$ \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{x_1 + x_2 + \\dots + x_n}{n} $$\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Total Checkouts\u0026#39;].mean()    Eine zweite wichtige Statistik ist der Median. Er ergibt sich aus dem Wert der Beobachtung, die die nach der Größe geordnete Messreihe in genau zwei gleich große Teile teilt. Für eine gerade Anzahl an Beobachtung wird der Mittelwert der Beobachtung an der Stelle $n/2$ und $n/2+1$ genommen:\n$$ x_{0.5} = \\begin{cases} x_{(n+1)/2}~, \\text{ n ungerade} \\\\\n\\frac{x_{n/2} + x_{n/2+1}}{2}~, \\text{ n gerade} \\end{cases} $$ für $x_1 \u0026lt; x_2 \u0026lt; \\dots \u0026lt; x_n$.\nBeispiel: Für $x=[8, 10, 11, 30]$ ist $n=4$ gerade und der Median wird berechnet mit $\\frac{x_2 + x_3}{2} = \\frac{10+11}{2} = 10.5$.\ndf[\u0026#39;Total Checkouts\u0026#39;].median()     Schauen Sie sich den Mittelwert und den Median der Variable Total Checkouts an. Warum sind die beiden Werte so unterschiedlich? Was ziehen Sie daraus für Schlüsse für weitere statistische Analysen und Reports?    Quantile Sie haben schon den Median $x_{0.5}$ als Lageparameter kennengelernt. Er teilt die geordnete Verteilung in zwei genau gleich große Teile. Allgemeiner lassen sich analog dazu die Quantile definieren: $x_{0.75}$ teil die geordnete Verteilung im Verhältnis 3:1. Das heißt, dass 75% der Beobachtungen kleiner als $x_{0.75}$ und 25% größer sind. Das $x_{0.25}$ Quantil teilt die Reihe im Verhältnis 1:3. Hier sind 25% der Beobachtungen kleiner und 75% größer.\ndf[\u0026#39;Total Checkouts\u0026#39;].quantile(q=[0.25, 0.5, 0.75])    Um Ausreißer in einer Variablen zu entfernen/ zu ersetzen, bietet es sich manchmal an, die größten (und/ oder kleinsten) $\\alpha\\%$ Beobachtungen zu identifizieren:\n# removes 1% of the data at both ends of the distribution alpha = 0.01 df[\u0026#39;Total Checkouts\u0026#39;].quantile([alpha/2, 1-alpha/2])    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/variance/",
	"title": "Streuungsmaße",
	"tags": [],
	"description": "",
	"content": "Die Zentralität einer Verteilung ist nur eine wichtige Kennzahl. Streuungsmaße geben zusätzlich an, wie stark die Daten einer Messreihe schwanken. Die Streuung einer Variable ist entscheidend z.B. bei der Beurteilung wie Wahrscheinlich extreme Werte vorkommen können.\nVarianz Die Distanz einer Beobachtung vom Mittelwert der zugrundeliegenden Variable wird Abweichung genannt. Der Mittelwert über die quadrierten Abweichungen wird als Varianz definiert:\n$$ s^2_x = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2 $$\ndf[\u0026#39;Total Checkouts\u0026#39;].var()    Das Quadrieren der Abweichungen hat zur Folge, dass das Vorzeichen verschwindet und das große Abweichungen mehr Gewicht erhalten. In der Formel wird durch $n-1$ anstatt durch $n$ geteilt. Dies ist theoretisch von Bedeutung, es hat aber in der Praxis meist keine Auswirkungen, wenn durch $n$ geteilt wird.\nEine geringe Varianz bedeutet, dass sich die Werte, die die Variable annehmen kann, nur geringfügig vom Mittelwert unterscheiden.\nStandardabweichung Die Standardabweichungen ist die Wurzel der Varianz: $$ s_x = \\sqrt{s_x^2} $$\ndf[\u0026#39;Total Checkouts\u0026#39;].std()    Variationskoeffizient Die absolute Größe der Varianz ist abhängig von deren Mittelwert. Ein Vergleich von Standardabweichungen verschiedener Variablen ist deswegen nicht sinnvoll. Möchte man die Streuung verschiedener Variablen vergleichen, macht es Sinn, eine normalisierte Größe, den Variationskoeffizient zu betrachten:\n$$ cv_x = \\frac{s_x}{\\bar{x}} $$\nWelche Variable streut mehr: 'Total Checkouts' oder 'Total Renewals'? Vergleichen Sie die Standardabweichungen und den Variationskoeffizient.\n  Spannweite Die Spannweite ist die Differenz zwischen dem maximalen und minmalem Wert\ndf[\u0026#39;Total Checkouts\u0026#39;].max() - df[\u0026#39;Total Checkouts\u0026#39;].min()    Interquartilsabstand Aus den Quantilen kann der Interquartilsabstand als robustes Streuungsmaß abgeleitet werden. Er ergibt sich aus der Differenz des 75%- zum 25%-Quantil: $$ x_{IQR} = x_{0.75} - x_{0.25} $$\ndf[\u0026#39;Total Checkouts\u0026#39;].quantile(q=0.75) - df[\u0026#39;Total Checkouts\u0026#39;].quantile(q=0.25)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/symmetrie/",
	"title": "Symmetrie und Schiefe",
	"tags": [],
	"description": "",
	"content": "  Verschiedene univariate Verteilungen     Related files   distributions.ipynb  (74 ko)    Um eine metrische Verteilung charakterisieren zu können, ist neben der zentralen Lage- und Streuung auch deren Symmetrie und Schiefe von Bedeutung. Die Symmetrie sagt etwas über die (Un-)Gleichverteilung einer Variablen aus. Bei stark assymetrischen Variablen (z.B. Haushaltseinkommen in Deutschland) ist das auftreten von kleinen Werten viel wahrscheinlicher, als das auftreten von sehr großen Werten (oder umgekehrt).\nDas Bild zeigt Histogramme für verschiedene simulierte Zufallswerte der Beta-Verteilung. Dabei wurden jeweils die Parameter der theoretischen Verteilung $\\alpha$ und $\\beta$ geändert. Somit kann eine große Bandbreite charakteristischer Verteilungen abgedeckt werden. Neben dem Histogram wurde auch der empirische Median und Mittelwert der Verteilung als vertikale Linien eingezeichnet.\nFür symmetrische Verteilungen gilt, dass der Mittelwert und der Median gleich sind und das Histogram an diesen Achsen gespiegelt werden kann. Eine linkssteile (rechtschiefe) Verteilung ergibt sich durch einige überdurchschnittlich große Werte. In diesem Fall ist der Mittelwert größer als der Median. Eine rechtssteile (linksschiefe) Verteilung ist durch einige unterdurchschnittlich kleine Werte geprägt. Hier ist der Median größer als der Mittelwert.\nZudem kann eine Verteilung auch Gleichverteilt, Bi- oder Multimodal sein. Im ersten Fall gibt es keinen Modus, also kein Wert der Verteilung, der am Häufigsten vorkommt. In den letzteren Fällen gibt es ein oder mehrere Modi. Im Histogram sind multimodale Verteilungen daran zu erkennen, dass sie typischerweise über zwei oder mehr \u0026ldquo;Gipfel\u0026rdquo; verfügen.\n Schauen Sie sich die verschiedenen Histogramme im Bild an und charakterisieren Sie jede einzelne Verteilung anhand von Schiefe, Symmetrie und dem Modus Die Grafik wurde mit dem angehängten Jupyter Notebook generiert. Hier können Sie auch selber andere Verteilungen simulieren und visualisieren.     Der Mittelwert und die Standardabweichung basieren auf den absoluten numerischen Werten der Beobachtungen. Deswegen können untypische sehr große oder sehr kleine Werte einer Verteilung (\u0026ldquo;Ausreißer\u0026rdquo;) diese Statistiken nach oben oder unten verzerren. Der Median und der Interquartilsabstand (IQR) hingegen basiert alleine auf der nach Größe sortierten Reihung der Beobachtungen und nicht auf den absoluten Werten. Deswegen sind diese Statistiken robust vor Ausreißern. Bei nicht-symmetrischen Verteilungen oder wenn Ausreißer vorliegen sollten deswegen immer auch robuste Statistiken mit angegeben werden. Ist die Verteilung Bi- oder Multimodal so können die Lagemaße Mittelwert und Median irreführend sein, da sie in der Regel nicht mit den \u0026ldquo;Gipfeln\u0026rdquo; der Verteilung (Modus) übereinstimmen.    Fallstudie: Verteilung der Ausleihen pro Kunde Mit dem folgenden Beispiel können Sie ein Histogram über die Anzahl der Ausleihen im Datensatz erstellen:\nimport pandas as pd import seaborn as sns %matplotlib inline sns.set() df = pd.read_csv(\u0026quot;../data/Library_Usage.csv\u0026quot;) sns.distplot(df['Total Checkouts'], kde=False)    Das Histogram zeigt, dass die Verteilung der Ausleihen durch einige sehr große Ausreißer geprägt ist. Der Mittelwert liegt hier bei $\\bar{x} = 162$, während der Median $x_{0.5} = 19$ sehr viel kleiner ist. Das 95%-Quantil liegt bei $x_{0.95} = 816$ Ausleihen. Das heißt das 95% der Beobachtungen im Datensatz weniger als 816 Ausleihen getätigt haben.\n Erstellen Sie einer neue Spalte 'Total Checkouts Sqrt' die die Wurzel über die Ausleihen enthält. Die Wurzel können sie mit df['Total Checkouts']**(0.5) berechnen. Schauen Sie sich das Histogram von 'Total Checkouts Sqrt' und charakterisieren Sie die Verteilung. Entfernen Sie nun die 5% größten Ausreißer aus dem Datensatz, indem Sie diesen nach Personen mit weniger als 816 Ausleihen filtern. Schauen Sie sich die Verteilung für 'Total Checkouts' des gefilterten Datensatzes in einem Histogram an und charakterisieren Sie diese. Welche Methode finden Sie besser geeignet, um mit fehlenden Werten umzugehen? Fallen Ihnen Vor- und Nachteile der jeweiligen Methoden ein?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/bivariate/",
	"title": "Bivariate Verteilungen",
	"tags": [],
	"description": "",
	"content": "Bisher haben Sie immer nur einzelne Variable betrachtet, zusammengefasst oder visualisiert. In vielen Fällen ist jedoch der Zusammenhang zwischen zwei Variablen von Interesse. Nach dieser Einheit sollten Sie die folgenden Fragen beantworten können:\n Leihen ältere Bibliothekskunden im Schnitt mehr Bücher aus als jüngere? Führen Kunden, die häufiger Ausleihen tätigen, im Schnitt auch häufiger Verlängerungen durch? Nimmt die Anzahl der Ausleihen mit zunehmender Dauer der Mitgliedschaft ab?  Zwei Variablen, die keinen Zusammenhang aufweisen, nennt man statistisch unabhängige Variablen. Für zwei metrische Variablen kann man außerdem zwischen einem positiven oder einem negativem Zusammenhang unterscheiden.\nBoxplot-Verteilung der Ausleihen nach Jahr der Registrierung.\n  Related files   boxplots.ipynb  (0 ko)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/",
	"title": "Praktische Einführung in Pandas",
	"tags": [],
	"description": "",
	"content": "Grundlage der statistischen Analyse sind Datentabellen: Jede Zeile der Tabelle entspricht einer Beobachtung. Jede Spalte entspricht einer statistischen Variable. Neue Beobachtungen und Variablen können dadurch einfach an die schon bestehende Tabelle angefügt werden.\nAchten Sie darauf, dass wenn es um Statistik und Programmierung geht mit \u0026ldquo;Variable\u0026rdquo; zwei Dinge gemeint sind:\n Variable im Kontext eines statistischen Merkmals, das in der Regel als Spalte eines Datensatzes vorliegt. Variablen im Kontext von Programmiersprachen beschreiben benannte Referenzen auf bestimmte Datenstrukturen oder Objekte (z.B. numbers = [1, 2, 3]).    Am Beginn jeder statistischen Analyse steht die Aufbereitung und Bereinigung der Daten. Damit ist die Behandlung von fehlenden oder falsch kodierten Werten, die Umkodierung und Transformation von statistischen Variablen oder die Berechnung neuer Spalten gemeint. Oft sind auch nur Untergruppen von Beobachtungen mit bestimmten Merkmausausprägungen von Interesse.\nViele statistische Methoden erfordern auch, dass die Daten nur als numerische Werte vorliegen. Daher müssen ordinale oder nominale Variablen, die als Text gespeichert sind (zum Beispiel ['male', 'female', 'female', ...]) in entsprechende numerische Werte umkodiert werden. Dabei wird jeder Kategorie ein numerischer Wert zugeordnet.\nDas Standard-Paket um mit Datentabellen in Python zu arbeiten, ist pandas. Das folgende Kapitel stellt anhand von vielen praktischen Beispielen zum Nachmachen die grundlegenden Konzepte in pandas vor. Ein Überblick über die Bibliothek und weitere relevante Python-Pakete gibt es hier.\n Erstellen Sie ein Jupyter Notebook in Ihrem Projektordner unter ./notebooks. Führen Sie die Beispiele in den folgenden Kurseinheiten im Notebook aus. Nach dieser Einheit sollten sie die folgenden Fragen beantworten können:  Wie kann ich Tabellendaten in pandas einlesen? Wie werden Daten in pandas angeordnet? Welche Datentypen können Spalten eines DataFrames annehmen? Wie kann ich einzelne Spalten oder Zeilen eines DataFrames auswählen?      Ihre Verzeichnisstruktur vom Projektordner sollte jetzt so aussehen:\ndata-librarian-3 ├── data │ └── Library_Usage.csv ├── notebooks │ ├── pandas_introduction.ipynb │ └── tutorial_jupyter.ipynb    What is Pandas? Introduction Video by Giles McMullen ( Untertitel auswählbar)    Dieses Cheat-Sheet gibt einen guten Überblick über die Datenverarbeitung mit Pandas.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/reflection/",
	"title": "Reflexion: Daten an Ihrem Arbeitsplatz",
	"tags": [],
	"description": "",
	"content": " Reflektion (15 Min) Schreiben Sie einen kurzen Text über die Verwendung von Daten und quantitativen Methoden an Ihrem Arbeitsplatz. Denken Sie dabei über folgende Fragen nach:\n Welche Daten sind bei Ihnen vorhanden? Mit welchen Daten arbeiten Sie oder würden Sie gerne arbeiten? Werden statistische Verfahren oder Maschinelles Lernen schon bei Ihnen eingesetzt? Welche Fragen oder Phänomene würden Sie gerne untersuchen? Was fänden Sie spannend herauszufinden?  Teilen Sie Ihren Text mit den anderen KursteilnehmerInnen im Forum zu Modul 3 auf der Kursplattform. Wenn Sie für die Anderen anonym bleiben möchten, können Sie mir auch den Text schicken und ich stelle ihn dann rein.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/visualizations/",
	"title": "Visualisierungen mit Seaborn",
	"tags": [],
	"description": "",
	"content": "    Im folgenden Abschnitt, wird ein Überblick über verschiedene Visualisierungsformen gegeben und anhand von Beispielen gezeigt, wie diese in Python mit der Bibliothek seaborn programmiert werden können.\nDas Thema Visualisierungen ist komplex: Es gibt sehr viele Parameter und Stellschrauben, die man auswendig lernen oder in den Dokumentationen der Bibliotheken pandas, seaborn und matplotlib Nachschlagen muss. Die Erstellung von eindrucksvollen und aussagekräftigen Graphiken erfordert viel Praxiserfahrung, Zeit und Mühe.\nAuf der anderen Seite lassen sich schon mit wenig Programmcode annehmbare Visualisierungen erstellen. Am besten orientieren Sie sich an den zahlreiche Beispielen und ändern den Code Ihren Erfordernissen an. Nach der Einheit sollten Sie die folgenden Fragen beantworten können:\n Wie können uni- und bivariate Verteilungen in Pandas visualisiert werden? Wie interpretiere und erstelle ich Boxplots, Histogramme und Streudiagramme?   Dieses Cheat-Sheet gibt einen guten Überblick über die Erstellung von Plots mit Seaborn.\n  Statistik bei FoxNews  Was fällt Ihnen an den folgenden Diagrammen von FoxNews auf? Was würden Sie anders machen? Welche Botschaft wollen die \u0026ldquo;Designer\u0026rdquo; vermitteln? Passt die Botschaft mit den Daten zusammen? (s. auch Quelle und Hintergründe, s. auch dieses Beispiel)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/visualizations/tutorial/",
	"title": "Tutorial",
	"tags": [],
	"description": "",
	"content": " Statistical analysis is a process of understanding how variables in a dataset relate to each other and how those relationships depend on other variables. Visualization can be a core component of this process because, when data are visualized properly, the human visual system can see trends and patterns that indicate a relationship. (https://seaborn.pydata.org/tutorial/relational.html)\n Die zahlreichen Funktionen, die seaborn bietet basieren immer auf dem gleichen Prinzip: Visualisiert werden (nominale, ordinale, metrische) Variablen eines Datensatzes, die in Form eines DataFrames vorliegt. Das Skalenniveau der Variablen bestimmt dabei die Art der Visualisierung. Variablen können verschiedenen Eigenschaften des Diagramms zugeordnet werden (z.B. die Punktgröße oder Farbe der Balken). Am besten lässt sich das Prinzip an Beispielen erkennen:\nGrundlagen Zuerst werden die benötigten Bibliotheken importiert und der Datensatz eingelesen. Panadas und numpy dienen zur Verarbeitung der Daten. Seaborn baut auf matplotlib, der Standard-Bibliothek für wissenschaftliche Grafiken, auf und bietete einige Verbesserungen und Vereinfachungen. Der Kommentar # matplotlib inline ist ein Magic Command für den Python Kernel in Jupyter Notebooks. Mit diesem Befehl werden Plots direkt im Notebook angezeigt.\nMit sns.set() wird das grundlegende Design der Plots dem Design von Seaborn angepasst. Die Funktion DataFrame.sample() wird verwendet, um eine Zufalsstichprobe der Größe $n=1000$ aus dem Datenstatz zu erstellen. Dies dient zur besseren Lesbarkeit des Plots in diesem Tutorial.\nimport pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np #matplotlib inline sns.set() # use sample to generate a random subsample df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;).sample(n=1000)    Der erste Plot sns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, data=df)    Die Funktion relplot() erzeugt ein Streudiagram zur Visualisierung einer bivariaten Verteilung mit metrischen Variablen. Jedes Wertepaar $(x_i,y_i)$ der Verteilung wird im Koordinatensystem als Punkt dargestellt. Die Variablen des Datensatzes werden über ihre Spaltennamen mit den Axen x und y des Plots verlinkt.\nPlotgröße Die Größe des Plots kann über die beiden Argumente height (Höhe in inches und aspect (Breite des Plots ergibt sich aus aspect * height) konfiguriert werden:\nsns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, data=df height=5, aspect=3)    Speichern Um den Plot als Bilddatei abzuspeichern wird die Funktion savefig() aus der matplotlib Bibliothek verwendet:\nplt.savefig(\u0026#39;../tutorial.png\u0026#39;, dpi=150)    Weitere Eigenschaften Mit den zusätzlichen Argumenten hue (Farbe der Punkte/ Linien/ Balken \u0026hellip;), size (Größe der Punkte/ Linien/ Balken \u0026hellip;), style können weitere Eigenschaften des Plots angepasst werden:\nsns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, hue=\u0026#39;Provided Email Address\u0026#39;, style=\u0026#39;Outside of County\u0026#39;, size=\u0026#39;Year Patron Registered\u0026#39;, data=df)    Die vollständige Liste aller Optionen kann hier eingesehen werden.\nPlot-Gitter Plots können mit dem row und/ oder col Argument auch anhand einer oder mehrerer Variablen in Form von Spalten und/ oder Zeilen angeordnet werden:\nsns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, size=\u0026#39;Year Patron Registered\u0026#39;, row=\u0026#39;Provided Email Address\u0026#39;, col=\u0026#39;Outside of County\u0026#39;, data=df)    Diese Form der Visualisierung ist in der Regel einem einzelnen Plot mit einer langen Legende vorzuziehen.\nFarbpaletten Mit dem Argument palette können verschiedene Farbpaletten für den hue Parameter ausgewählt werden. Zu empfehlen sind die Paletten von ColorBrewer:\nsns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, hue=\u0026#39;Provided Email Address\u0026#39;, col=\u0026#39;Outside of County\u0026#39;, palette=sns.color_palette(\u0026#39;Accent\u0026#39;, 2), data=df)    Mit der Funktion sns.color_palette können verschiedene Paletten anhand ihres Namens ausgewählt werden. Wichtig ist hierbei, die Anzahl der benötigten Farben mit anzugeben.\n"
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/visualizations/examples/",
	"title": "Weitere Beispiele",
	"tags": [],
	"description": "",
	"content": "Im Tutorial haben Sie gesehen, wie Sie ein Streudiagramm erstellen können. Hier werden exemplarisch weitere Möglichkeiten gezeigt, die Daten des Datensatzes zu visualisieren. Die wichtigste Funktion ist hierbei sns.catplot().\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np # matplotlib inline sns.set() df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) Nominale und ordinale Variablen Univariate Häufigkeits- und Bivariate Kreuztatabellen können mit Balkendiagrammen visualisiert werden:\nsns.catplot(y=\u0026#39;Year Patron Registered\u0026#39;, data=df, kind=\u0026#39;count\u0026#39;, color=\u0026#34;steelblue\u0026#34;) sns.catplot(y=\u0026#39;Age Range\u0026#39;, hue=\u0026#39;Provided Email Address\u0026#39;, data=df, kind=\u0026#39;count\u0026#39;) sns.catplot(x=\u0026#39;Patron Type Definition\u0026#39;, data=df, kind=\u0026#39;count\u0026#39;, col=\u0026#39;Year Patron Registered\u0026#39;, col_wrap=4) Metrische Variablen Univariate Verteilungen werden mit Histogrammen oder Kernel-Dichte Schätzern visualisiert:\n# Histogram sns.distplot(df[\u0026#39;Total Renewals\u0026#39;], kde=False) # With density estimation sns.distplot(df[\u0026#39;Total Renewals\u0026#39;], kde=True) Kombination aus metrischen und nominalen/ ordinalen Variablen # Swarmplot sns.catplot(x=\u0026#39;Year Patron Registered\u0026#39;, y = \u0026#39;Total Renewals\u0026#39;, data=df, kind=\u0026#39;swarm\u0026#39;, color=\u0026#34;steelblue\u0026#34;, aspect=4) # Boxplot sns.catplot(col=\u0026#39;Year Patron Registered\u0026#39;, y = \u0026#39;Total Renewals\u0026#39;, hue=\u0026#39;Provided Email Address\u0026#39;, data=df, kind=\u0026#39;box\u0026#39;, color=\u0026#34;steelblue\u0026#34;, aspect=4) "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/quiz_pandas/",
	"title": "Recap: Quiz",
	"tags": [],
	"description": "",
	"content": " .quiz fieldset { border-color: black; border-width: 10px; margin-bottom: 1em; } .quiz legend { font-size: 105%; font-weight: 600; padding-left: 15px; padding-right: 15px; padding-top: 15px; } .quiz label { display: block; line-height: 1.75em; } .quiz input[type=\"radio\"] { margin-right: 10px; page-break-after: avoid; page-break-before: avoid; } .quiz input[type=\"submit\"] { background: black; color: white; display: block; font-size: 120%; font-weight: 600; height: 2.5em; margin-top: 2em; text-transform: uppercase; width: 100%; } .quiz table { color: white; font-weight: bold; margin: 1em auto 2em auto; width: 100%; } .quiz td { padding: 5px 15px; text-align: left; width: 60px; } .quiz td.missing-label, .quiz td.missing-score { background: #CECBC2; } .quiz td.right-label, .quiz td.right-score { background: #74b559; } .quiz td.wrong-label, .quiz td.wrong-score { background: #D01F3C; }    var choices = \"read_excel,to_excel,load_excel,from_excel\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wie lautet der Name der Pandas Funktion, mit der Excel-Dateien eingelesen werden können?\"; var answer = 1 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"df.rows,len(df),df.shape[1],df.size\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Mit welchem Befehl kann die Anzahl der Zeilen aus einem DataFrame auslesen?\"; var answer = 2 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"40,423448,0,215\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wie viele fehlende Werte enthält die \\x27Age Range\\x27 Variable des Datensatzes?\"; var answer = 4 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"263544,7797,159904,12003\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wie viele Beobachtungen zwischen 60 und 64 Jahren waren zuletzt im Jahr 2016 aktive Kunden der Bibliothek?\"; var answer = 2 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var quiz = new Quiz(\"pandas_quiz\", questions, {\"shuffle\": true});   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/quiz_statistics/",
	"title": "Recap: Quiz",
	"tags": [],
	"description": "",
	"content": " .quiz fieldset { border-color: black; border-width: 10px; margin-bottom: 1em; } .quiz legend { font-size: 105%; font-weight: 600; padding-left: 15px; padding-right: 15px; padding-top: 15px; } .quiz label { display: block; line-height: 1.75em; } .quiz input[type=\"radio\"] { margin-right: 10px; page-break-after: avoid; page-break-before: avoid; } .quiz input[type=\"submit\"] { background: black; color: white; display: block; font-size: 120%; font-weight: 600; height: 2.5em; margin-top: 2em; text-transform: uppercase; width: 100%; } .quiz table { color: white; font-weight: bold; margin: 1em auto 2em auto; width: 100%; } .quiz td { padding: 5px 15px; text-align: left; width: 60px; } .quiz td.missing-label, .quiz td.missing-score { background: #CECBC2; } .quiz td.right-label, .quiz td.right-score { background: #74b559; } .quiz td.wrong-label, .quiz td.wrong-score { background: #D01F3C; }    var choices = \"rechtsschief,linkschief,rechtssteil,symmetrisch\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wenn der Median einer Verteilung kleiner als das Arithmetische Mittel ist, dann ist die Verteilung in der Regel...\"; var answer = 1 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"71,609,23,2\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wie viel Prozent der unter 10 Jährigen hat eine Email Adresse angeben?\"; var answer = 1 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"0.60,9.0,36.0,6.0\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wie lautet das 60% Quantil der Spalte \\x27Total Checkouts\\x27?\"; var answer = 3 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"'25 to 34 years',91083,423233,'75 years and over'\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Was ist der Modalwert der Spalte \\x27Age Range\\x27?\"; var answer = 1 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"111, 8965, 63, 27\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wie groß ist der IQR für die Anzahl der Verlängerungen?\"; var answer = 4 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var quiz = new Quiz(\"pandas_quiz\", questions, {\"shuffle\": true});   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/inference/bootstrap/",
	"title": "Das Bootstrapping Verfahren",
	"tags": [],
	"description": "",
	"content": "Das Ziel der Inferenzstatistik ist es, aus einer einzelnen Stichprobe $x_1, \\dots, x_n$ die Stichproben-Verteilung eines Schätzers, wie dem Mittelwert $\\bar{x}$ oder dem Median $x_{0.5}$, herzuleiten. Wenn die Stichproben-Verteilung eines Schätzers vorliegt kann damit der Wert des tatsächlichen unbekannten Populationsparameters eingegrenzt werden.\nFür viele Schätzer kann deren Stichproben-Verteilug theoretisch hergeleitet werden. Neben der theoretischen Herangehensweise, gibt es auch eine intuitive empirische Methode, das Bootstrapping-Verfahren. Es basiert auf der Simulation von vielen Stichproben. Simulation bedeutet, dass die Stichproben nicht real erhoben, sondern alle aus der einzigen vorhanden Stichprobe erstellt werden.\nEine einzelne Bootstrapping-Stichprobe erhält man, indem aus der vorhanden Stichprobe der Größe $n$, genau $n$ Beobachtungen mit Zurücklegen zufällig gezogen werden. Das bedeutet, dass Beobachtungen mehrmals in der simulierten Stichprobe vorkommen können.\nBeispiel Nehmen Sie an, dass die Stichprobe die folgenden $n=7$ Werte enthält:\nimport pandas as pd x = pd.Series([21, 13, 8, 14, 10, 12, 5]) x.mean() Eine simulierte Bootstrapping-Stichprobe erhalten Sie indem Sie aus der vorhandenen Stichprobe genau $n=7$ Werte mit Zurücklegen (replace=True) zufällig auswählen:\nx.sample(n=len(x), replace=True)    Für jede simulierte Stichprobe wird daraufhin der zu interessierende Schätzerwert berechnet. Um möglichst exakte Ergebnisse zu erhalten sollten mindestens $S \\geq 5000$ Simulationen durchgeführt werden. Man erhält damit eine Ännäherung an die tatsächliche Stichprobenverteilung des Schätzwerts:\nBeispiel (Fortsetzung) Wir erstellen eine Bootstrapping-Verteilung für den Stichproben-Mittelwert. Die Anzahl der Simuluationen wird auf $S=10000$ festgelegt. Mit einer For-Loop Schleife wird die Simulation wiederholt. In jeder Simulation wird eine Bootstrapping-Stichprobe erstellt und deren Mittelwert berechnet.\nx_means = [] S=10000 for i in range(S): x_mean = x.sample(n=len(x), replace=True).mean() x_means.append(x_mean) Die Mittelwerte jeder Simulation werden in der Liste x_means abgespeichert. Die Liste enthält nun eine empirische Stichprobenverteilung des Mittelwerts. Nun können Sie sich die Verteilung des Stichproben-Mittelwertes beispielsweise in einem Histogram ansehen:\n#matplotlib inline import seaborn as sns sns.set() sns.distplot(x_means, kde=False, bins=35) Wie viele Mittelwerte liegen zwischen 9 und 11?\nx_means = pd.Series(x_means) x_means.between(9,11).mean()     Lesen Sie den Datensatz ein Um eine homogene Stichprobe zu erhalten filtern Sie nach Bibliothekskunden die sich im Jahr 2010 registriert haben und auch noch im Jahr 2016 (als der Datensatz erstellt wurde) aktiv waren. Achtung: Die Spalte 'Circulation Active Year' wird standardmäßig als Text eingelesen. Betrachten Sie die Variable 'Total Renewals'. Wie viele Verlängerungen wurden im Mittel durchgeführt? Erstellen Sie, wie oben beschrieben, eine Stichprobenverteilung für den Mittelwert. Wie viel Prozent der Stichproben-Mittelwerte liegen zwischen 89 und 92 Verlängerungen? Wie groß müssen Sie das Intervall wählen, sodass 90% aller Bootstrapping-Mittelwerte darin liegen (Tip: Nutzen Sie die Funktion pandas.Series.quantile)?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/pandas/",
	"title": "pandas",
	"tags": [],
	"description": "Bearbeitung, Transformation, Aggregation und Zusammenfassung von Datensätzen. Baut auf numpy auf.",
	"content": "pandas baut auf numpy auf und vereinfacht stark die Bearbeitung, Transformation, Aggregation und Zusammenfassung von zweidimensionalen Datensätzen sowie deren Import und Export in Python. Die zentralen Datenstrukturen in pandas sind Series und DataFrame.\nSeries sind eindimensionale Listen eines Datentypes, ähnlich wie arrays in numpy. Datentypen können ganzzahlige Zahlen (int), binäre Werte vom Typ true oder false (bool), Strings (str) oder reale Zahlen (float) sein.\nIn einem DataFrame werden mehrere Series gleicher Länge spaltenweise zu einer zweidimensionalen Tabelle (wie einer Excel Tabelle) zusammengefasst. Ein DataFrame besitzt außerdem auch immer Spalten- und Zeilennamen.\nWie auch numpy, bietet pandas darüber hinaus viele Funktionen aus der Statistik, zum Beschreiben von Daten. Eine Übersicht gibt es hier.\n# import the library and give it a shorter name \u0026#39;pd\u0026#39; import pandas as pd # create a dataframe by hand with two columns and three rows df = pd.DataFrame({ \u0026#39;month\u0026#39;: [1, 2, 3], \u0026#39;temperatur\u0026#39;: [-12, 3, 9] }) # print out some descriptive statistics df.describe()    Pandas (15 Min)  Kopieren Sie das Codebeispiel in ein Jupyter Notebook und führen Sie es aus. Fügen Sie weitere Temperatur und Monats-Werte dem DataFrame hinzu. Welche Statistiken liefert ein Aufruf der Funktion describe()?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/matplotlib/",
	"title": "matplotlib",
	"tags": [],
	"description": "Bietet 2D Plotting Funktionalitäten.",
	"content": "matplotlib ist das Standard-Paket zum Erstellen von wissenschaftlichen 2-dimensionalen statischen Graphiken. Die grundlegende Struktur in matplotlib ist figure, eine leere graphische Fläche, die mit Linien, Balken, Punten, Beschriftungen und Axen befüllt werden kann. Der fertige Plot kann dann in diversen Formaten abgespeichert oder auf dem Bildschirm angezeigt werden.\n# import the package and give it the shorter name \u0026#39;plt\u0026#39; # matplotlib inline import matplotlib.pyplot as plt # create some dummy data x = range(1, 10) # make a simple scatter plot of the data plt.plot(x, x, c=\u0026#34;green\u0026#34;, linestyle=\u0026#39;\u0026#39;, marker=\u0026#39;+\u0026#39;)    matplotlib (15 Min)  Kopieren Sie den Code in ein Jupyter Notebook. Ändern Sie die Farbe der Pukte im Plot von grün auf schwarz. Ändern Sie den Aufruf so um, dass statt Punkte, Linien angezeigt werden. Hier finden Sie die Dokumentation der Funktion matplotlib.pyplot.plot.    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/inference/significance/",
	"title": "Konfidenzintervalle und Signifikanz",
	"tags": [],
	"description": "",
	"content": "Im vorherigen Beispiel haben Sie mit Hilfe des Bootstrapping-Verfahrens die Stichprobenverteilung geschätzt. Wenn die Stichprobenverteilung bekannt ist, können damit Aussagen über den tatsächlichen Parameter in der Population (im Bild mit $\\mu$ bezeichnet) getroffen werden.\nEine häufig angewandte Methode sind Konfidenzintervalle (KI). Sie geben einen Bereich aus der Stichprobenverteilung des Schätzwertes an, der den wahren Wert in der Population mit hoher Wahrscheinlichkeit überdeckt. Die Wahrscheinlichkeit wird mit $1-\\alpha$ angegeben. Der Wert $\\alpha$ wird Signifikanzniveau gennannt und vor der Bestimmung des Intervalls festgelegt. Üblicherweise wird $\\alpha=0.10$, $\\alpha=0.05$, oder $\\alpha=0.01$ gesetzt.\nEin breites Konfidenzintervall zeigt auf, dass der Populationsparameter nur sehr ungenau bestimmt werden kann. Mit größerer Stichprobenvergröße verkleinert sich in der Regel der Stichprobenfehler und damit auch das Intervall: Es können präzisere Aussagen über die Population getroffen werden.\nMit einem niedrigeren Signifikanzniveau $\\alpha$ kann sichergestellt werden, dass das KI den wahren Wert mit höherer Wahrscheinlichkeit überdeckt. Diese geringere Fehlertoleranz hat jedoch ein breiteres und damit weniger präzises Interval zur Folge.\nDas Konfidenzintervall kann aus der mit dem Bootstraping-Verfahren geschätzten Stichprobenverteilung ermittelt werden: Es entspricht genau den entsprechenden Quantilen der geschätzten Stichprobenverteilung: Soll beispielsweise ein 90%-KI zum Signifikanzniveau von $\\alpha=0.10$ erstellt werden, so lässt sich die untere Grenze aus der Verteilung als $\\bar{x}_\\frac{\\alpha}{2} = \\bar{x}_{0.05}$ ablesen. Die obere Grenze als $\\bar{x}_{1-\\frac{\\alpha}{2}} = \\bar{x}_{0.95}$. Somit ist sichergestellt, dass 90% aller mit dem Bootstrapping Verfahren ermittelter Stichprobenmittelwerte innerhalb dieses Intervals liegen. Damit überdeckt das KI mit 90% Wahrscheinlichkeit den wahren Populationsparameter $\\mu$.\nMit dem Bootstrapping Verfahren können Sie nicht nur Konfidenzintervalle für den Mittelwert angeben, sondern auch für viele weitere Statistiken, wie den Median oder den Korrelationskoeffizienten zwischen zwei Variablen.\nBeispiel (Fortsetzung) Die Stichprobenmittelwerte aus der Bootstraping-Simulation werden zuerst in einer Series umgewandelt. Mit der Funktion quantile können die Quantile der Stichprobenverteilung bestimmt werden. Sie entsprechen dem geschätzten Konfidenzinterval zum Signifikanzniveau $\\alpha$.\nx_means = pd.Series(x_means) alpha = 0.10 x_means.quantile((alpha/2,1-alpha/2))    Berechnen Sie ein 90%-Konfidenzintervall jeweils für den Mittelwert und den Median der Variablen Total Renewals und `Total Checkouts\u0026rdquo;.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/seaborn/",
	"title": "seaborn",
	"tags": [],
	"description": "Verbesserung und Weiterentwicklung der matplotlib Bibliothek.",
	"content": "seaborn baut auf matplotlib auf und bietet eine Vielzahl von Funktionen, die es erlauben schnell und einfach schöne statistische Visualisierungen zu erstellen. Seaborn ist also keine komplett eigenenständige Graphik-Bibliothek, sondern nutzt intern die Funktionalitäten und Datenstrukturen von matplotlib.\nEine wichtige Funktion ist die sns.set() Methode. Wenn sie am Anfang eines Python-Scripts ausgeführt wird, wird intern das Design der Plots erheblich verbessert. Alle plots, die nach dem Aufruf der Funktion erstellt werden, sehen viel besser aus.\nTesten Sie den Unterschied mit dem folgenden Beispiel:\n# import the libraries and give them some shorter names import matplotlib.pyplot as plt import seaborn as sns # setup the seaborn library sns.set() # create the same plot as in the previous example x = range(1, 10) plt.plot(x, x)    Wenn Sie im Jupyter Notebook das Code-Beispiel ausgeführt haben und danach den Aufruf sns.set() entfernen, ändert sich das Design des Plots erstmal nicht. Für einen \u0026ldquo;Reset\u0026rdquo; müssen Sie den Kernel (also der im Hintergrund laufende Python Prozess) mit einem Klick auf  neu starten.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/inference/two-sample-test/",
	"title": "Mittelwertvergleiche",
	"tags": [],
	"description": "",
	"content": "Ein häufiges Problem bei der statistischen Datenanalyse ist die Frage, ob signifikante Unterschiede in den Mittelwerten zweier Subpopulationen bestehen: Leihen Frauen beispielsweise im Mittel signifikant mehr aus als männliche Bibliothekskunden? Tätigen Kunden im Ruhestand im Mittel weniger Verlängerungen als junge Kunden?\nVon einem signifikanten Unterschied spricht man, wenn die Differenz zwischen den Mittelwerten zweier Stichproben so groß ist, dass es sehr Unwahrscheinlich ist, dass dieser Unterschied alleine aufgrund der rein zufälligen Schwankungen durch die Stichprobenziehung enstanden ist.\nWenn wir mit $\\mu_x$ und $\\mu_y$ die wahren aber unbekannten Mittelwerte in der Population bezeichnen und mit $\\Delta_{xy}$ die Differenz dieser Mittelwerte, dann lautet unsere **Hypothese**:\n$$ H_0: \\mu_x = \\mu_y \\iff \\Delta_{xy} = \\mu_x - \\mu_y = 0 $$\nWenn diese Hypothese zutrifft, dann gibt es keine Unterschiede in den Mittelwerten der beiden Populationen. Wie lässt sich diese Hypothese nun anhand von zwei Stichproben $x = x_1, \\dots, x_{n_x}$ und $y = y_1, \\dots, y_{n_y}$ und deren Mittelwerten $\\bar{x}$ und $\\bar{y}$ statistisch überprüfen?\nWir können das bisherige Bootstrapping-Verfahren auch hier anwenden, um die Stichprobenverteilung von $d_{xy} = \\bar{x}-\\bar{y}$ zu schätzen. In jeder Simulation $s = 1, \\dots, S$ erstellen wir eine Bootstrapping-Stichprobe von $x$ und $y$ und berechnen darauf die Differenz $d_{xy}$ über die Mittelwerte.\nBeachten Sie, dass sich das grundlegende Verfahren und die Bestimmung der Konfidenzintervalle im Vergleich zur vorherigen Lektion nicht ändert. Einzig der zu interessierende Schätzwert wird ausgetauscht: Wo vorher nur die Stichprobenverteilung von $\\bar{x}$ bestimmt wurde, ist es nun die Verteilung von $d_{xy} = \\bar{x}-\\bar{y}$.\n  Zum Signifikanzniveau $\\alpha=0.05$ erhalten wir mit den Quantilen $d_{\\frac{\\alpha}{2}}$ und $d_{1-\\frac{\\alpha}{2}}$ das 95%-Konfidenzintervall für die Mittelwertdifferenz. Mit einer Wahrscheinlichkeit von $1-\\alpha = 95\\%$ überdeckt das Konfidenzintervall damit den unbekannten Populationsparameter $\\Delta_{xy} = \\mu_x - \\mu_y$.\nLiegt das Konfidenz-Intervall für die Differenz zweier Mittelwerte ausschließlich im positiven oder negativem Bereich (also außerhalb des Nullpunktes), können wir daraus schließen, dass die beiden Stichproben mit hoher Evidenz unterschiedlichen Populationen mit verschiedenen Mittelwerten entstammen. Die aufgestellte Hypothese kann deswegen abgelehnt werden.\n  import pandas as pd alpha = 0.05 S=10000 x = pd.Series([3, 3, 5, 8, 7, 3, 2, 5, 8, 1]) y = pd.Series([3, 10, 9, 8, 2, 3, 6, 7, 11, 6]) print((x.mean(), y.mean())) dxy = [] for i in range(S): x_mean = x.sample(n=len(x), replace=True).mean() y_mean = y.sample(n=len(y), replace=True).mean() dxy.append(x_mean - y_mean ) pd.Series(dxy).quantile((alpha/2, 1-alpha/2))    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/scipy/",
	"title": "scipy",
	"tags": [],
	"description": "Funktionen und Methoden aus der Statistik.",
	"content": "scipy ist fest mit numpy und pandas verbunden und bietet eine Menge an Funktionen und Methoden aus der Mathematik und Statistik an.\nFür uns ist vor alle das Paket scipy.stats Interessant. Mit ihm können Zufallszahlen aus verschiedensten statistischen Verteilungen generiert werden oder auch statistische Tests durchgeführt werden. Hier finden Sie einen Überblick über alle Methoden des Pakets.\nIm folgenden Beispiel wird ein Zweistichproben-t-Test an zwei numerischen Listen durchgeführt.\n# import the package stats from the library scipy from scipy import stats # create two numerical arrays x = [12, 10, 11, 13, 14, 10, 13, 13, 22] y = [1, 4, 2, 3, 5, 2, 1, 0, 0, 1, 2] # perform a two sample t-test, to test if the samples have different means stats.ttest_ind(x,y)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/scikitlearn/",
	"title": "scitkit-learn",
	"tags": [],
	"description": "Bietet Funktionen und Methoden für maschinelles Lernen.",
	"content": "scikit-learn ist eine umfangreiche Bibliothek für maschinelles Lernen in Python. Es bietet eine Vielzahl an verschiedenen Algorithmen, mit denen zum Beispiel Vorhersagen oder Bilderkennung durchgeführt werden können.\n  Faces recognition example using eigenfaces and SVMshttps://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py\n  # import the packages import numpy as np from sklearn.linear_model import LinearRegression # create some dummy dependent and independent variable X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) y = - 1 * X[:,0] + 2 * X[:,1] # estimate a linear regression and print out the coefficients reg = LinearRegression().fit(X, y) reg.coef_    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/",
	"title": "Grundlagen",
	"tags": [],
	"description": "",
	"content": "27.01 – 02.02 Grundlagen der Datenanalyse in Python Diese Einheit gibt eine Einführung in die Aufgaben und grundlegenden Begriffe der angewandten Statistik. Im zweiten Teil wird das pandas Paket vorgestellt und gezeigt, wie Datensätze eingelesen und bearbeitet werden können.\nZiele  Beschreiben Sie Datensätze mit dem statistischen Grundvokabular Lesen Sie Datensätze als DataFrames in Python ein und aus Filtern Sie DataFrames nach Spalten oder Zeilen Erstellen Sie neue Variablen Projektaufgabe Die Pressestelle der San Francisco Public Library möchte einen Online-Artikel zum Kundenstamm der Bibliothek erstellen. Dazu hat sie Ihnen einen Datensatz geschickt, den Sie auswerten sollen.\n Erstellen Sie eine Beschreibung des Datensatzes unter Verwendung des statistischen Grundvokabulars. Lesen Sie den Datensatz ein ein Bereinigen Sie den Datensatz von fehlenden Werten und berechnen Sie die neue Variable Membership Duration.      "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/",
	"title": "Deskriptive Statistik und Visualisierungen",
	"tags": [],
	"description": "",
	"content": "03.02 – 09.02 Deskriptive Statistik II und Visualisierungen Dieses Modul gibt eine Einführung in die deskriptive Statistik mit pandas und zeigt, wie statistische Visualisierungen in Python erstellt werden können.\nZiele  Berechnen und interpretieren Sie grundlegende Lage- und Streuungsmaße Beschreiben Sie univariate stetige und diskrete Verteilungen Beschreiben und Berechnen Sie Statistiken für stetige und diskrete bivariate Verteilungen Erstellen Sie einfache Visualisierungen  Projektaufgabe Für den Online-Artikel zum Kundenstamm der Bibliothek braucht die Pressestelle einige interessanten Zahlen zum Thema Alter und Bibliotheksnutzung. Außerdem möchte sie die Daten in einer Info-Graphik zusammenstellen.\nFür eine erste Demo sind Sie verantwortlich.\n Berechnen Sie 2-3 Statistiken und Erstellen Sie 2-3 Visualisierungen basierend auf den Informationen im Datensatz. Nutzen Sie pandas zur Berechnung der Statistiken und seaborn für die Visualisierungen.  Schicken Sie bis spätestens zum Projekttag Ihren Report in Form eines integrierten Python Notebooks an malte@bonart.de.\n  Beispielfragen, die Sie mit dem Datensatz beantworten und visualisieren können:\n Wie viele Senioren und Kinder sind Kunden der San Francisco Public Library? Wie viele Nutzer möchten per Mail informiert werden? Wie alt sind diese Nutzer durchschnittlich im Vergleich zu Nutzern, die per Post informiert werden möchten? Wie viele Ausleihen werden im Mittel pro Altersgruppe und pro Jahr getätigt? Ist die Streuung zwischen den Gruppen gleich?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/inference/",
	"title": "Inferenzstatistik",
	"tags": [],
	"description": "",
	"content": "10.02 – 16.02 Ausblick: Grundlagen der Inferenzstatistik Diese Einheit gibt einen ersten Einblick in die angewandte Inferenzstatistik mit dem Bootstrapping-Verfahren und stellt die Berechnung von Konfidenzintervallen für den Mittelwert und Median in Python vor. Die Inhalte und Aufgaben dieser Einheit sind anspruchsvoller als bisher. Wenn Sie Probleme mit der Lösung der Projektaufgabe haben, können wir diese am Präsenztag gemeinsam besprechen.\nZiele  Testen Sie, ob signifikante Unterschiede zwischen den Mittelwerten zweier Sub-Populationen bestehen.  Projektaufgabe Unterscheidet sich das durchschnittliche Ausleihverhalten von jungen und älteren Bibliotheksnutzern signifikant voneinander? Gehen Sie zur Beantwortung der Frage die folgenden Schritte durch:\n Lesen Sie den Datensatz ein Um eine homogene Stichprobe zu erhalten filtern Sie nach Bibliothekskunden die sich im Jahr 2010 registriert haben und auch noch im Jahr 2016 (als der Datensatz erstellt wurde) aktiv waren. Erstellen Sie jeweils eine Series der Total Checkouts für zwei Sub-Population: Betrachten Sie jeweils YOUNG ADULTs und SENIORs aus der Variable Patron Type Definition.  Wie viele Beobachtungen sind jeweils in den beiden Populationen? Was sind deren Mittelwerte (Median, arithmetisches Mittel)? Was ist Ihre Vermutung? Bestehen signifikante Unterschiede in den Mittelwerten dieser beiden Gruppen?   Berechnen Sie das Konfidenzinterval für die Differenz der Mittelwerte (Mediane) mit dem Bootstrapping Verfahren! Wie interpretieren Sie das Ergebnis? Unterscheiden sich die Mittelwerte signifikant voneinander? Ändert sich das Ergebniss, wenn Sie das Signifikanzniveau $\\alpha$ ändern?  Schicken Sie bis spätestens zum Projekttag Ihren Report in Form eines integrierten Python Notebooks an malte@bonart.de.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://bonartm.github.io/data-librarian/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]