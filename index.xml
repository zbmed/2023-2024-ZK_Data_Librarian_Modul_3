<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Modul 3 on Zertifikatskurs Data Librarian - Modul 3 - Daten analysieren und darstellen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/</link><description>Recent content in Modul 3 on Zertifikatskurs Data Librarian - Modul 3 - Daten analysieren und darstellen</description><generator>Hugo -- gohugo.io</generator><language>de-DE</language><atom:link href="https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/index.xml" rel="self" type="application/rss+xml"/><item><title>Conda und Anaconda</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/anaconda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/anaconda/</guid><description>Conda ist eine freie und offene Softwarepaketverwaltung für Python. Neben der Möglichkeit, Pakete (packages, libraries) für Python aus dem Internet zu installieren, können mit conda virtuelle Umgebungen (environments) angelegt werden. Diese Umgebungen beinhalten nur die Pakete und Python Versionen, die für ein spezifisches Projekt gebraucht werden. Umgebungen können mit anderen Personen geteilt werden, sodass sichergestellt ist, dass alle Programmierer mit den gleichen Paketen und Versionen arbeiten, auch wenn sie unterschiedliche Systeme (Windows, Linux, MacOS) verwenden.</description></item><item><title>Häufigkeiten</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/frequency/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/frequency/</guid><description>Kategoriale Variablen werden in Häufigkeitstabellen zusammengefasst. Dabei wird für jede Ausprägung die Anzahl der Beobachtungen gezählt:
import pandas as pd df = pd.read_csv(&amp;#34;../data/Library_Usage.csv&amp;#34;) df[&amp;#39;Age Range&amp;#39;].value_counts() Mit der Funktion value_counts() kann man sich absolute Häufigkeitstabellen ausgeben lassen. Mit dem zusätzlichen Argumentaufruf normalize=True werden relative Häufigkeiten berechnet:
df[&amp;#39;Age Range&amp;#39;].value_counts(normalize=True) Der Modus sind dabei die Merkmalsausprägungen, die die meisten Beobachtungen besitzen: age_mode = df[&amp;#39;Age Range&amp;#39;].mode() age_mode[0] 3.1 Häufigkeiten (20 Min) Erstelle eine Häufigkeitsverteilung für die Variable 'Year Patron Registered'.</description></item><item><title>Lösungen zu den Kursaufgaben</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/solutions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/solutions/</guid><description>Hier finden sich die Musterlösungen aller Einheiten im Überblick.
1. Kursorganisation und Vorbereitung 2. Grundlagen der Datenanalyse in Python 3. Deskriptive Statistik und Visualisierungen 4. Exkurs: Inferenzstatistik 6. Maschinelles Lernen - Praxis mit scikit-Learn 7. Maschinelles Lernen - Automatische Textanalyse</description></item><item><title>Series und DataFrames</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/series/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/series/</guid><description>Series und DataFrames sind die zentralen Datenstrukturen in Pandas. Series sind wie standardmäßige Listen in Python, mit dem wichtigen Unterschied, dass Series nur Werte eines einzelnen Datentyps enthalten können.
import pandas as pd x = pd.Series([34, 12, 23, 45]) print(x) x.dtype Ein Datentyp ist die grundlegende Einheit, in der einzelne Werte in einer Programmiersprache vom Computer gespeichert und verarbeitet werden können. Beispiele für Datentypen in pandas sind: float für Gleitkommazahlen, int für Ganzzahlen, bool für binäre True, False Werte oder datetime für Datumswerte.</description></item><item><title>Statistik</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/statistic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/statistic/</guid><description>Justus Perthes (1838): Rhein, Elbe und Oder Statistik ist die traditionelle Wissenschaft von der Erhebung und Analyse von Daten. Sie verfügt über eine großes theoretisches und mathematisches Fundament und lässt sich in die Teilgebiete deskriptive (Beschreiben), explorative (Suchen) und schließende (Induktion) Statistik unterteilen.
Im Kapitel Grundbegriffe der Statistik wirst Du mehr über die Grundlagen der Statistik nachschlagen können.</description></item><item><title>Statistische Inferenz</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/overview/</guid><description>Bisher haben wir vorliegende Daten einer Stichprobe mit Visualisierungen und Statistiken beschrieben und zusammengefasst. Von Interesse sind aber in der Regel die Zusammenhänge und Statistiken in der Gesamtpopulation.
Beispiel Wahlumfrage: Es werden zufällig $n=100$ Personen aus dem Wahlregister gezogen und nach nach ihren Parteipräferenzen befragt. Man kann dann beispielsweise den relativen Anteil der Personen in der Stichprobe, die eine bestimmte Partei favorisieren, bestimmen. Damit hat man einen Schätzwert für den tatsächlichen Wert, wenn man alle Personen des Wahlregisters befragt hätte.</description></item><item><title>Ein- und Ausgabe</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/io/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/io/</guid><description>Die Funktionen zur Ein- und Ausgabe von Daten in pandas sind umfangreich aber systematisch organisiert. Um beispielsweise eine .csv Datei einzulesen und in einer Variable zu speichern verwendet man die Funktion read_csv:
import pandas as pd df = pd.read_csv(&amp;#34;../data/Library_Usage.csv&amp;#34;) df.head() Um einen eingelesenen Datensatz beispielsweise im .json Textformat zu speichern verwendet man die Funktion to_json:
df.to_json(&amp;#34;../data/Library_Usage.json&amp;#34;) Manche Funktion aus dem pandas Paket sind statische Funktionen: Sie sind an kein konkretes Objekt gebunden, sondern werden über den Bibliotheksnamen pd aufgerufen.</description></item><item><title>Jupyter Notebooks</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/notebooks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/notebooks/</guid><description>Hier findet sich eine kleine Erinnerung und Beispielaufgabe zum Thema Jupyter Notebooks.
Die Projektaufgaben und Code-Beispiele in diesem Modul werden über Jupyter Notebooks erstellt und verteilt.
Jupyter Notebooks bieten eine browserbasierte graphische Schnittstelle zur Python Programmierumgebung. Deswegen können Notebooks auf jedem System gestartet werden, man benötigt dazu nur einen Web-Browser und eine lokale installierte Version von Python.
Darüber hinaus bieten Notebooks die Möglichkeit Text, Visualisierungen und Code in einer integrierten Datei zu erstellen.</description></item><item><title>Machine Learning</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/ml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/ml/</guid><description>Seit der Erfindung des Personal Computers und des Internets werden statistische Probleme immer komplexer und größer. Die Datenmengen erfordern neue effiziente Strukturen zum Speichern und Auffinden der Informationen.
Maschinelles Lernen (Machine Learning oder Statistical Learning) bedeutet in diesem Kontext relevante und signifikante Muster und Trends aus den Daten zu extrahieren um die Daten &amp;ldquo;zu verstehen&amp;rdquo;. Dabei spielen Computer und deren wachsende Rechenpower eine immer größere Rolle. Sie haben die klassische angewandte Statistik revolutioniert und es sind vor allem Ingenieure und Informatiker, die die Weiterentwicklung der Disziplin heutzutage vorantreiben.</description></item><item><title>Auswahl und Erstellung von Spalten</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/columns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/columns/</guid><description>Die Spalten eines DataFrames werden über einen Spaltenindex referenziert. Üblicherweise besteht der Spaltenindex aus Spaltennamen in Textform:
import pandas as pd df = pd.read_csv(&amp;#34;../data/Library_Usage.csv&amp;#34;) df.columns Einzelne Series können wie bei einem Python Dictionary mit df[&amp;lt;name&amp;gt;] extrahiert werden. Mehre Spalten mit df[[&amp;lt;name1&amp;gt;, &amp;lt;name2]]. Wenn Du Spalten mit der doppelten Liste [[...]] auswählst, erhältst Du in jedem Fall wieder einen DataFrame zurück. Das Ergebnis der Auswahl kannst Du bei Bedarf wieder in einer Variablen abspeichern:</description></item><item><title>Data Science</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/data-science/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/data-science/</guid><description>Ein Data Scientist ist eine Person, die oder der Wissen und Erkenntnisse aus strukturierten und unstrukturierten Daten gewinnt. Data Science ist eine interdisziplinäre Disziplin und liegt irgendwo in der Schnittmenge von angewandter Statistik, angewandter Informatik (Hacking skills) und spezielles Fachwissen (domain knowledge / substantive expertise).
Aufgrund der stark angewachsenen Mengen an unstrukturierten Daten aus heterogenen Datenquellen (Text, Bilder, Sensoren, Netzwerke, Videos, &amp;hellip;) reichen die Methoden und Fertigkeiten, die die Statistik traditionellerweise liefert und vermittelt, nicht mehr aus, um diese Daten effizient zu strukturieren, aggregieren, kombinieren, analysieren und visualisieren zu können:</description></item><item><title>Auswahl von Zeilen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/rows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/rows/</guid><description>Die Zeilen eines DataFrames können über drei verschiedene Arten ausgewählt werden. Das System kann am Anfang etwas verwirrend sein. Wir betrachten hier nur den wichtigsten Fall der Zeilenauswahl: Die Auswahl über logische Ausdrücke mittels loc[]:
import pandas as pd df = pd.read_csv(&amp;#34;../data/Library_Usage.csv&amp;#34;) df.loc[df[&amp;#39;Total Checkouts&amp;#39;] &amp;gt; 10000] Der Ausdruck df['Total Checkouts'] &amp;gt; 10000 wird zuerst ausgewertet und ergibt eine boolesche Series mit Einträgen True wenn die Beobachtung mehr als 1000 Ausleihen getätigt hat und False sonst.</description></item><item><title>Exkurs: Fehlende Werte</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/na/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/na/</guid><description>Real erhobene Daten sind meistens unsauber und fehlerhaft. Ein häufiges Problem dabei sind fehlende Werte, also Beobachtungen für die manche Merkmale nicht erhoben wurden. In jedem Datensatz werden fehlende Werte anders gekennzeichnet, aber man findet oft diese Kodierungen wieder: &amp;quot;-999&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;None&amp;quot;, &amp;quot;NULL&amp;quot;, &amp;quot;#N/A&amp;quot;.
Wenn beispielsweise der Mittelwert einer statistischen Variable berechnet wird, so muss entschieden werden, wie mit fehlenden Werten umgegangen werden soll: Sollen die Werte entfernt werden?</description></item><item><title>Nützliche Funktionen in Pandas</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/functions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/functions/</guid><description>Mit df.head() kannst Du Dir die ersten $n$ Zeilen eines DataFrames anzeigen lassen:
import pandas as pd df = pd.read_csv(&amp;#34;../data/Library_Usage.csv&amp;#34;) df.head() Analog dazu funktioniert die Funktion df.tail().
2.9 Pandas Funktionen (5 Min) Schau Dir die Dokumentation für die Funktion head() hier an. Wie kannst Du Dir die ersten $100$ Zeilen anzeigen lassen?
Mit df.info() erhältst Du speicherbezogene Informationen über das Objekt. Mit df.describe() werden nützliche deskriptive Statistiken für alle numerischen Spalten eines Datensatzes ausgegeben.</description></item><item><title>Kursorganisation</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/modules/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/modules/</guid><description>Wir haben das Modul in mehrere Einheiten unterteilt. Du kannst Dir die Zeit für die Bearbeitung der Einheiten selber aufteilen, der vorgeschlagene Zeitplan dient der eigenen Orientierung. Einzelne Aufgabestellungen sind terminiert, damit Du (und die anderen Teilnehmenden) Chance auf ein Feedback haben.
Du findest in den Einheiten verschiedene Aufgaben mit unterschiedlichem Schwierigkeitsgrad, sowie zu Beginn einiger Einheiten praktische Projektaufgaben. Wenn Du mit einer Aufgabe nicht weiterkommen oder zu viel Zeit aufwenden musst, kannst Du diese jederzeit per Mail oder Chat mit den Betreuern besprechen oder zur Besprechung am Präsenztag bereithalten.</description></item><item><title>Termine und Kurseinheiten</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/schedule/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/schedule/</guid><description>Hier findest Du einen Überblick über die einzelnen Moduleinheiten.
Datum Titel Ziele 1. Kursorganisation und Vorbereitungen Überblick über das gesamte Modul (Kurseinheiten, Termine) Anpassung des eigenen Git-Repositoriums für das Arbeiten mit einem offenen Datensatz der San Francisco Library Erinnerung: Installation von Anaconda mit Python auf Deinem Rechner Erinnerung: Installation bzw. Nutzung von Jupyter Notebooks auf Deinem Rechner Überblick über verschiedene Python Pakete und Bibliotheken 19.02.2024 Virtueller Modulauftakt (16:00-17:00 Uhr) Kurze Vorstellungsrunde und Überblick über die Modulinhalte 19.</description></item><item><title>Projektordner und Datensatz</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/dataset/</guid><description>Im gesamten Modul werden wir wieder mit Git arbeiten. Darüber hinaus werden wir mit einem offenen Kundendatensatz der öffentlichen Bibliothek in San Francisco arbeiten. Führe daher die unteren Anpassungen durch.
1.1. Anlegen eines Projektordners und Nutzung von Git (15 Min) Im Modul 1 hast Du ein Git Repositorium erstellt und GitHub gespiegelt. Bitte erstelle in diesem Repositorium einen Ordner mit dem Namen Modul_3. Dieser Ordner wird Dein Projektordner für dieses Modul.</description></item><item><title>numpy</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/numpy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/numpy/</guid><description>numpy bietet den array als zentrale Datenstruktur. Mit ihm lassen sich numerische Daten effizient im Arbeitsspeicher (RAM) erstellen, ein- und auslesen, bearbeiten und aggregieren.
Numpy bietet neben dem array viele Funktionen an, mit denen sich effizient Berechnungen auf diesen durchführen lassen können. Außerdem wird die klassische Matrizenrechnung unterstützt (s. nachfolgendes Beispiel).
# import the library and give it a shorter name &amp;#39;np&amp;#39; import numpy as np # create 100 randomly distributed numbers X = np.</description></item><item><title>pandas</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/pandas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/pandas/</guid><description>pandas baut auf numpy auf und vereinfacht stark die Bearbeitung, Transformation, Aggregation und Zusammenfassung von zweidimensionalen Datensätzen sowie deren Import und Export in Python. Die zentralen Datenstrukturen in pandas sind Series und DataFrame.
Series sind eindimensionale Listen eines Datentypes, ähnlich wie arrays in numpy. Datentypen können ganzzahlige Zahlen (int), binäre Werte vom Typ true oder false (bool), Strings (str) oder reale Zahlen (float) sein.
In einem DataFrame werden mehrere Series gleicher Länge spaltenweise zu einer zweidimensionalen Tabelle (wie einer Excel Tabelle) zusammengefasst.</description></item><item><title>matplotlib</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/matplotlib/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/matplotlib/</guid><description>matplotlib ist das Standard-Paket zum Erstellen von wissenschaftlichen 2-dimensionalen statischen Graphiken. Die grundlegende Struktur in matplotlib ist figure, eine leere graphische Fläche, die mit Linien, Balken, Punkten, Beschriftungen und Axen befüllt werden kann. Der fertige Plot kann dann in diversen Formaten abgespeichert oder auf dem Bildschirm angezeigt werden.
# import the package and give it the shorter name &amp;#39;plt&amp;#39; %matplotlib inline import matplotlib.pyplot as plt # create some dummy data x = range(1, 10) # make a simple scatter plot of the data plt.</description></item><item><title>seaborn</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/seaborn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/seaborn/</guid><description>seaborn baut auf matplotlib auf und bietet eine Vielzahl von Funktionen, die es erlauben schnell und einfach schöne statistische Visualisierungen zu erstellen. Seaborn ist also keine komplett eigenenständige Graphik-Bibliothek, sondern nutzt intern die Funktionalitäten und Datenstrukturen von matplotlib.
Eine wichtige Funktion ist die sns.set() Methode. Wenn sie am Anfang eines Python-Scripts ausgeführt wird, wird intern das Design der Plots erheblich verbessert. Alle plots, die nach dem Aufruf der Funktion erstellt werden, sehen viel besser aus.</description></item><item><title>scipy</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/scipy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/scipy/</guid><description>scipy ist fest mit numpy und pandas verbunden und bietet eine Menge an Funktionen und Methoden aus der Mathematik und Statistik an.
Für uns ist vor allem das Paket scipy.stats Interessant. Mit ihm können Zufallszahlen aus verschiedensten statistischen Verteilungen generiert werden oder auch statistische Tests durchgeführt werden. Hier findest Du einen Überblick über alle Methoden des Pakets.
Im folgenden Beispiel wird ein Zweistichproben-t-Test an zwei numerischen Listen durchgeführt.
# import the package stats from the library scipy from scipy import stats # create two numerical arrays x = [12, 10, 11, 13, 14, 10, 13, 13, 22] y = [1, 4, 2, 3, 5, 2, 1, 0, 0, 1, 2] # perform a two sample t-test, to test if the samples have different means stats.</description></item><item><title>scitkit-learn</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/scikitlearn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/scikitlearn/</guid><description>scikit-learn ist eine umfangreiche Bibliothek für maschinelles Lernen in Python. Es bietet eine Vielzahl an verschiedenen Algorithmen, mit denen zum Beispiel Vorhersagen oder Bilderkennung durchgeführt werden können.
Faces recognition example using eigenfaces and SVMshttps://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py
# import the packages import numpy as np from sklearn.linear_model import LinearRegression # create some dummy dependent and independent variable X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) y = - 1 * X[:,0] + 2 * X[:,1] # estimate a linear regression and print out the coefficients reg = LinearRegression().</description></item><item><title>Grundbegriffe der Statistik</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/basic_terms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/basic_terms/</guid><description>Für den statistischen Teil dieses Moduls ist ein gewisses Grundvokabular und Verständnis von allgemeinen Beispielen notwendige Voraussetzung.
Daher ist in der Moodle-Kursumgebung das Einführungskapitel des Buchs Statistik: Der Weg zur Datenanalyse zum alleinigen persönlichen Gebrauch hinterlegt.1
Der Text gibt einen Einstieg in die Aufgaben und Anwendungsbereiche der Statistik und erklärt die grundlegenden Begriffe, mit denen Daten und Datensätze charakterisiert werden können. Nach der Lektüre solltest Du beispielsweise die folgenden Fragen beantworten können:</description></item><item><title>Lagemaße</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/mean/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/mean/</guid><description>Für metrische Variablen beschreiben Lagemaße die Zentralität einer Verteilung.
Wir werden uns hier auf die Lagemaße Mittelwert, Median und Quantil beschränken.
Mittelwert Das bekannteste Lagemaß ist der empirische Mittelwert (arithmetisches Mittel):
$$ \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i = \frac{x_1 + x_2 + \dots + x_n}{n} $$
Denk wieder an das Beispiel Wahlumfrage, wo $x_1, \dots, x_n$ die Beobachtungen beschreiben. Dabei stellt $n$ die Anzahl der Beobachtungen dar und $x_i$ beschreibt die Beobachtung an der i-ten Stelle.</description></item><item><title>Streuungsmaße</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/variance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/variance/</guid><description>Die Zentralität einer Verteilung (die durch Lagemaße beschrieben wird) ist nur eine wichtige Kennzahl. Streuungsmaße geben zusätzlich an, wie stark die Daten einer Messreihe schwanken. Die Streuung einer Variable ist entscheidend z.B. bei der Beurteilung mit welcher Wahrscheinlichkeit extreme Werte vorkommen können. Die bekanntesten Streuungsmaße sind die Varianz, die Standardabweichung und der Variantionskoeffizient.
Varianz Die Distanz einer Beobachtung vom Mittelwert der zugrundeliegenden Variable wird Abweichung genannt. Der Mittelwert über die quadrierten Abweichungen wird als Varianz definiert:</description></item><item><title>Symmetrie und Schiefe</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/symmetrie/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/symmetrie/</guid><description>Verschiedene univariate Verteilungen Related files distributions.ipynb (145 ko) Um eine metrische Verteilung charakterisieren zu können, ist neben der zentralen Lage- und Streuung auch deren Symmetrie und Schiefe von Bedeutung. Die Symmetrie sagt etwas über die (Un-)Gleichverteilung der Werte einer Variablen aus. Bei stark asymmetrischen Variablen (z.B. Haushaltseinkommen in Deutschland) ist das auftreten von kleinen Werten viel wahrscheinlicher, als das auftreten von sehr großen Werten (oder umgekehrt).
Das Bild zeigt Histogramme für verschiedene simulierte Zufallswerte der Beta-Verteilung.</description></item><item><title>Kreuztabellen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/bivariate/cross_tables/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/bivariate/cross_tables/</guid><description>Um zwei ordinale oder nominale Variablen miteinander zu vergleichen, eignen sich Kreuztabellen. Jeder Wert in der Kreuztabelle entspricht der Anzahl der Beobachtungen im Datensatz mit genau dieser Kombination an Merkmalsausprägungen.
Hier ein Beispiel (mit dem Argument na_values=&amp;quot;none&amp;quot; markiert pandas die &amp;quot;none&amp;quot; Einträge in der Spalte 'Notice Preference Definition' als fehlende Werte):
import pandas as pd df = pd.read_csv( &amp;#34;../data/Library_Usage.csv&amp;#34;, na_values=&amp;#34;none&amp;#34; ) pd.crosstab( df[&amp;#39;Notice Preference Definition&amp;#39;], df[&amp;#39;Age Range&amp;#39;], margins=True ) Age Range 0 to 9 years 10 to 19 years 20 to 24 years 25 to 34 years 35 to 44 years 45 to 54 years 55 to 59 years 60 to 64 years 65 to 74 years 75 years and over All Notice Preference Definition Email 28740 54936 22701 88200 77618 45165 17336 15539 27170 15069 392474 None 3952 11921 2680 4469 4101 3154 1740 2115 4544 4228 42904 All 32692 66857 25381 92669 81719 48319 19076 17654 31714 19297 435378 Eine Kreuztabelle mit absoluten Werten ist häufig schwer zu interpretieren, wenn die Randverteilungen ungleich verteilt sind.</description></item><item><title>Korrelation</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/bivariate/correlation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/bivariate/correlation/</guid><description>Für zwei metrische Variablen lässt sich der Zusammenhang über die sog. Kovarianz berechnen.
Wenn die Variablen mit $x$ und $y$ bezeichnet werden, ergibt sich die Kovarianz aus der Formel:
$$ s_{x, y}^2 = \frac{1}{N-1}\sum_{i=1}^{N}(x_i-\bar{x})(y_i-\bar{y}) , $$
wobei $\bar{x}$ und $\bar{y}$ die entsprechenden Mittelwerte darstellen und $N$ die Größe der Stichprobe (die Anzahl an Elementen in der Datenreihe von $x$ oder $y$).
Ein positiver Wert der Kovarianz drückt aus, dass wenn die Werte der einen Variablen steigen, dies auch für die andere Variable gilt.</description></item><item><title>Reflexion: Datenprojekte an der eigenen Arbeitsstelle</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/reflection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/reflection/</guid><description>2.10 Datenprojekte an der eigenen Arbeitsstelle (20 Min) Schreibe einen kurzen Text über die Verwendung von Daten und quantitativen Methoden an Deinem Arbeitsplatz. Denk dabei über folgende Fragen nach:
Welche Daten sind an Deiner Arbeitsstelle vorhanden? Mit welchen Daten arbeitest Du oder würdest Du gerne arbeiten? Werden statistische Verfahren oder Maschinelles Lernen schon an Deiner Arbeitsstelle eingesetzt? Welche Fragen oder Phänomene würdest Du gerne untersuchen? Was fändest Du spannend herauszufinden?</description></item><item><title>Tutorial</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/visualizations/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/visualizations/tutorial/</guid><description>Statistical analysis is a process of understanding how variables in a dataset relate to each other and how those relationships depend on other variables. Visualization can be a core component of this process because, when data are visualized properly, the human visual system can see trends and patterns that indicate a relationship. https://seaborn.pydata.org/tutorial/relational.html
Die zahlreichen Funktionen, die seaborn bietet basieren immer auf dem gleichen Prinzip. Visualisiert werden (nominale, ordinale, metrische) Variablen eines Datensatzes, die in Form eines DataFrames vorliegen.</description></item><item><title>Weitere Beispiele</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/visualizations/examples/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/visualizations/examples/</guid><description>Im Tutorial hast Du gesehen, wie Du ein Streudiagramm erstellen kannst. Hier werden exemplarisch weitere Möglichkeiten gezeigt, die Daten des Datensatzes zu visualisieren. Die wichtigste Funktion ist hierbei sns.catplot().
import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np # matplotlib inline sns.set() df = pd.read_csv(&amp;#34;../data/Library_Usage.csv&amp;#34;) Nominale und ordinale Variablen Univariate Häufigkeits- und Bivariate Kreuztatabellen können mit Balkendiagrammen visualisiert werden:
sns.catplot(y=&amp;#39;Year Patron Registered&amp;#39;, data=df, kind=&amp;#39;count&amp;#39;, color=&amp;#34;steelblue&amp;#34;) sns.</description></item><item><title>Recap: Quiz</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/quiz_pandas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/quiz_pandas/</guid><description/></item><item><title>Recap: Quiz</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/quiz_statistics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/quiz_statistics/</guid><description/></item><item><title>Musterlösungen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/solutions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/solutions/</guid><description>2. Grundlagen der Datenanalyse in Python 2.1 Grundbegriffe Wie viele Merkmale besitzt der Datensatz? 14. Wie groß ist die Stichprobengröße des Datensatzes? 436290. Wer oder was sind die Merkmalsträger? Bibliothekskunden der SF Public Library. Von wann bis wann wurden die Daten erhoben? Das Bibliothekssystem wurde 2003 installiert. Die Daten reichen bis 2023. Wie lässt sich die Grundgesamtheit beschreiben? Handelt es sich um eine Vollerhebung? Grundgesamtheit sind alle Bibliothekskunden der San Francisco Library.</description></item><item><title>Recap: Quiz</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/quiz_intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/quiz_intro/</guid><description/></item><item><title>Musterlösungen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/solutions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/solutions/</guid><description>3. Deskriptive Statistik und Visualisierungen 3.1 Häufigkeiten Lösungen 3.1.solutions_frequency.ipynb (5 ko) 3.2 Mittelwert und Median Der Median ist robust gegenüber Ausreißern, da er nicht auf den absoluten sondern nur auf der relativen Reihung der Beobachtungen basiert. Wird beispielsweise der größte Wert einer Messreihe um den Faktor 1000 tausend vergrößert, so ändert sicht der Median nicht.
Der Mittelwert hingegen basiert auf den absoluten Werten. Da die Variable Total Checkouts einige wenige sehr große Ausreißer enthält, ist der Mittelwert hier viel größer.</description></item><item><title>Musterlösungen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/solutions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/solutions/</guid><description>1. Kursorganisation und Vorbereitung Quiz Strg+Enter 436290, len(df) siehe (unter sns.set()): https://seaborn.pydata.org/introduction.html</description></item><item><title>Recap: Quiz Maschinelles Lernen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_basics/quiz_machine_learning_basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_basics/quiz_machine_learning_basics/</guid><description/></item><item><title>Das Bootstrapping Verfahren</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/bootstrap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/bootstrap/</guid><description>Das Ziel der Inferenzstatistik ist es, aus einer einzelnen Stichprobe $x_1, \dots, x_n$ die Stichproben-Verteilung eines Schätzers, wie dem Mittelwert $\bar{x}$ oder dem Median $x_{0.5}$, herzuleiten. Wenn die Stichproben-Verteilung eines Schätzers vorliegt kann damit der Wert des tatsächlichen unbekannten Populationsparameters eingegrenzt werden.
Für viele Schätzer kann deren Stichproben-Verteilung theoretisch hergeleitet werden. Neben der theoretischen Herangehensweise, gibt es auch eine intuitive empirische Methode, das Bootstrapping-Verfahren. Es basiert auf der Simulation von vielen Stichproben.</description></item><item><title>Konfidenzintervalle und Signifikanz</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/significance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/significance/</guid><description>Im vorherigen Beispiel hast Du mit Hilfe des Bootstrapping-Verfahrens die Stichprobenverteilung geschätzt. Wenn die Stichprobenverteilung bekannt ist, können damit Aussagen über den tatsächlichen Parameter in der Population (im Bild mit $\mu$ bezeichnet) getroffen werden.
Eine häufig angewandte Methode sind Konfidenzintervalle (KI). Man gibt einen Bereich aus der Stichprobenverteilung des Schätzwertes an, der den wahren Wert in der Population mit hoher Wahrscheinlichkeit überdeckt. Die Wahrscheinlichkeit wird mit $1-\alpha$ angegeben. Der Wert $\alpha$ wird Signifikanzniveau genannt und vor der Bestimmung des Intervalls festgelegt.</description></item><item><title>Mittelwertvergleiche</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/two-sample-test/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/two-sample-test/</guid><description>Ein häufiges Problem bei der statistischen Datenanalyse ist die Frage, ob signifikante Unterschiede in den Mittelwerten zweier Subpopulationen bestehen: Leihen Frauen beispielsweise im Mittel signifikant mehr aus als männliche Bibliothekskunden? Tätigen Kunden im Ruhestand im Mittel weniger Verlängerungen als junge Kunden?
Von einem signifikanten Unterschied spricht man, wenn die Differenz zwischen den Mittelwerten zweier Stichproben so groß ist, dass es sehr Unwahrscheinlich ist, dass dieser Unterschied alleine aufgrund der rein zufälligen Schwankungen durch die Stichprobenziehung entstanden ist.</description></item><item><title>Musterlösungen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/solutions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/solutions/</guid><description> 4. Exkurs: Inferenzstatistik Lösungen solutions_inference.ipynb (12 ko)</description></item><item><title>Klassifikation</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/classification/</guid><description>Bitte erstell ein neues Jupyter-Notebook und nennen es Klassifikation.
Wir nutzen ein Datenset, das handschriftliche Ziffern in Form von 8x8 Feldern mit Werten der Farbstärkte darstellt. Eine Beschreibung des Datensets gib es bei scikit-learn und im UC Irvine Machine Learning Repository. Dieses Datenset bringt scikit-learn selber mit.
Wir importieren eine Funktion zum Laden des Datensets und rufen dieses auf.
from sklearn.datasets import load_digits digits = load_digits() Die Daten und Metadaten sind in einem sogenannten Bunch-Objekt organisiert</description></item><item><title>Regression</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/regression/</guid><description>Bitte erstell ein neues Jupyter-Notebook und nennen esRegression.
Hier führen wir eine einfache Regression von zwei Features (also in zwei Dimensionen durch), da dies sich leichter visualisieren lässt. Tatsächlich ist man aber in der Anzahl an Features nicht eingeschränkt. Ziel ist es ein Regression-Model zu erstellen, in dem man einen numerischen Eingabe-Wert (x) eingibt und einen numerischen Ausgabe-Wert (y) erhält.
Wir erzeugen und ein künstlichen Datenset von 500 Datenpunkten mit Hilfe der Funktion make_regression.</description></item><item><title>Text-Klassifikation</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/text_classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/text_classification/</guid><description>Text-Klassifikations-Beispiel Das Beispiel basiert auf einem offenen Datensat von Newsgroup-Nachtrichten und orientiert sich an diesem offiziellen Tutorial von scikit-learn zur Textanalyse.
Wir nutzen Dokumente von mehreren Newsgroups und trainieren damit einen Classifier, der dann eine Zudordnung von neuen Texten auf eine dieser Gruppen durchführen kann. Sprich die Newsgroups stellen die Klassen/Tags dar, mit denen wir neue Texte klassifizieren. Wie nutzen eine einfachen Bag-of-Word-Ansatz in dem wir (normalisierte) Häufigkeit von Wörtern als Features nutzen.</description></item><item><title>Dimensionsreduktion</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/dimension_reduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/dimension_reduction/</guid><description>Bitte erstelle ein neues Jupyter-Notebook und nennen es Dimensionsreduktion.
Wir laden das Datenset:
from sklearn.datasets import load_digits digits = load_digits() Wir wollen mit einer Pricinple Component Analysis (PCA) die Dimensionreduktion durchführen. Hierzu laden wir zuerst die Klasse und erzeugen eine Instanz davon. Wir geben an, dass wir danach nur zwei Dimensionen (&amp;ldquo;components&amp;rdquo;) erhalten möchten. Dadurch lassen sie die Daten später auf in einem Plot mit zwei Achsen abbilden.
from sklearn.decomposition import PCA pca = PCA(random_state=1, n_components=2) Wir können die Tranformation mit der Funktion fit_transform durchführen.</description></item><item><title>Quiz</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/quiz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/quiz/</guid><description/></item><item><title>Quiz</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/quiz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/quiz/</guid><description/></item><item><title>Musterlösungen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/solutions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/solutions/</guid><description>Hier die Jupyter Notebooks mit Musterlösungen (ablegen mit &amp;ldquo;Speichern unter&amp;rdquo;):
Klassifikation Regression Dimensionsreduktion</description></item><item><title>Musterlösungen</title><link>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/solutions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/solutions/</guid><description>Hier ist das Jupyter Notebooks mit der Text-Klassifikation-Musterlösung (ablegen mit &amp;ldquo;Speichern unter&amp;rdquo;).</description></item></channel></rss>