[{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/anaconda/","title":"Conda und Anaconda","tags":[],"description":"","content":"Conda ist eine freie und offene Softwarepaketverwaltung für Python. Neben der Möglichkeit, Pakete (packages, libraries) für Python aus dem Internet zu installieren, können mit conda virtuelle Umgebungen (environments) angelegt werden. Diese Umgebungen beinhalten nur die Pakete und Python Versionen, die für ein spezifisches Projekt gebraucht werden. Umgebungen können mit anderen Personen geteilt werden, sodass sichergestellt ist, dass alle Programmierer mit den gleichen Paketen und Versionen arbeiten, auch wenn sie unterschiedliche Systeme (Windows, Linux, MacOS) verwenden.\nAnaconda basiert auf conda. Mit Anaconda werden eine Vielzahl von Paketen, die für die Datenanalyse gebraucht werden, schon vorinstalliert. Außerdem bietet Anaconda eine vorinstallierte Entwicklungsumgebung (Spyder IDE) und eine vorinstallierte Version von Jupyter, mit der Notebooks gestartet werden können.\n1.3 Anaconda (10 Min) Falls noch nicht getan (sollte durch Modul 1 und 2 aber der Fall sein), kannst Du Anaconda hier für Dein Betriebssystem herunterladen. Wir verwenden die Version für Python 3.12. Öffne den mit Anaconda installierten Anaconda Navigator und verschaff Dir einen Überblick über die vorhandenen Programme. "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/frequency/","title":"Häufigkeiten","tags":[],"description":"","content":"Kategoriale Variablen werden in Häufigkeitstabellen zusammengefasst. Dabei wird für jede Ausprägung die Anzahl der Beobachtungen gezählt:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Age Range\u0026#39;].value_counts() Mit der Funktion value_counts() kann man sich absolute Häufigkeitstabellen ausgeben lassen. Mit dem zusätzlichen Argumentaufruf normalize=True werden relative Häufigkeiten berechnet:\ndf[\u0026#39;Age Range\u0026#39;].value_counts(normalize=True) Der Modus sind dabei die Merkmalsausprägungen, die die meisten Beobachtungen besitzen: age_mode = df[\u0026#39;Age Range\u0026#39;].mode() age_mode[0] 3.1 Häufigkeiten (20 Min) Erstelle eine Häufigkeitsverteilung für die Variable 'Year Patron Registered'. Wieviel Prozent der Kunden wurden 2013 im System registriert? Wie viele in den kommenden Jahren? Was fällt Dir auf? Wieviel Prozent der Kunden sind zwischen 25 und 34 Jahren? Ersetze die fehlenden Werte in der Spalte Age Range durch den Modus dieser Spalte. Nutze dazu die Funktion DataFrame.fillna (siehe hier für die Dokumentation). Denkst Du, es handelt sich dabei um eine gute Methode, fehlende Werte zu ersetzen? Welche anderen Strategien fallen Dir ein? "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/solutions/","title":"Lösungen zu den Kursaufgaben","tags":[],"description":"","content":"Hier finden sich die Musterlösungen aller Einheiten im Überblick.\n1. Kursorganisation und Vorbereitung 2. Grundlagen der Datenanalyse in Python 3. Deskriptive Statistik und Visualisierungen 4. Exkurs: Inferenzstatistik 6. Maschinelles Lernen - Praxis mit scikit-Learn 7. Maschinelles Lernen - Automatische Textanalyse "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/","title":"Modul 3","tags":[],"description":"","content":"Daten analysieren und darstellen Herzlich Willkommen zum dritten Modul Daten analysieren und darstellen des ZBWI Zertifikationskurs \u0026ldquo;Data Librarian\u0026rdquo;. In diesem Modul möchten wir Dir einen praktischen Einblick in die Datenanalyse mit der Programmiersprache Python geben.\nNachdem Du im ersten Modul schon die grundlegenden Werkzeuge und Programmiertechniken kennengelernt und im zweiten vertieft hast, wirst Du Dich in den kommenden Wochen anhand von praktischen Beispielen und Aufgaben Grundlagen der deskriptiven Statistik, der Datenvisualisierung und des maschinellen Lernens aneignen. Dabei kannst Du Deine Programmier- und Datenanalysekenntnisse in Python verbessern und im bibliothekarischen Kontext anwenden. Nach dem Modul solltest Du ein grundsätzliches Verständnis wichtiger statischer Verfahren und Methoden des Maschinellen Lernen kennen, besitzen und anwenden können.\nDie Kurseinheiten bauen aufeinander auf. Wir empfehlen Dir deswegen durch die Inhalte dieses Moduls mit den Pfeiltasten zu navigieren. In der linken Navigationsleiste wird Dein Fortschritt gespeichert. Diese Webseite ist für alle Endgeräte optimiert. Du kannst daher auch Dein Smartphone oder Tablet zum Lesen nutzen. Starte nun mit der ersten Kurseinheit indem Du auf klickst oder mit den Pfeiltasten navigierst. Viel Spaß! "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/series/","title":"Series und DataFrames","tags":[],"description":"","content":"Series und DataFrames sind die zentralen Datenstrukturen in Pandas. Series sind wie standardmäßige Listen in Python, mit dem wichtigen Unterschied, dass Series nur Werte eines einzelnen Datentyps enthalten können.\nimport pandas as pd x = pd.Series([34, 12, 23, 45]) print(x) x.dtype Ein Datentyp ist die grundlegende Einheit, in der einzelne Werte in einer Programmiersprache vom Computer gespeichert und verarbeitet werden können. Beispiele für Datentypen in pandas sind: float für Gleitkommazahlen, int für Ganzzahlen, bool für binäre True, False Werte oder datetime für Datumswerte. Text wird im pandas-spezifischen Format object abgespeichert. Für einen DataFrame der beispielsweise in einer Variable mit dem Namen df gepeichert ist, kannst Du Dir die Datentypen jeder Spalte mit df.dtypes ausgeben lassen.\nEin DataFrame fasst mehrere Series gleicher Länge zu einer Datentabelle mit Zeilen (Beobachtungen), Spalten (Variablen) und Spaltennamen (Variablennamen) zusammen.\nEin reguläres Python-Dictionary mit den Spaltennamen als Keys und den Values in Form von Python-Listen kann in ein DataFrame transfomiert werden:\nimport pandas as pd data = {\u0026#39;month\u0026#39;: [\u0026#39;Jan\u0026#39;, \u0026#39;Feb\u0026#39;, \u0026#39;Mar\u0026#39;], \u0026#39;temp\u0026#39;: [-5, 2, 3], \u0026#39;below_zero\u0026#39;: [True, False, False]} df = pd.DataFrame(data) print(df) In der praktischen Datenanalyse wirst Du nur selten DataFrames oder Series manuell erstellen, sondern im Computer abgespeicherte Datentabellen aus anderen Formaten, wie Excel oder .csv einlesen.\n2.2 Skalenniveau und Datentypen (15 Min) Welches Skalenniveau besitzen die Variablen im obigen Beispiel? Sind die Variablen stetig oder diskret? Was ist der Datentyp jeder einzelnen Spalte? Erweitere das Python-Dictionary und erstelle eine neue Zeile mit den Werten ('Apr', 5, False) erstelle eine neue Spalte year mit den Werten (2020, 2020, 2020, 2020) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/statistic/","title":"Statistik","tags":[],"description":"","content":" Justus Perthes (1838): Rhein, Elbe und Oder Statistik ist die traditionelle Wissenschaft von der Erhebung und Analyse von Daten. Sie verfügt über eine großes theoretisches und mathematisches Fundament und lässt sich in die Teilgebiete deskriptive (Beschreiben), explorative (Suchen) und schließende (Induktion) Statistik unterteilen.\nIm Kapitel Grundbegriffe der Statistik wirst Du mehr über die Grundlagen der Statistik nachschlagen können.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/overview/","title":"Statistische Inferenz","tags":[],"description":"","content":"Bisher haben wir vorliegende Daten einer Stichprobe mit Visualisierungen und Statistiken beschrieben und zusammengefasst. Von Interesse sind aber in der Regel die Zusammenhänge und Statistiken in der Gesamtpopulation.\nBeispiel Wahlumfrage: Es werden zufällig $n=100$ Personen aus dem Wahlregister gezogen und nach nach ihren Parteipräferenzen befragt. Man kann dann beispielsweise den relativen Anteil der Personen in der Stichprobe, die eine bestimmte Partei favorisieren, bestimmen. Damit hat man einen Schätzwert für den tatsächlichen Wert, wenn man alle Personen des Wahlregisters befragt hätte.\nZieht man eine weitere Stichprobe, so werden die neuen Schätzwerte nicht genau mit denen aus der vorherigen Stichprobe übereinstimmen. Zieht man noch eine Stichprobe, so wird auch hier der Mittelwert wieder geringfügig anders sein. Will man deswegen eine Aussage über die tatsächlichen Anteile in der Gesamtpopulation treffen, so ist diese Aussage immer mit Unsicherheit behaftet.\nDer Mittelwert/relative Anteil ändert sich mit jeder Stichprobe, die man zieht. Damit können die auf einer Stichprobe berechneten Schätzwerte als statistische Variablen betrachtet werden. Wie für andere Variablen auch, kann diese Stichprobenverteilung deskriptiv beschrieben werden.\nIn der Realität wird aber in der Regel nur eine einzige Stichprobe der Größe $n$ gezogen. Wie kann mann dann von den Mittelwerten einer einzelnen Stichprobe auf den \u0026ldquo;wahren\u0026rdquo; zugrundeliegenden Wert in der Gesamtpopulation schließen? Wie kann man die Unsicherheiten, die dabei auftreten quantifizieren? Mit diesen Fragen beschäftigt sich die Inferenzstatistik!\nDer Stichprobenfehler gibt an, wie stark ein Schätzwert (z.B. Relativer Anteil Personen mit Präferenz für Partei A) von Stichprobe zu Stichprobe schwankt. Damit wird also die Varianz eines Schätzers angegeben. Für viele Statistiken, wie das arithmetische Mittel, kann dessen Verteilung theoretisch hergeleitet werden.\nBesitzt beispielsweise die Stichprobenverteilung für den Mittelwert eine hohe Varianz (d.h. mit jeder weiteren Stichprobe würden die berechneten Mittelwerte stark schwanken), dann lässt sich der tatsächliche Mittelwert in der Population nur schlecht eingrenzen. In der Regeln verringert sich die Varianz eines Schätzers je größer die Stichprobe ist.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/io/","title":"Ein- und Ausgabe","tags":[],"description":"","content":"Die Funktionen zur Ein- und Ausgabe von Daten in pandas sind umfangreich aber systematisch organisiert. Um beispielsweise eine .csv Datei einzulesen und in einer Variable zu speichern verwendet man die Funktion read_csv:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df.head() Um einen eingelesenen Datensatz beispielsweise im .json Textformat zu speichern verwendet man die Funktion to_json:\ndf.to_json(\u0026#34;../data/Library_Usage.json\u0026#34;) Manche Funktion aus dem pandas Paket sind statische Funktionen: Sie sind an kein konkretes Objekt gebunden, sondern werden über den Bibliotheksnamen pd aufgerufen. Beispiele: pd.read_csv, pd.to_numeric, pd.crosstab. Andere Funktionen sind an ein bestimmtes Objekt, welches mit einer Variable referenziert wird, gebunden. In der Regel ist dies ein DataFrame oder eine Series. Beispiele: df.to_csv, df.corr, df.head, x.mean. Mache Dich mit dem Unterschied vertraut: Was bedeuten pd und df und x in den Beispielen? Exkurs: Einlesen von Daten Die Festplatte des Computers dient zur persistenten Speicherung von Dateien. Auch wenn der Strom weg ist, bleiben diese darauf erhalten. Die hohe Speicherfähigkeit hat ihren Preis: Die Zugriffszeiten, d.h. die Zeit die die Festplatte benötigt um z.B. Zeilen einer Textdatei zu lesen und die Werte an den Prozessor zu übergeben, sind hoch.\nDeswegen gibt es neben dem Festplattenspeicher auch noch den Arbeitsspeicher (RAM). Dessen Zugriffszeiten sind wesentlich schneller, die Daten sind jedoch nicht persistent. Wenn Du z.B. eine Tabelle mit Excel öffnest, dann werden die Daten von der Festplatte in den Arbeitsspeicher geladen. Das gleiche, nur ohne graphische Oberfläche, passiert, wenn Du Daten mit dem pandas Paket einliest.\nDa normalerweise der Datensatz komplett in den Arbeitsspeicher geladen werden muss, können prinzipiell nicht beliebig große Datenmengen bearbeitet werden.\n2.3 Exkurs: Datenrundreise (30 Min) Informiere Dich hier über die verschiedenen Funktionen zur Ein- und Ausgabe. Lies den Datensatz \u0026quot;../data/Library_Usage_Small.csv\u0026quot; ein (Download hier). Er enthält nur 10 Zeilen des originalen Datensatzes (aus Performancegründen). Speichere den DataFrame als .json ab. Lies die .json ein und speichere den DataFrame als .html Tabelle ab (Die .html Datei lässt sich auch mit einem Browser öffnen). Lies dann die .html Datei ein (Achtung: read_html gibt eine Liste von DataFrames zurück!) und speichere den DataFrame als .xlsx Datei ab (Die .xlsx Datei lässt sich auch mit Excel öffnen). Lies nun die .xlsx Datei ein und speichere den DataFrame wieder als .csv ab. Achte darauf, den ursprünglichen originalen Datensatz nicht zu überschreiben. Vergleiche Sie die originale .csv Version mit der Version, nach der Datenrundreise. Ist alles gleich geblieben? 2.4 Exkurs: Arbeitsspeicher (30 Min) Finde heraus, wie viel freier Arbeitsspeicher Dein Computer hat (Das Betriebssystem und Hintergrundprogramme verbrauchen auch RAM). Wie viele int64 Werte, also Zahlen, die 8 Byte (=64 Bit) Speicher benötigen, kannst Du damit theoretisch in den Arbeitsspeicher laden? Tip: Nutze Google zum Umrechnen. Wie viele Beobachtungen kann eine Tabelle mit 100 numerischen Variablen damit maximal theoretisch haben, damit Du diese noch bearbeiten kannst? Nutze die Funktion df.memory_usage() um Dir den tatsächlich benötigten Speicher eines DataFrames oder einer Series anzeigen zu lassen. Mit dem Funktionsargument deep=True wird der Wert genau ermittelt und nicht nur geschätzt. "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/notebooks/","title":"Jupyter Notebooks","tags":[],"description":"","content":"Hier findet sich eine kleine Erinnerung und Beispielaufgabe zum Thema Jupyter Notebooks.\nDie Projektaufgaben und Code-Beispiele in diesem Modul werden über Jupyter Notebooks erstellt und verteilt.\nJupyter Notebooks bieten eine browserbasierte graphische Schnittstelle zur Python Programmierumgebung. Deswegen können Notebooks auf jedem System gestartet werden, man benötigt dazu nur einen Web-Browser und eine lokale installierte Version von Python.\nDarüber hinaus bieten Notebooks die Möglichkeit Text, Visualisierungen und Code in einer integrierten Datei zu erstellen. Somit können einfach statistische Reports und Analysen erstellt werden. Die Replizierbarkeit der Ergebnisse ist auch gewährleistet, da jede Person, die Programmierschritte im Notebook auf dem eignen Rechner wiederholen kann.\nJupyter Notebook enthält einen Dateimanager mit dem Du durch die Ordner und Dateien Deines Systems navigieren kannst. Mit einem Klick auf eine Notebook-Datei öffnet sich ein neues Browser-Tab mit dem Notebook. Notebooks bestehen immer aus Text/ Markdown oder Code Zellen (cells). Der Python Code in den Zellen kann ausgeführt werden und das Ergebnis wird direkt im Notebook angezeigt.\n1.4 Erinnerung: Jupyter Notebooks (20 Min) Lade dieses Notebook herunter (Rechtsklick -\u0026gt; Ziel/Link speichern unter\u0026hellip;) und speichere es in dem Ordner ./notebooks/ im Ordner Modul_3. Starte Jupyter Notebook über die Kommandozeile oder über den Anaconda Navigator. Navigiere zu dem Speicherort des Notebooks und öffne es. Markiere die Code-Zelle und führe sie mit einem Klick auf den Run Button oder mit der Tastenkombination Strg+Enter aus. Ändere die Farbe der Punkte im Plot von Grün auf Rot. Füge das Datum und Deinen Namen der Text-Zelle hinzu. Notebook-Dateien erkennt man an der Dateiendung .ipynb. Diese Dateien kann man in Jupyter mit dem integrierten Dateimanager öffnen. Jupyter startet man entweder über den Anaconda Navigator oder über den folgenden Befehl in der Kommandozeile (Die Kommandozeile danach nicht wieder schließen!): jupyter notebook Über http://localhost:8890 im Browser, gelangt man zur Oberfläche von Jupyter. "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/ml/","title":"Machine Learning","tags":[],"description":"","content":"Seit der Erfindung des Personal Computers und des Internets werden statistische Probleme immer komplexer und größer. Die Datenmengen erfordern neue effiziente Strukturen zum Speichern und Auffinden der Informationen.\nMaschinelles Lernen (Machine Learning oder Statistical Learning) bedeutet in diesem Kontext relevante und signifikante Muster und Trends aus den Daten zu extrahieren um die Daten \u0026ldquo;zu verstehen\u0026rdquo;. Dabei spielen Computer und deren wachsende Rechenpower eine immer größere Rolle. Sie haben die klassische angewandte Statistik revolutioniert und es sind vor allem Ingenieure und Informatiker, die die Weiterentwicklung der Disziplin heutzutage vorantreiben.1\nIm Maschinellen Lernen steht insbesondere die Vorhersagekraft und Generalisierbarkeit von statistischen Methoden und Algorithmen im Vordergrund. Ziel ist es, möglichst gute Prognosen, beispielsweise bei der Gesichtserkennung, zu machen. Die klassische Statistik dagegen ist stärker an den kausalen Zusammenhängen und der Stärke von signifikanten Einflüssen einzelner Faktoren auf ein Resultat interessiert.\nTeachable Machine von Google ermöglicht das Trainieren von Machine Learning Modellen im eigenen Web-Browser ohne das Programmiercode geschrieben werden muss. Somit wird das Grundprinzip des Maschinellen Lernens auch ohne Vorkenntnisse erfahrbar und man bekommt ein gutes Gespür für die Möglichkeiten und Grenzen der Methode. Experimentiere zum Beispiel mit dem Bild-Klassifikator.\nMaschinelles Lernen und die angewandte Statistik besitzen große Überschneidungen und beide Gebiete bauen auf Erkenntnissen der Wahrscheinlichkeitstheorie auf. Machine Learning ist dabei auch ein Teilgebiet der Künstlichen Intelligenz, die als die Automatisierung von intellektuelle Aufgaben, die normalerweise von Menschen durchgeführt werden verstanden wird.2\nKI kann auch allein mit durch Programmierer fest eingebauten Regeln entstehen. Diese Regeln legen fest, wie eine Eingabe (z.B Pixelwerte eines Bilds oder die Anzahl der gestrigen Sonnenstunden) in eine Ausgabe (Wahrscheinlichkeit für ein Katzenbild oder heutige Regenwahrscheinlichkeit) transformiert wird. Damit diese Art der KI erfolgreich ist, braucht es ein großes Vorwissen und spezielle Expertise. Algorithmen des Maschinellen Lernens werden hingegen trainiert, d.h. sie lernen selbstständig die optimalen Regeln, die von einer Eingabe zu einer Ausgabe schließen lassen. Damit diese Transformation Erfolg hat benötigt das System viele Beispiele, für die die Ausgabe bekannt ist, um von diesen zu lernen.2\nHastie, Tibshirani, Friedman (2017): The Elements of Statistical Learning, Springer.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFrancois Chollet (2018): Deep Learning with Python, Manning.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/columns/","title":"Auswahl und Erstellung von Spalten","tags":[],"description":"","content":"Die Spalten eines DataFrames werden über einen Spaltenindex referenziert. Üblicherweise besteht der Spaltenindex aus Spaltennamen in Textform:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df.columns Einzelne Series können wie bei einem Python Dictionary mit df[\u0026lt;name\u0026gt;] extrahiert werden. Mehre Spalten mit df[[\u0026lt;name1\u0026gt;, \u0026lt;name2]]. Wenn Du Spalten mit der doppelten Liste [[...]] auswählst, erhältst Du in jedem Fall wieder einen DataFrame zurück. Das Ergebnis der Auswahl kannst Du bei Bedarf wieder in einer Variablen abspeichern:\nx = df[\u0026#39;Total Renewals\u0026#39;] df[[\u0026#39;Total Renewals\u0026#39;, \u0026#39;Total Checkouts\u0026#39;]] column_names = [\u0026#39;Total Renewals\u0026#39;, \u0026#39;Total Checkouts\u0026#39;] # auxiliary variable subset = df[column_names] print(x) print(subset) Spalten können mit einer Zuweisung (=) überschrieben oder neu erstellt werden:\ndf[\u0026#39;dummy_variable\u0026#39;] = 5 Bei der Auswahl von Spalten und Zeilen wird keine Kopie des DataFrames oder der Series erstellt, sondern nur eine Referenz auf die ursprüngliche Tabelle. Wenn Du Daten in der ursprünglichen Tabelle änderst, so ändert sich auch die Referenz:\nx = df[\u0026#39;Total Renewals\u0026#39;] df[\u0026#39;Total Renewals\u0026#39;] = 5 x Berechnungen auf schon bestehenden Variablen können auch direkt einer neuen Spalte zugeordnet werden:\nimport numpy as np df[\u0026#39;is_adult\u0026#39;] = df[\u0026#39;Patron Type Definition\u0026#39;] == \u0026#39;Adult\u0026#39; df[\u0026#39;log_renewals\u0026#39;] = np.log(df[\u0026#39;Total Renewals\u0026#39;] + 1) Im ersten Beispiel wurde zuerst die Anweisung df['Patron Type Definition'] == 'Adult' durchgeführt. Das implizite Ergebnis dieser Anweisung ist eine Series mit booleschen Werten True oder False. Die neu erstellte Series wird dann in einer neuen Spalte is_adult dem DataFrame angehängt.\nIm zweiten Beispiel wurde der Logarithmus auf den Werten der Spalte Total Renewals berechnet und einer neuen Spalte log_renewals zugewiesen.\n2.5 Fallstudie: Feature Engineering (30 Min) Ziel ist es, eine neue Variable Membership Duration zu erstellen, die für jeden Kunden die aktive Mitgliedschaft in Monaten seit der Registrierung misst. Die aktive Mitgliedschaft wird definiert als:\n\u0026#39;Membership Duration\u0026#39; = (\u0026#39;Circulation Active Year\u0026#39; - \u0026#39;Year Patron Registered\u0026#39;)*12 + \u0026#39;Circulation Active Month\u0026#39; Versuche die folgenden Codebeispiele nachzuvollziehen, auch wenn Du nicht alle Funktionen im Detail kennst oder verstehst.\nDie Spalte Circulation Active Year ist als Text und nicht als Zahl abgespeichert! Konvertiere die Spalte in ein numerisches Format. Überschreibe die ursprüngliche Variable mit den neuen Werten. Nutze dieses Codesnippet: pd.to_numeric( df[\u0026#39;Circulation Active Year\u0026#39;], errors=\u0026#39;coerce\u0026#39; ) Die Spalte Circulation Active Month enthält die Monatsnamen als Text. Für die Berechnung muss diese in ein numerisches Format konvertiert werden.\nZuerst konvertieren wir die Spalte in ein Datumsformat. Das geht mit der Funktion pd.to_datetime. Überschreibe wieder die ursprüngliche Variable mit den neuen Werten. Du kannst dieses Codesnippet nutzen: pd.to_datetime( df[\u0026#39;Circulation Active Month\u0026#39;], errors=\u0026#39;coerce\u0026#39;, format=\u0026#34;%b\u0026#34; ) Jetzt extrahieren wir den Monat als Zahl aus der Spalte: df[\u0026#39;Circulation Active Month\u0026#39;].dt.month Berechne nun die aktive Mitgliedschaftsdauer in Monaten wie oben definiert und weise das Ergebnis der Spalte Membership Duration zu.\nNimm an, dass Einträge mit fehlenden Werten bedeutet, dass die Person 0 Monate aktiv Mitglied gewesen ist. Ersetze dazu alle NaN values in der neuen Variable mit der Zahl 0. Nutze dieses Codesnippet (siehe auch den nachfolgenden Abschnitt Exkurs: Fehlende Werte über die Behandlung fehlender Werte):\ndf[\u0026#39;Membership Duration\u0026#39;].fillna(0) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/data-science/","title":"Data Science","tags":[],"description":"","content":"Ein Data Scientist ist eine Person, die oder der Wissen und Erkenntnisse aus strukturierten und unstrukturierten Daten gewinnt. Data Science ist eine interdisziplinäre Disziplin und liegt irgendwo in der Schnittmenge von angewandter Statistik, angewandter Informatik (Hacking skills) und spezielles Fachwissen (domain knowledge / substantive expertise).\nAufgrund der stark angewachsenen Mengen an unstrukturierten Daten aus heterogenen Datenquellen (Text, Bilder, Sensoren, Netzwerke, Videos, \u0026hellip;) reichen die Methoden und Fertigkeiten, die die Statistik traditionellerweise liefert und vermittelt, nicht mehr aus, um diese Daten effizient zu strukturieren, aggregieren, kombinieren, analysieren und visualisieren zu können:\nA Data Scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician 1.\nQuelle: http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram\nJoel Grus (2019): Data Science from Scratch, O\u0026rsquo;Reilly.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/rows/","title":"Auswahl von Zeilen","tags":[],"description":"","content":"Die Zeilen eines DataFrames können über drei verschiedene Arten ausgewählt werden. Das System kann am Anfang etwas verwirrend sein. Wir betrachten hier nur den wichtigsten Fall der Zeilenauswahl: Die Auswahl über logische Ausdrücke mittels loc[]:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df.loc[df[\u0026#39;Total Checkouts\u0026#39;] \u0026gt; 10000] Der Ausdruck df['Total Checkouts'] \u0026gt; 10000 wird zuerst ausgewertet und ergibt eine boolesche Series mit Einträgen True wenn die Beobachtung mehr als 1000 Ausleihen getätigt hat und False sonst.\nMit einer booleschen Series lassen sich dann die Zeilen des DataFrame auswählen: Es werden genau die Zeilen zurückgegeben, bei denen die Series True Werte enthält.\nAnstatt alles in einer Zeile zu schreiben, können wir auch eine Hilfsvariable erstellen, die den booleschen Vektor zwischenspeichert:\nrow_filter = df[\u0026#39;Total Checkouts\u0026#39;] \u0026gt; 10000 df.loc[row_filter] Für den booleschen Zeilenfilter können komplexe logische Ausdrücke unter Zuhilfenahme der Operatoren \u0026lt;, \u0026gt;, \u0026amp;, |, == u.s.w. gebildet werden. Welche Zeilen werden hier gefiltert?\nrow_filter = (df[\u0026#39;Patron Type Definition\u0026#39;] == \u0026#39;Senior\u0026#39;) \u0026amp; (df[\u0026#39;Notice Preference Definition\u0026#39;] == \u0026#39;Email\u0026#39;) df.loc[row_filter] Logische Operatoren Ausdruck Beschreibung \u0026lt;/ \u0026lt;= kleiner/ kleiner gleich \u0026gt; / \u0026gt;= größer/ größer gleich == gleich != ungleich \u0026amp; elementweises logisches und (True und True ergeben True, sonst False) ` ` ~ elementweise logische negation (True ergibt False und umgekehrt) Nützlich ist auch die Funktion Series.between(left, right), mit der eine boolesche Series erstellt wird, die True ist wenn der Wert der ursprünglichen Series zwischen oder auf den Werten left und right liegt. Im folgenden Beispiel wird gezeigt, dass die beiden Filter das gleiche Ergebnis liefern:\nfilter1 = (df[\u0026#39;Total Checkouts\u0026#39;] \u0026gt;= 20) \u0026amp; (df[\u0026#39;Total Checkouts\u0026#39;] \u0026lt;= 80) filter2 = df[\u0026#39;Total Checkouts\u0026#39;].between(20, 80) all(filter1 == filter2) 2.6 Filtern (30 Min) Filtere den Datensatz nach Kindern unter 10 Jahren. Wie viele Einträge erhältst Du? Gibt es Personen mit mehr als 20000 Ausleihen? Wie viele Personen stammen aus dem Stadtteil (Richmond)? Wie viele Prozent der Beobachtungen haben eine Membership Duration von Null Monaten? "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/na/","title":"Exkurs: Fehlende Werte","tags":[],"description":"","content":"Real erhobene Daten sind meistens unsauber und fehlerhaft. Ein häufiges Problem dabei sind fehlende Werte, also Beobachtungen für die manche Merkmale nicht erhoben wurden. In jedem Datensatz werden fehlende Werte anders gekennzeichnet, aber man findet oft diese Kodierungen wieder: \u0026quot;-999\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;NULL\u0026quot;, \u0026quot;#N/A\u0026quot;.\nWenn beispielsweise der Mittelwert einer statistischen Variable berechnet wird, so muss entschieden werden, wie mit fehlenden Werten umgegangen werden soll: Sollen die Werte entfernt werden? Sollen die fehlenden Werte durch einen bestimmten Wert ersetzt werden?\nIn DataFrames werden fehlende Werte durch das Schlüsselwort NaN (\u0026quot;Not a Number\u0026quot;) angezeigt. Beim Einlesen von Daten (siehe z.B. die read_csv Funktion) können mit dem Argument na_values zusätzliche Kodierungen für fehlerhafte Werte mit angegeben werden.\n2.7 Datentypen (1 Minute) Was unterscheidet den Wert None vom Wert \u0026quot;None\u0026quot;? Was den Wert 5 vom Wert \u0026quot;5\u0026quot;? Was den Wert \u0026quot;NaN\u0026quot; vom Wert NaN? Ist True und \u0026quot;True\u0026quot; das gleiche?\nBehandlung von Fehlenden Werten Pandas bietet für Series und DataFrames die nützlichen Funktionen isna(), notna(), dropna() und fillna() an um fehlende Werte zu identifizieren, zu entfernen oder mit anderen Werten zu ersetzen.\nFilter Die Funktionen isna (notna) geben eine boolesche Series zurück, die True (False) ist, wenn an der Stelle ein fehlender Wert steht. Damit pandas fehlende Werte korrekt erkennt, müssen diese vorher erst in das interne Format NaN umgewandelt werden (siehe oben).\ndf[df[\u0026#39;Age Range\u0026#39;].isna()] df[df[\u0026#39;Age Range\u0026#39;].notna()] Mit diesem nützlichen Befehl kannst Du Dir schnell die Anzahl fehlender Werte in jeder Spalte ausgeben lassen:\ndf.isna().sum() Dies funktioniert, da Python bei Bedarf einen booleschen Wert implizit in ein numerisches Format konvertiert. True wird zu 1 konvertiert und False zu 0.\nEntfernen # drops all rows that contain at least one missing values df.dropna() # drops all missing values in this series df[\u0026#39;Age Range\u0026#39;].dropna() Ersetzen df[\u0026#39;Age Range\u0026#39;].fillna(\u0026#34;keine Angabe\u0026#34;) Standardmäßig werden bei den Operationen fillna oder dropna neue Series oder DataFrames zurückgegeben. Die originale Variable bleibt dabei unangetastet. Mit dem Argument inplace=True werden die originalen Objekte direkt überschrieben.\n2.8 Exkurs: Fehlende Werte (20 Min) Welche Spalten enthalten alles fehlende Werte? Lies den Datensatz ein und erstelle einen DataFrame der keine Beobachtungen mit fehlenden Werten mehr enthält. Speichere diesen unter dem Namen Library_Usage_Clean.csv ab. Wie viele Beobachtungen wurden dabei entfernt? "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/","title":"Kursorganisation und Vorbereitungen","tags":[],"description":"","content":"Kursorganisation und Vorbereitungen Diese Einheit gibt einen Überblick über die Kursinhalte, wichtige Termine und die benötigte Software und Python-Pakete. Viele der hier besprochenen Dinge werden Dir schon bekannt vorkommen und die Software hast Du schon für die vorherigen Module auf Deinem Rechner installiert.\nZiele Überblick über das gesamte Modul (Kurseinheiten, Termine) Anpassung des eigenen Git-Repositoriums für das Arbeiten mit einem offenen Datensatz der San Francisco Library Erinnerung: Installation von Anaconda mit Python 3.12 auf Deinem Rechner Erinnerung: Installation bzw. Nutzung von Jupyter Notebooks auf Deinem Rechner Überblick über verschiedene Python Pakete und Bibliotheken Einordnung der Begriffe Statistik, Data Science und Machine Learning Zum schnellen Nachschlagen: Unsere Python-Kurzreferenz oder dieses Cheat-Sheet gibt einen guten Überblick über die wichtigsten Befehle in Python.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/functions/","title":"Nützliche Funktionen in Pandas","tags":[],"description":"","content":"Mit df.head() kannst Du Dir die ersten $n$ Zeilen eines DataFrames anzeigen lassen:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df.head() Analog dazu funktioniert die Funktion df.tail().\n2.9 Pandas Funktionen (5 Min) Schau Dir die Dokumentation für die Funktion head() hier an. Wie kannst Du Dir die ersten $100$ Zeilen anzeigen lassen?\nMit df.info() erhältst Du speicherbezogene Informationen über das Objekt. Mit df.describe() werden nützliche deskriptive Statistiken für alle numerischen Spalten eines Datensatzes ausgegeben. Um alle Spalten miteinzubeziehen nutze das Funktionsargument include='all':\ndf.describe(include=\u0026#39;all\u0026#39;) Viele Funktionen funktionieren für DataFrames und Series gleichermaßen:\nprint(df.min()) print(df[\u0026#39;Total Renewals\u0026#39;].min()) Mit der Funktion sum() werden die Werte einer Spalte aufaddiert:\ndf[\u0026#39;Total Renewals\u0026#39;].sum() df[\u0026#39;Total Renewals\u0026#39;].between(100, 200).sum() DataFrames besitzen drei wichtige Attribute, die Informationen über die Spalten, die Datentypen und die Anzahl der Elemente geben:\ndf.columns df.dtypes df.shape "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/modules/","title":"Kursorganisation","tags":[],"description":"","content":"Wir haben das Modul in mehrere Einheiten unterteilt. Du kannst Dir die Zeit für die Bearbeitung der Einheiten selber aufteilen, der vorgeschlagene Zeitplan dient der eigenen Orientierung. Einzelne Aufgabestellungen sind terminiert, damit Du (und die anderen Teilnehmenden) Chance auf ein Feedback haben.\nDu findest in den Einheiten verschiedene Aufgaben mit unterschiedlichem Schwierigkeitsgrad, sowie zu Beginn einiger Einheiten praktische Projektaufgaben. Wenn Du mit einer Aufgabe nicht weiterkommen oder zu viel Zeit aufwenden musst, kannst Du diese jederzeit per Mail oder Chat mit den Betreuern besprechen oder zur Besprechung am Präsenztag bereithalten. Zu den meisten Aufgaben finden sich am Ende der jeweiligen Einheit Musterlösungen. Die Projektaufgaben fassen die Inhalte der jeweiligen Einheit in einer alltagsnahen Aufgabenstellung zusammen und sollten nach Bearbeitung der jeweiligen Einheit machbar sein. Bei den Aufgaben und Projektaufgaben, die mit einer Abgabefrist versehen sind, hast Du die Möglichkeit ein Feedback zu erhalten. Die anderen Aufgaben und Projektaufgaben sind für Dein Selbststudium konzipiert.\nDer erste Teil des Moduls (19.02.2024 - 04.03.2024, Einheit 1 bis 4) wird von Ania Lopez betreut und behandelt grundlegende klassische Konzepte der angewandten Statistik. Der zweite Teil des Moduls (05.03.2024 - 17.03.2024, Einheit 5 bis 7) wird von Konrad Förstner betreut und gibt einen Überblick über Themen des maschinellen Lernens.\nAm virtuellen Präsenztag, der am 22.03.2024 stattfindet, werden wir im Voraus gesammelte Fragen gemeinsam beantworten und diskutieren. Du wirst an dem Tag Zeit haben, an einem persönlichen Datenanalyseprojekt zu arbeiten. Die Kursleiter werden Dich dabei unterstützen und individuell betreuen. Am Ende des Präsenztages stellen alle KursteilnehmerInnen ihre Ergebnisse in einer Kurzpräsentation vor.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/schedule/","title":"Termine und Kurseinheiten","tags":[],"description":"","content":"Hier findest Du einen Überblick über die einzelnen Moduleinheiten.\nDatum Titel Ziele 1. Kursorganisation und Vorbereitungen Überblick über das gesamte Modul (Kurseinheiten, Termine) Anpassung des eigenen Git-Repositoriums für das Arbeiten mit einem offenen Datensatz der San Francisco Library Erinnerung: Installation von Anaconda mit Python auf Deinem Rechner Erinnerung: Installation bzw. Nutzung von Jupyter Notebooks auf Deinem Rechner Überblick über verschiedene Python Pakete und Bibliotheken 19.02.2024 Virtueller Modulauftakt (16:00-17:00 Uhr) Kurze Vorstellungsrunde und Überblick über die Modulinhalte 19.02.2024 – 24.02.2024 2. Grundbegriffe der Statistik und Einführung in Pandas Kenntnisse des statistischen Grundvokabulars und Anwendung auf die Beschreibung eines Datensatzes Ein- und Auslesen von Datensätzen als DataFrames in Python Filtern von DataFrames nach Spalten oder Zeilen Erstellung neuer Variablen 24.02.2024 Abgabetermin Aufgabe 2.10 Beschreibung eigener Datenprojekte 24.02.2024 – 04.03.2024 3. Deskriptive Statistik und Visualisierung Berechne grundlegende Lage- und Streuungsmaße Bereche Statistiken für bivariate Verteilungen Erstelle einfache Visualisierungen 4. Exkurs: Inferenzstatistik (freiwillige Vertiefung) Durchführung eines Zwei-Stichproben Mittelwerttests 05.03.2024 – 10.03.2024 5. Maschinelles Lernen - Grundlagen / 6. Maschinelles Lernen - Praxis mit scikit-learn Führe Regression, Klassifikation und Dimensionreduktion mit scikit-learn durch 11.03.2024 Virtuelle Fragestunde (16:00-17:00 Uhr) Kläre Deine Fragen mit den Betreuern 12.03.2024 – 17.03.2024 7. Machinelles Lernen - Automatische Textanalyse / Textmining-Grundlagen Führe eine Textanalyse durch Formuliere einfache quantitative Fragen für den Projekttag als Exposé (max. 1 Seite Text) 18.03.2024 Abgabetermin Projektaufgabe Deskriptive Statistik Praktische Anwendung der Inhalte des ersten Teil des Moduls 22.03.2023 Präsenztag (10-16 Uhr) Nimm an der Frage und Antwortrunde teil Finde geeignete Daten zum Lösen der Fragen Beantworte Deine Frage mit den gelernten statistischen Tools Bereite die Ergebnisse in Form einer Visualisierung auf Stelle die Ergebnisse in einer Kurzpräsentation in Deiner Gruppe vor (\u0026lt; 5 Minuten) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/dataset/","title":"Projektordner und Datensatz","tags":[],"description":"","content":"Im gesamten Modul werden wir wieder mit Git arbeiten. Darüber hinaus werden wir mit einem offenen Kundendatensatz der öffentlichen Bibliothek in San Francisco arbeiten. Führe daher die unteren Anpassungen durch.\n1.1. Anlegen eines Projektordners und Nutzung von Git (15 Min) Im Modul 1 hast Du ein Git Repositorium erstellt und GitHub gespiegelt. Bitte erstelle in diesem Repositorium einen Ordner mit dem Namen Modul_3. Dieser Ordner wird Dein Projektordner für dieses Modul. Dort legst Du alle Datensätze und Jupyter Notebooks ab. Erstelle einen Unterordner ./data/ und einen Unterordner ./notebooks/ innerhalb Deines Projektordners. Füge die Dateien auch in das Git-Repositorium hinzu (kleine Erinnerung git add und git commit -m \u0026quot;Erläuterung\u0026quot;, große Erinnerung). Im ersten Teil des Moduls werden wir einen offenen Kundendatensatz der öffentlichen Bibliothek in San Francisco analysieren.\nThe Integrated Library System (ILS) is composed of bibliographic records including inventoried items, and patron records including circulation data. The data is used in the daily operation of the library, including circulation, online public catalog, cataloging, acquisitions, collection development, processing, and serials control. This dataset represents the usage of inventoried items by patrons \u0026hellip; (Abstract taken from here)\n1.2 Arbeiten mit dem Datensatz der SFPL (20 Min) Besuche das offene Daten-Portal der Stadt San Francisco und informiere Dich über den Datensatz. Lade den Datensatz Library_Usage.csv aus dem Internet herunter und speichere ihn im Projektordner im Unterordner ./data/ ab. Stell sicher, dass Dein Projektordner die folgende Verzeichnisstruktur aufweist: Module_3 ├── data │ └── Library_Usage.csv ├── notebooks Auf der Seite findest Du eine detallierte Erklärung der einzelnen Variablen des Datensatzes (=Spalten der Tabelle).\nbooks by 1 brian is licesed under CC BY-NC-SA 2.0\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/","title":"Recap Python-Grundlagen","tags":[],"description":"","content":"Im Folgenden findest Du Informationen zum Nachschlagen und Auffrischen Deiner Python-Kenntnisse und eine Übersicht über nützliche Python-BIbliotheken.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/","title":"Python Pakete und Bibliothekten","tags":[],"description":"","content":"Die folgende Liste gibt einen kurzen Überblick über die wichtigsten Python Bibliotheken, von denen Du manche im Modul näher kennenlernen wirst. Im ersten Teil des Modules werden wir hauptsächlich mit pandas und seaborn arbeiten.\nProgramming Recap Module, Paket, Library Ein Python Skript mit der Endung .py wird Modul genannt. Eine Sammlung von Modulen in einem Ordner, wird Paket (package) genannt. Eine Sammlung von Paketen innerhalb eines größeren Projekts wird Bibliothek (library) genannt. Ein framework ist eine große grundlegende Bibliothek, mit einem bestimmten Zweck und mit vielen Paketen, die voneinander abhängen und aufeinander aufbauen. Die Begriffe werden aber nicht einheitlich benutzt und der Übergang ist oft fließend. Kommentare stehen immer hinter dem # Zeichen. Text steht immer in Anführungszeichen, z.B \u0026quot;hallo\u0026quot; oder 'hi'. Mit dem import Befehl können externe Bibliotheken mit mehr Funktionalitäten geladen werden. Mit dem Zuweisungsoperator = können Objekte einem Variablennamen oder einem Funktionswert zugeordnet werden, z.B: x = 1, text = 'hallo'. Funktionen werden mit runden Klammern aufgerufen und können Funktionsargumente besitzen, z.B. sum([1, 2, 3]). Viele Funktionen sind Bestandteil von Bibliotheken und werden dann wie folgt aufgerufen: \u0026lt;paketname\u0026gt;.\u0026lt;funktionsname\u0026gt;(\u0026lt;funktionsargumente\u0026gt;) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/numpy/","title":"numpy","tags":[],"description":"Effizientes Handling und Bearbeitung von numerischen Arrays.","content":"numpy bietet den array als zentrale Datenstruktur. Mit ihm lassen sich numerische Daten effizient im Arbeitsspeicher (RAM) erstellen, ein- und auslesen, bearbeiten und aggregieren.\nNumpy bietet neben dem array viele Funktionen an, mit denen sich effizient Berechnungen auf diesen durchführen lassen können. Außerdem wird die klassische Matrizenrechnung unterstützt (s. nachfolgendes Beispiel).\n# import the library and give it a shorter name \u0026#39;np\u0026#39; import numpy as np # create 100 randomly distributed numbers X = np.random.normal(size=100) # transform X into a 2-dimensional array of size 20x5 X.reshape((20, 5)) # calculate the matrix dot product: X*X\u0026#39;, where X\u0026#39; is the transpose of X X.dot(X.T) Wenn Dir die Matrizenrechnung nicht geläufig ist, probiere aus das obere Beispiel nachzuvollziehen, indem Du mit ganz einfachen 2x2-dimensionale Matrizen rechnest, denen Du vorab einfache feste Werte vorgibst.\nMit numpy kann beispielsweise ein Bild als dreidimensionales numpy array dargestellt werden: Die ersten zwei Dimensionen beschreiben die Farbintensität der Pixel auf einer zweidimensionalen Fläche. Die dritte Dimension speichert die jeweiligen Pixelwerte für die Farbkanäle rot, grün und blau.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/pandas/","title":"pandas","tags":[],"description":"Bearbeitung, Transformation, Aggregation und Zusammenfassung von Datensätzen. Baut auf numpy auf.","content":"pandas baut auf numpy auf und vereinfacht stark die Bearbeitung, Transformation, Aggregation und Zusammenfassung von zweidimensionalen Datensätzen sowie deren Import und Export in Python. Die zentralen Datenstrukturen in pandas sind Series und DataFrame.\nSeries sind eindimensionale Listen eines Datentypes, ähnlich wie arrays in numpy. Datentypen können ganzzahlige Zahlen (int), binäre Werte vom Typ true oder false (bool), Strings (str) oder reale Zahlen (float) sein.\nIn einem DataFrame werden mehrere Series gleicher Länge spaltenweise zu einer zweidimensionalen Tabelle (wie einer Excel Tabelle) zusammengefasst. Ein DataFrame besitzt außerdem auch immer Spalten- und Zeilennamen.\nWie auch numpy, bietet pandas darüber hinaus viele Funktionen aus der Statistik zum Beschreiben von Daten. Eine Übersicht gibt es hier.\n# import the library and give it a shorter name \u0026#39;pd\u0026#39; import pandas as pd # create a dataframe by hand with two columns and three rows df = pd.DataFrame({ \u0026#39;month\u0026#39;: [1, 2, 3], \u0026#39;temperatur\u0026#39;: [-12, 3, 9] }) # print out some descriptive statistics df.describe() 1.5 pandas (15 Min) Kopiere das obere Codebeispiel in ein Jupyter Notebook, speichere es in Deinem Projektordner und führe es aus. Füge weitere Temperatur und Monats-Werte dem DataFrame hinzu. Welche Statistiken liefert ein Aufruf der Funktion describe()? "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/matplotlib/","title":"matplotlib","tags":[],"description":"Bietet 2D Plotting Funktionalitäten.","content":"matplotlib ist das Standard-Paket zum Erstellen von wissenschaftlichen 2-dimensionalen statischen Graphiken. Die grundlegende Struktur in matplotlib ist figure, eine leere graphische Fläche, die mit Linien, Balken, Punkten, Beschriftungen und Axen befüllt werden kann. Der fertige Plot kann dann in diversen Formaten abgespeichert oder auf dem Bildschirm angezeigt werden.\n# import the package and give it the shorter name \u0026#39;plt\u0026#39; %matplotlib inline import matplotlib.pyplot as plt # create some dummy data x = range(1, 10) # make a simple scatter plot of the data plt.plot(x, x, c=\u0026#34;green\u0026#34;, linestyle=\u0026#39;\u0026#39;, marker=\u0026#39;+\u0026#39;) 1.6 matplotlib (15 Min) Kopiere das obere Codebeispiel in ein Jupyter Notebook, speichere es in Deinem Projektordner und führe es aus. Ändere die Farbe der Pukte im Plot von grün auf schwarz. Ändere den Aufruf so um, dass statt Punkte, Linien angezeigt werden. Hier findest Du die Dokumentation der Funktion matplotlib.pyplot.plot. "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/seaborn/","title":"seaborn","tags":[],"description":"Verbesserung und Weiterentwicklung der matplotlib Bibliothek.","content":"seaborn baut auf matplotlib auf und bietet eine Vielzahl von Funktionen, die es erlauben schnell und einfach schöne statistische Visualisierungen zu erstellen. Seaborn ist also keine komplett eigenenständige Graphik-Bibliothek, sondern nutzt intern die Funktionalitäten und Datenstrukturen von matplotlib.\nEine wichtige Funktion ist die sns.set_theme() Methode. Wenn sie am Anfang eines Python-Scripts ausgeführt wird, wird intern das Design der Plots erheblich verbessert. Alle plots, die nach dem Aufruf der Funktion erstellt werden, sehen viel besser aus.\nTeste den Unterschied mit dem folgenden Beispiel:\n# import the libraries and give them some shorter names import matplotlib.pyplot as plt import seaborn as sns # setup the seaborn library sns.set_theme() # create the same plot as in the previous example x = range(1, 10) plt.plot(x, x) Wenn Du im Jupyter Notebook das Code-Beispiel ausgeführst hast und danach den Aufruf sns.set_theme() entfernst, ändert sich das Design des Plots erstmal nicht. Für einen \u0026ldquo;Reset\u0026rdquo; musst Du den Kernel (also der im Hintergrund laufende Python Prozess) mit einem Klick auf neu starten.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/scipy/","title":"scipy","tags":[],"description":"Funktionen und Methoden aus der Statistik.","content":"scipy ist fest mit numpy und pandas verbunden und bietet eine Menge an Funktionen und Methoden aus der Mathematik und Statistik an.\nFür uns ist vor allem das Paket scipy.stats Interessant. Mit ihm können Zufallszahlen aus verschiedensten statistischen Verteilungen generiert werden oder auch statistische Tests durchgeführt werden. Hier findest Du einen Überblick über alle Methoden des Pakets.\nIm folgenden Beispiel wird ein Zweistichproben-t-Test an zwei numerischen Listen durchgeführt.\n# import the package stats from the library scipy from scipy import stats # create two numerical arrays x = [12, 10, 11, 13, 14, 10, 13, 13, 22] y = [1, 4, 2, 3, 5, 2, 1, 0, 0, 1, 2] # perform a two sample t-test, to test if the samples have different means stats.ttest_ind(x,y) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/recap_python/packages/scikitlearn/","title":"scitkit-learn","tags":[],"description":"Bietet Funktionen und Methoden für maschinelles Lernen.","content":"scikit-learn ist eine umfangreiche Bibliothek für maschinelles Lernen in Python. Es bietet eine Vielzahl an verschiedenen Algorithmen, mit denen zum Beispiel Vorhersagen oder Bilderkennung durchgeführt werden können.\nFaces recognition example using eigenfaces and SVMshttps://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py\n# import the packages import numpy as np from sklearn.linear_model import LinearRegression # create some dummy dependent and independent variable X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) y = - 1 * X[:,0] + 2 * X[:,1] # estimate a linear regression and print out the coefficients reg = LinearRegression().fit(X, y) reg.coef_ "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/basic_terms/","title":"Grundbegriffe der Statistik","tags":[],"description":"","content":"Für den statistischen Teil dieses Moduls ist ein gewisses Grundvokabular und Verständnis von allgemeinen Beispielen notwendige Voraussetzung.\nDaher ist in der Moodle-Kursumgebung das Einführungskapitel des Buchs Statistik: Der Weg zur Datenanalyse zum alleinigen persönlichen Gebrauch hinterlegt.1\nDer Text gibt einen Einstieg in die Aufgaben und Anwendungsbereiche der Statistik und erklärt die grundlegenden Begriffe, mit denen Daten und Datensätze charakterisiert werden können. Nach der Lektüre solltest Du beispielsweise die folgenden Fragen beantworten können:\nWas ist Statistik? Was macht Statistik? Welche grundlegenden Begriffe werden in der Statistik verwendet? Wie kann das Messniveau für Spalten eines Datensatzes bestimmt werden? 2.1 Grundbegriffe (60 Min) Beantworte und diskutiere die folgenden Fragen konkret für den in Einheit 1 heruntergeladenen San Francisco Library Usage Datensatz. Notiere Deine Ergebnisse in Stichpunkte.\nWie viele Merkmale besitzt der Datensatz? Wie groß ist die Stichprobengröße des Datensatzes? Wer oder was sind die Merkmalsträger? Von wann bis wann wurden die Daten erhoben? Wie lässt sich die Grundgesamtheit beschreiben? Handelt es sich um eine Vollerhebung? Welche Merkmale sind stetig? Welche diskret? Welchem Skalenniveau entsprechen die einzelnen Merkmale (Nominal-, Ordinal- oder Metrische Skala)? Enthält der Datensatz fehlende Werte? Handelt es sich um Querschnitts-, Längsschnitss- oder Paneldaten? Über das oben genannte Kapitel hinaus sind auch die englischen Lehrbücher von OpenIntro zu empfehlen. Sie können kostenlos auf der Webseite heruntergeladen werden. Für diesen Kurs sind insbesondere die ersten zwei Kapitel des Buchs Introductory Statistics with Randomization and Simulation 2 relevant. Auch diese Literaturhinweise sind in der Moodle-Kursumgebung hinterlegt.\nFahrmeir, Ludwig, Christian Heumann, Rita Künstler, Iris Pigeot, and Gerhard Tutz. Statistik: Der Weg zur Datenanalyse. Springer-Verlag, 2016, https://www.springer.com/de/book/9783662503713.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDiez, David, Christopher Barr, and Mine Cetinkaya-Rundel. Introductory Statistics with Randomization and Simulation, 2014, https://www.openintro.org/stat/textbook.php.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/","title":"Univariate Verteilungen","tags":[],"description":"","content":"In der Statistik geben Verteilungen an, wie wahrscheinlich oder häufig eine bestimmte Merkmausausprägung oder eine Kombination von Merkmausausprägungen ist. Univariate Verteilung beschreiben dabei die Wahrscheinlichkeiten einer einzelnen statischen Variablen, während bivariate oder multivariate Verteilungen sich auf zwei oder mehr Variablen beziehen.\nEmpirische Verteilungen beziehen sich dabei auf die Häufigkeiten in beobachtbaren Daten während theoretische Verteilungen mathematische Funktionen sind, die meist von einigen wenigen Parametern abhängen.\nStatistiken wie der Mittelwert oder die Varianz dienen der Beschreibung und Charakterisierung von Verteilungen mittels einiger weniger aussagekräftigen Kennzahlen. Dabei gibt es Statistiken, die oft nur auf Variablen eines bestimmten Skalenniveaus (kleine Erinnerung) anwendbar sind.\nKategoriale Variablen (nominale und ordinale Variablen), werden typischerweise in Häufigkeitstabellen zusammengefasst. Wichtige Kennzahlen für metrische Variablen sind hingegen u.a. die zentrale Lage, Streuung und die Symmetrie.\nNach diesem Kapitel solltest Du die folgenden Fragen beantworten können:\nWie erstelle und interpretiere ich eine (relative) Häufigkeitstabelle? Welche grundlegenden Statistiken kann ich mit pandas-Funktionen ausrechnen? Was ist der Unterschied zwischen dem Median und dem arithmetischem Mittel? Welche Funktionen gibt es, um die Streuung einer Variablen zu messen? Welche univariaten Verteilungstypen gibt es? Im Folgenden werden mit $x = x_1, \\dots, x_n$ eine (univariate) Reihe von Beobachtungen beschrieben. Dabei stellt $n$ die Anzahl der Beobachtungen dar und $x_i$ beschreibt die Beobachtung an der i-ten Stelle.\nBeispiel Wahlumfrage: Es werden zufällig n=100 Personen aus dem Wahlregister gezogen und nach nach ihren Parteipräferenzen befragt. Eine Beobachtung $x_i$ stellt dabei die Charakteristiken einer befragten Person $i$ dar (neben der Parteipräferenz können das allgemeine beschreibende Daten sein wie Alter, Geschlecht, etc.)\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/mean/","title":"Lagemaße","tags":[],"description":"","content":"Für metrische Variablen beschreiben Lagemaße die Zentralität einer Verteilung.\nWir werden uns hier auf die Lagemaße Mittelwert, Median und Quantil beschränken.\nMittelwert Das bekannteste Lagemaß ist der empirische Mittelwert (arithmetisches Mittel):\n$$ \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{x_1 + x_2 + \\dots + x_n}{n} $$\nDenk wieder an das Beispiel Wahlumfrage, wo $x_1, \\dots, x_n$ die Beobachtungen beschreiben. Dabei stellt $n$ die Anzahl der Beobachtungen dar und $x_i$ beschreibt die Beobachtung an der i-ten Stelle.\nHier ein Beispiel aus unserem Datensatz:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Total Checkouts\u0026#39;].mean() Median Eine zweite wichtige Statistik ist der Median. Er ergibt sich aus dem Wert der Beobachtung, die die nach der Größe geordnete Messreihe in genau zwei gleich große Teile teilt. Für eine gerade Anzahl an Beobachtung wird der Mittelwert der zwei Beobachtungen an den Stellen $n/2$ und $n/2+1$ genommen:\n$$ x_{0.5} = \\begin{cases} x_{(n+1)/2}~, \\text{ n ungerade} \\\\ \\frac{x_{n/2} + x_{n/2+1}}{2}~, \\text{ n gerade} \\end{cases} $$ für $x_1 \u0026lt; x_2 \u0026lt; \\dots \u0026lt; x_n$.\nBeispiel: Für $x=[8, 10, 11, 30]$ ist die Anzahl der Beobachtungen $n=4$ gerade und der Median wird berechnet mit $\\frac{x_2 + x_3}{2} = \\frac{10+11}{2} = 10.5$.\ndf[\u0026#39;Total Checkouts\u0026#39;].median() 3.2 Mittelwert und Median (20 Min) Schau Dir den Mittelwert und den Median der Variable Total Checkouts an. Warum sind die beiden Werte so unterschiedlich? Was ziehst Du daraus für Schlüsse für weitere statistische Analysen und Reports? Wenn Dir nach der Aufgabe zum Mittelwert und Median die Begriffe noch nicht klar sind und Dich die oberen Formeln sehr abschrecken, kann es auch manchmal hilfreich sein, sich diese Statistiken mit einfachen Beispielen aus der Schule erklären zu lassen.\nEmpfehlenswert sind hierzu die Videos von Lehrerschmidt auf Youtube.\nVersuche im Anschluss anhand der einfachen Beispiele, die oberen Formeln nachzuvollziehen.\nQuantile Wir haben schon den Median als Lageparameter kennengelernt, dieser wird auch als $x_{0.5}$ bezeichnet. Er teilt die geordnete Verteilung in zwei genau gleich große Teile. Allgemeiner lassen sich dazu die Quantile definieren: $x_{0.75}$ teilt die geordnete Verteilung im Verhältnis 3:1. Das heißt, dass 75% der Beobachtungen kleiner als $x_{0.75}$ und 25% größer sind. Das $x_{0.25}$ Quantil teilt die Reihe im Verhältnis 1:3. Hier sind 25% der Beobachtungen kleiner und 75% größer als der Wert $x_{0.25}$.\ndf[\u0026#39;Total Checkouts\u0026#39;].quantile(q=[0.25, 0.5, 0.75]) Um Ausreißer in einer Variablen zu entfernen bzw. zu ersetzen, bietet es sich manchmal an, die größten (und oder kleinsten) $\\alpha\\%$ Beobachtungen zu identifizieren:\n# identifies 0.5% of the data at both ends of the distribution alpha = 0.005 df[\u0026#39;Total Checkouts\u0026#39;].quantile([alpha, 1-alpha]) 3.3 Exkurs: Ausreißerentfernung I (30 Min) Identifziere jeweils die 1.5% größten Werte in der Spalte Total Checkouts. Definiere diese Werte als Ausreißer. Erstelle einen Datensatz, für den diese Ausreißer entfernt sind. Handelt es sich hierbei um eine gute Methode, Ausreißer zu identifizieren und zu behandeln? Welche anderen Strategien kennst Du? "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/variance/","title":"Streuungsmaße","tags":[],"description":"","content":"Die Zentralität einer Verteilung (die durch Lagemaße beschrieben wird) ist nur eine wichtige Kennzahl. Streuungsmaße geben zusätzlich an, wie stark die Daten einer Messreihe schwanken. Die Streuung einer Variable ist entscheidend z.B. bei der Beurteilung mit welcher Wahrscheinlichkeit extreme Werte vorkommen können. Die bekanntesten Streuungsmaße sind die Varianz, die Standardabweichung und der Variantionskoeffizient.\nVarianz Die Distanz einer Beobachtung vom Mittelwert der zugrundeliegenden Variable wird Abweichung genannt. Der Mittelwert über die quadrierten Abweichungen wird als Varianz definiert:\n$$ s^2_x = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2 $$\nDenk wieder an das Beispiel Wahlumfrage, wo $x_1, \\dots, x_n$ die Beobachtungen beschreiben. Dabei stellt $n$ die Anzahl der Beobachtungen dar und $x_i$ beschreibt die Beobachtung an der i-ten Stelle. In der obigen Formel bezeichnet $\\bar{x}$ den Mittelwert.\nAngewendet auf unseren Datensatz für die Anzahl der Ausleihen:\ndf[\u0026#39;Total Checkouts\u0026#39;].var() Eine geringe Varianz bedeutet, dass sich die Werte, die die Variable annehmen kann, nur geringfügig vom Mittelwert unterscheiden. Das Quadrieren der Abweichungen hat zur Folge, dass das Vorzeichen verschwindet und das große Abweichungen mehr Gewicht erhalten. In der Formel wird durch $n-1$ anstatt durch $n$ geteilt. Dies ist theoretisch von Bedeutung, um einen unverzerrten Schätzer zu erhalten, es hat aber für große $n$ in der Praxis keine Auswirkungen ob man durch $n$ oder $n-1$ teilen.\nStandardabweichung Die Standardabweichungen ist die Wurzel der Varianz: $$ s_x = \\sqrt{s_x^2} $$\ndf[\u0026#39;Total Checkouts\u0026#39;].std() Variationskoeffizient Die absolute Größe der Varianz ist abhängig vom Mittelwert der Variablen. Ein Vergleich von Standardabweichungen verschiedener Variablen ist deswegen nicht sinnvoll. Möchte man die Streuung verschiedener Variablen vergleichen, macht es Sinn, eine normalisierte Größe, den Variationskoeffizienten zu betrachten:\n$$ cv_x = \\frac{s_x}{\\bar{x}} $$\n3.4 Varianz (15 Min) Welche Variable streut mehr: 'Total Checkouts' oder 'Total Renewals'? Vergleiche die Standardabweichungen und den Variationskoeffizienten miteinander.\nWeitere Steruungsmaße sind die Spannweite und der Interquartilabstand.\nSpannweite Die Spannweite ist die Differenz zwischen dem maximalen und minmalem Wert\ndf[\u0026#39;Total Checkouts\u0026#39;].max() - df[\u0026#39;Total Checkouts\u0026#39;].min() Interquartilsabstand Aus den Quantilen kann der Interquartilsabstand als robustes Streuungsmaß abgeleitet werden. Er ergibt sich aus der Differenz des 75%- zum 25%-Quantil: $$ x_{IQR} = x_{0.75} - x_{0.25} $$\ndf[\u0026#39;Total Checkouts\u0026#39;].quantile(q=0.75) - df[\u0026#39;Total Checkouts\u0026#39;].quantile(q=0.25) 3.5 Exkurs: Ausreißerentfernung II (30 Min) Identifiziere positive Ausreißer in der Spalte Total Checkouts. Ausreißer werden jetzt als Beobachtungen $x_i$ definiert, für die gilt: $$ x_i \u0026gt; x_{0.75} + 1.5x_{IQR} $$ Das heißt, eine Beobachtungen gilt als Ausreißer, wenn sie größer als die Summe aus dem 75% Quantil und dem 1.5-fachen des Interquartilsabstands ist. Wie viel Prozent der Beobachtungen im Datensatz werden mit dieser Methode als Ausreißer markiert? "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/univariate/symmetrie/","title":"Symmetrie und Schiefe","tags":[],"description":"","content":" Verschiedene univariate Verteilungen Related files distributions.ipynb (145 ko) Um eine metrische Verteilung charakterisieren zu können, ist neben der zentralen Lage- und Streuung auch deren Symmetrie und Schiefe von Bedeutung. Die Symmetrie sagt etwas über die (Un-)Gleichverteilung der Werte einer Variablen aus. Bei stark asymmetrischen Variablen (z.B. Haushaltseinkommen in Deutschland) ist das auftreten von kleinen Werten viel wahrscheinlicher, als das auftreten von sehr großen Werten (oder umgekehrt).\nDas Bild zeigt Histogramme für verschiedene simulierte Zufallswerte der Beta-Verteilung. Dabei wurden jeweils die Parameter der theoretischen Verteilung $\\alpha$ und $\\beta$ geändert. Somit kann eine große Bandbreite charakteristischer Verteilungen abgedeckt werden. Neben dem Histogramm wurde auch der empirische Median und Mittelwert der Verteilung als vertikale Linien eingezeichnet.\nFür symmetrische Verteilungen gilt, dass der Mittelwert und der Median gleich sind und das Histogramm an diesen Achsen gespiegelt werden kann. Eine linkssteile (rechtschiefe) Verteilung ergibt sich durch einige überdurchschnittlich große Werte. In diesem Fall ist der Mittelwert größer als der Median. Eine rechtssteile (linksschiefe) Verteilung ist durch einige unterdurchschnittlich kleine Werte geprägt. Hier ist der Median größer als der Mittelwert.\nZudem kann eine Verteilung auch Gleichverteilt, Bi- oder Multimodal sein. Im ersten Fall gibt es keinen Modus, also keinen Wert der Verteilung, der am Häufigsten vorkommt. In den letzteren Fällen gibt es ein oder mehrere Modi. Im Histogramm sind multimodale Verteilungen daran zu erkennen, dass sie typischerweise über zwei oder mehr \u0026ldquo;Gipfel\u0026rdquo; verfügen.\n3.6 Symmetrie und Schiefe (15 Min) Schau Dir die verschiedenen Histogramme im Bild an und charakterisiere jede einzelne Verteilung anhand von Schiefe, Symmetrie und Modus Die Grafik wurde mit dem oben angehängten Jupyter Notebook generiert. Hier kannst Du auch selber andere Verteilungen simulieren und visualisieren. Der Mittelwert und die Standardabweichung basieren auf den absoluten numerischen Werten der Beobachtungen. Deswegen können untypische sehr große oder sehr kleine Werte einer Verteilung (\u0026ldquo;Ausreißer\u0026rdquo;) diese Statistiken nach oben oder unten verzerren. Der Median und der Interquartilsabstand (IQR) hingegen basiert alleine auf der nach Größe sortierten Reihung der Beobachtungen und nicht auf den absoluten Werten. Deswegen sind diese Statistiken robust vor Ausreißern. Bei nicht-symmetrischen Verteilungen oder wenn Ausreißer vorliegen sollten deswegen immer auch robuste Statistiken mit angegeben werden. Ist die Verteilung Bi- oder Multimodal (s. nachfolgende Einheit) so können die Lagemaße Mittelwert und Median irreführend sein, da sie in der Regel nicht mit den \u0026ldquo;Gipfeln\u0026rdquo; der Verteilung (Modus) übereinstimmen. Verteilung der Ausleihen pro Kunde Mit dem folgenden Beispiel kannst Du ein Histogramm über die Anzahl der Ausleihen im Datensatz erstellen:\nimport pandas as pd import seaborn as sns %matplotlib inline sns.set_theme() df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) sns.distplot(df[\u0026#39;Total Checkouts\u0026#39;], kde=False) Das Histogram zeigt, dass die Verteilung der Ausleihen durch einige sehr große Ausreißer geprägt ist. Der Mittelwert liegt hier bei $\\bar{x} = 162$, während der Median $x_{0.5} = 19$ sehr viel kleiner ist. Das 95%-Quantil liegt bei $x_{0.95} = 816$ Ausleihen. Das heißt das 95% der Beobachtungen im Datensatz weniger als 816 Ausleihen getätigt haben.\n3.7 Exkurs: Ausreißerentfernung III (30 Min) Erstelle eine neue Spalte 'Total Checkouts Sqrt', die die Wurzel über die Spalte Total Checkouts enthält. Die Wurzel für jede Beobachtung kannst Du mit df['Total Checkouts']**(0.5) berechnen. Schaue Dir das Histogramm von 'Total Checkouts Sqrt' an und charakterisiere die Verteilung. Vergleiche diese Methode mit den vorherigen zwei Verfahren zur Ausreißerbehandlung (Aufgaben 3.3 und 3.5). Welche Methode findest Du besser geeignet, um mit fehlenden Werten umzugehen? Fallen Dir Vor- und Nachteile der jeweiligen Methoden ein? "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/bivariate/","title":"Bivariate Verteilungen","tags":[],"description":"","content":"Bisher haben wir immer nur einzelne Variablen betrachtet, zusammengefasst oder visualisiert. In vielen Fällen ist jedoch der Zusammenhang zwischen zwei Variablen von Interesse.\nNach diesem Kapitel solltest Du beispielsweise die folgenden Fragen beantworten können:\nLeihen ältere Bibliothekskunden im Schnitt mehr Bücher aus als jüngere? Führen Kunden, die häufiger Ausleihen tätigen, im Schnitt auch häufiger Verlängerungen durch? Nimmt die Anzahl der Ausleihen mit zunehmender Dauer der Mitgliedschaft ab? Zwei Variablen, die keinen Zusammenhang aufweisen, nennt man statistisch unabhängige Variablen. Für zwei metrische Variablen kann man außerdem zwischen einem positiven oder einem negativem Zusammenhang unterscheiden.\nBoxplot-Verteilung der Ausleihen nach Jahr der Registrierung. Das Notebook kannst Du Dir hier herunterladen.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/bivariate/cross_tables/","title":"Kreuztabellen","tags":[],"description":"","content":"Um zwei ordinale oder nominale Variablen miteinander zu vergleichen, eignen sich Kreuztabellen. Jeder Wert in der Kreuztabelle entspricht der Anzahl der Beobachtungen im Datensatz mit genau dieser Kombination an Merkmalsausprägungen.\nHier ein Beispiel (mit dem Argument na_values=\u0026quot;none\u0026quot; markiert pandas die \u0026quot;none\u0026quot; Einträge in der Spalte 'Notice Preference Definition' als fehlende Werte):\nimport pandas as pd df = pd.read_csv( \u0026#34;../data/Library_Usage.csv\u0026#34;, na_values=\u0026#34;none\u0026#34; ) pd.crosstab( df[\u0026#39;Notice Preference Definition\u0026#39;], df[\u0026#39;Age Range\u0026#39;], margins=True ) Age Range 0 to 9 years 10 to 19 years 20 to 24 years 25 to 34 years 35 to 44 years 45 to 54 years 55 to 59 years 60 to 64 years 65 to 74 years 75 years and over All Notice Preference Definition Email 28740 54936 22701 88200 77618 45165 17336 15539 27170 15069 392474 None 3952 11921 2680 4469 4101 3154 1740 2115 4544 4228 42904 All 32692 66857 25381 92669 81719 48319 19076 17654 31714 19297 435378 Eine Kreuztabelle mit absoluten Werten ist häufig schwer zu interpretieren, wenn die Randverteilungen ungleich verteilt sind. Deswegen sollten die Werte entweder Spaltenweise oder Zeilenweise normalisiert werden:\npd.crosstab( df[\u0026#39;Notice Preference Definition\u0026#39;], df[\u0026#39;Age Range\u0026#39;], margins=True, normalize=1 ) Ergibt eine Normalisierung der Spalten, sodass sich diese jeweils zu 100% aufaddieren:\nAge Range 0 to 9 years 10 to 19 years 20 to 24 years 25 to 34 years 35 to 44 years 45 to 54 years 55 to 59 years 60 to 64 years 65 to 74 years 75 years and over All Notice Preference Definition Email 0.879114 0.821694 0.894409 0.951775 0.949816 0.934725 0.908786 0.880197 0.856719 0.780899 0.901456 None 0.120886 0.178306 0.105591 0.048225 0.050184 0.065275 0.091214 0.119803 0.143281 0.219101 0.098544 Von den Nutzern zwischen 0 und 9 Jahren möchten 88% (0.879114 von 1) per Mail informiert werden.\nWird das Argument normalize=0 verwendet, so werden die Zeilen der Tabelle normalisiert. Entsprecht ändern sich die Interpretation:\nAge Range 0 to 9 years 10 to 19 years 20 to 24 years 25 to 34 years 35 to 44 years 45 to 54 years 55 to 59 years 60 to 64 years 65 to 74 years 75 years and over Notice Preference Definition Email 0.073228 0.139974 0.057841 0.224728 0.197766 0.115078 0.044171 0.039592 0.069228 0.038395 None 0.092113 0.277853 0.062465 0.104163 0.095585 0.073513 0.040556 0.049296 0.105911 0.098546 All 0.075089 0.153561 0.058296 0.212847 0.187697 0.110982 0.043815 0.040549 0.072842 0.044322 Von den Kunden, die per Mail informiert werden möchtem befinden sich ca. 12% (0.115078 von 1) in der Altersgruppe 45 bis 54 Jahre.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/bivariate/correlation/","title":"Korrelation","tags":[],"description":"","content":"Für zwei metrische Variablen lässt sich der Zusammenhang über die sog. Kovarianz berechnen.\nWenn die Variablen mit $x$ und $y$ bezeichnet werden, ergibt sich die Kovarianz aus der Formel:\n$$ s_{x, y}^2 = \\frac{1}{N-1}\\sum_{i=1}^{N}(x_i-\\bar{x})(y_i-\\bar{y}) , $$\nwobei $\\bar{x}$ und $\\bar{y}$ die entsprechenden Mittelwerte darstellen und $N$ die Größe der Stichprobe (die Anzahl an Elementen in der Datenreihe von $x$ oder $y$).\nEin positiver Wert der Kovarianz drückt aus, dass wenn die Werte der einen Variablen steigen, dies auch für die andere Variable gilt. Eine negative Kovarianz bedeutet hingegen, dass wenn die Werte der einen Variablen steigen, die Werte der anderen Variablen sinken.\nAuch wenn die Kovarianz mit der Stärke des Zusammenhangs steigt, ist es immer noch relativ schwierig, aus dem errechneten Wert herauszufinden, wie stark der Zusammenhang zwischen den Variablen ist.\nZusätzlich zur Kovarianz, ist der Korrelationskoeffizient eine wichtige Kennzahl. Der Korrelationskoeffizient $\\rho_{x, y}$ misst für zwei metrische Variablen $x$ und $y$ die Stärke des linearen Zusammenhangs. Man sagt auch, dass der Korrelationskoeffizient die standardisierte Kovanrianz darstellt.\nDer Korrelationskoeffizient ist definiert als: $$ \\rho_{x, y} = \\frac{s_{x, y}^2}{s_xs_y} = \\frac{\\sum_{i=1}^{N}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{N}(x_i-\\bar{x})^2\\sum_{i=1}^{N}(y_i-\\bar{y})^2}} . $$\nIn dieser Formel erkennt man im Zähler die Kovarianz und im Nenner die einzelnen Varianzen (die entsprechenden $N-1$-Werte kürzen sich bei Austellen der Formel weg).\nDer Koeffizient kann Werte zwischen $-1$ (negativer Zusammenhang) und $1$ (positiver Zusammenhang) annehmen. Nachfolgend ein Beispiel von zufällig generierten Variablen mit verschiedenen Korrelationskoeffizienten: Zugehöriges Notebook zum Nachvollziehen und Ausprobieren: correlation.ipynb (168 ko) Mit pandas kannst Du natürlich auch Korrelationen ausrechnen: import pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Total Checkouts\u0026#39;].corr(df[\u0026#39;Total Renewals\u0026#39;]) Auch wenn durch die Kovarianz und Korrelationskoeffizienten mathematische Zusammenhänge zwischen Variablen berechnet werden können: große positive oder negative Korrelationen sind kein Indiz für kausale Zusammenhänge!\nBeispiel für Scheinkorrelation in Zeitreihen ($\\rho_{x,y}=0.99$) Quelle: tylervigen.com\n3.8 Exkurs: Anscombe-Quartett (30 Min) Das Anscombe Quartett ist ein Datenstatz, der aus 4 bivariaten Verteilungen besteht. Über die Spaltennamen ['x1', 'y1'], ['x2', 'y2'], ..., ['x4', 'y4'] können die zusammengehörenden Datenpaare ausgewählt werden.\nLies den Datensatz ein. Berechne den Mittelwert, Median und die Standardabweichung der Spalten. Berechne jeweils die Korrelation zweier zusammenhängender Spalten [x\u0026lt;i\u0026gt;, y\u0026lt;i\u0026gt;]. Erstelle jeweils ein Streudiagram zweier zusammenhängender Spalten [x\u0026lt;i\u0026gt;, y\u0026lt;i\u0026gt;]. Was fällt Dir auf? Informiere Dich über den Datensatz hier. "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/ml/","title":"Einordnung: Statistik, Data Science und Machine Learning","tags":[],"description":"","content":"Seit einigen Jahren sind Data Science und Machine Learning zu alltäglichen Begriffen geworden. Studiengänge im Bereich Data Science werden neu eingerichtet oder schon bestehende Abschlüsse umbenannt. Anwendungen des Maschinellen Lernens sind Thema in Massenmedien. Der Zertifikatskurs Data Librarian spiegelt diese Popularität wider.\nUm die Zusammenhänge zwischen den Fachgebieten besser zu verstehen, wird im Folgenden ein kurzer Überblick über die Begriffe gegeben. Für Interessierte gibt es Verweise zu weiteren Quellen.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/pandas/","title":"Praktische Einführung in Pandas","tags":[],"description":"","content":"Grundlage der statistischen Analyse sind Datentabellen: Jede Zeile der Tabelle entspricht einer Beobachtung. Jede Spalte entspricht einer statistischen Variable. Neue Beobachtungen und Variablen können dadurch einfach an die schon bestehende Tabelle angefügt werden.\nWenn es um Statistik und Programmierung geht werden mit \u0026ldquo;Variable\u0026rdquo; oft zwei unterschiedliche Dinge benannt:\nVariable im Kontext eines statistischen Merkmals, das in der Regel als Spalte eines Datensatzes vorliegt. Variablen im Kontext von Programmiersprachen beschreiben benannte Referenzen auf bestimmte Datenstrukturen oder Objekte (z.B. numbers = [1, 2, 3]). Am Beginn jeder statistischen Analyse steht die Aufbereitung und Bereinigung der Daten. Damit ist die Behandlung von fehlenden oder falsch kodierten Werten, die Umkodierung und Transformation von statistischen Variablen oder die Berechnung neuer Spalten gemeint. Oft sind auch nur Untergruppen von Beobachtungen mit bestimmten Merkmausausprägungen von Interesse.\nViele statistische Methoden erfordern auch, dass die Daten nur als numerische Werte vorliegen. Daher müssen ordinale oder nominale Variablen, die als Text gespeichert sind (zum Beispiel ['male', 'female', 'female', ...]) in entsprechende numerische Werte umkodiert werden. Dabei wird jeder Kategorie ein numerischer Wert zugeordnet.\nDas Standard-Paket um mit Datentabellen in Python zu arbeiten, ist pandas. Das folgende Kapitel stellt anhand von vielen praktischen Beispielen und Übungen die grundlegenden Konzepte in pandas vor.\nVorbereitung für die nachfolgenden Aufgaben in Einheit 2 Erstelle ein Jupyter Notebook in Deinem Projektordner unter ./notebooks mit dem Namen pandas_introduction.ipynb. Führe die Beispiele in den nachfolgenden Abschnitten aus und versuche, die Aufgaben zu lösen. Nach diesem Kapitel solltest Du die folgenden Fragen beantworten können: Wie kann ich Tabellendaten in pandas einlesen? Wie werden Daten in pandas angeordnet? Welche Datentypen können Spalten eines DataFrames annehmen? Wie kann ich einzelne Spalten oder Zeilen eines DataFrames auswählen? Deine Verzeichnisstruktur vom Projektordner sollte jetzt ungefähr so aussehen:\nModule_3 ├── data │ └── Library_Usage.csv ├── notebooks │ ├── pandas_introduction.ipynb │ └── example_jupyter.ipynb What is Pandas? Introduction Video by Giles McMullen ( Untertitel auswählbar) Dieses Cheat-Sheet gibt einen guten Überblick über die Datenverarbeitung mit Pandas.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/reflection/","title":"Reflexion: Datenprojekte an der eigenen Arbeitsstelle","tags":[],"description":"","content":" 2.10 Datenprojekte an der eigenen Arbeitsstelle (20 Min) Schreibe einen kurzen Text über die Verwendung von Daten und quantitativen Methoden an Deinem Arbeitsplatz. Denk dabei über folgende Fragen nach:\nWelche Daten sind an Deiner Arbeitsstelle vorhanden? Mit welchen Daten arbeitest Du oder würdest Du gerne arbeiten? Werden statistische Verfahren oder Maschinelles Lernen schon an Deiner Arbeitsstelle eingesetzt? Welche Fragen oder Phänomene würdest Du gerne untersuchen? Was fändest Du spannend herauszufinden? Teile Deinen Text bis zum 24.02.2024 mit den anderen KursteilnehmerInnen im Forum zu Modul 3 auf der Moodle-Kursplattform. Wenn Du für die anderen lieber anonym bleiben möchtest, kannst Du mir auch den Text schicken und ich stelle ihn dann rein.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/visualizations/","title":"Visualisierungen mit Seaborn","tags":[],"description":"","content":" Im folgenden Abschnitt wird ein Überblick über verschiedene Visualisierungsformen gegeben und anhand von Beispielen gezeigt, wie diese in Python mit der Bibliothek seaborn programmiert werden können.\nDas Thema Visualisierungen ist komplex: Es gibt sehr viele Parameter und Stellschrauben, die man auswendig lernen oder in den Dokumentationen der Bibliotheken pandas, seaborn und matplotlib Nachschlagen muss. Die Erstellung von eindrucksvollen und aussagekräftigen Graphiken erfordert viel Praxiserfahrung, Zeit und Mühe. Die Einführungsseite von seaborn ist sehr hilfreich um eine erste Eindruck zu gewinne.\nAuf der anderen Seite lassen sich schon mit wenig Programmcode annehmbare Visualisierungen erstellen. Am besten orientierst Du Dich an den zahlreiche Beispielen online und änderst den Code Deinen Erfordernissen an. Nach dem Kapitel solltest Du die folgenden Fragen beantworten können:\nWie können uni- und bivariate Verteilungen in Pandas visualisiert werden? Wie interpretiere und erstelle ich Boxplots, Histogramme und Streudiagramme? Das Buch \u0026ldquo;Fundamentals of Data Visualization\u0026rdquo; (Claus O. Wilke, O\u0026rsquo;Reilly, 2019) vermittelt ein sehr gutes Grundverständnis wie man Daten effektiv in Visualisierungen übersetzt. Das Buch steht auch unter CC-BY-SA-Lizenz online zur Verfügung. Zumindest das Kapitel Visualizing data: Mapping data onto aesthetics sollte man sich unbedingt anschauen. Eine weitere gute, offene Quelle für Tipps zur Visualisierung stellt Serie \u0026ldquo;Points of View\u0026rdquo; von Nature Methods dar. \u0026quot;\nDieses Cheat-Sheet gibt einen guten Überblick über die Erstellung von Plots mit Seaborn.\n3.9 Balkendiagramme bei Fox News (15 Min) Was fällt Dir an den folgenden Diagrammen von FoxNews auf? Was würdest Du anders machen? Welche Botschaft wollten die \u0026ldquo;Designer\u0026rdquo; vermutlich vermitteln? Passt die Botschaft mit den Daten zusammen? (s. auch Quelle und Hintergründe oder auch dieses Beispiel) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/visualizations/tutorial/","title":"Tutorial","tags":[],"description":"","content":" Statistical analysis is a process of understanding how variables in a dataset relate to each other and how those relationships depend on other variables. Visualization can be a core component of this process because, when data are visualized properly, the human visual system can see trends and patterns that indicate a relationship. https://seaborn.pydata.org/tutorial/relational.html\nDie zahlreichen Funktionen, die seaborn bietet basieren immer auf dem gleichen Prinzip. Visualisiert werden (nominale, ordinale, metrische) Variablen eines Datensatzes, die in Form eines DataFrames vorliegen. Das Skalenniveau der Variablen bestimmt dabei die Art der Visualisierung. Variablen können verschiedenen Eigenschaften des Diagramms zugeordnet werden (z.B. die Punktgröße oder Farbe der Balken). Am besten lässt sich das Prinzip an einem Beispiel erkennen.\nGrundlagen Zuerst werden die benötigten Bibliotheken importiert und der Datensatz eingelesen. Panadas und numpy dienen zur Verarbeitung der Daten. Seaborn baut auf matplotlib, der Standard-Bibliothek für wissenschaftliche Grafiken, auf und bietet einige Verbesserungen und Vereinfachungen. Der Kommentar % matplotlib inline ist ein Magic Command für den Python Kernel in Jupyter Notebooks. Mit diesem Befehl werden Plots direkt im Notebook angezeigt.\nMit sns.set_theme() wird das grundlegende Design der Plots dem Design von Seaborn angepasst. Die Funktion DataFrame.sample() wird verwendet, um eine Zufalsstichprobe der Größe $n=1000$ aus dem Datenstatz zu erstellen. Dies dient zur besseren Lesbarkeit des Plots in diesem Tutorial.\nimport pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np %matplotlib inline sns.set_theme() # use sample to generate a random subsample df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;).sample(n=1000) Der erste Plot sns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, data=df) Die Funktion relplot() erzeugt ein Streudiagram zur Visualisierung einer bivariaten Verteilung mit metrischen Variablen. Jedes Wertepaar $(x_i,y_i)$ der Verteilung wird im Koordinatensystem als Punkt dargestellt. Die Variablen des Datensatzes werden über ihre Spaltennamen mit den Axen x und y des Plots verlinkt.\nPlotgröße Die Größe des Plots kann über die beiden Argumente height (Höhe in inches und aspect (Breite des Plots ergibt sich aus aspect * height) konfiguriert werden:\nsns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, data=df height=5, aspect=3) Speichern Um den Plot als Bilddatei abzuspeichern wird die Funktion savefig() aus der matplotlib Bibliothek verwendet:\nplt.savefig(\u0026#39;../tutorial.png\u0026#39;, dpi=150) Weitere Eigenschaften Mit den zusätzlichen Argumenten hue (Farbe der Punkte/ Linien/ Balken \u0026hellip;), size (Größe der Punkte/ Linien/ Balken \u0026hellip;), style können weitere Eigenschaften des Plots angepasst werden:\nsns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, hue=\u0026#39;Provided Email Address\u0026#39;, style=\u0026#39;Within San Francisco County\u0026#39;, size=\u0026#39;Year Patron Registered\u0026#39;, data=df) Die vollständige Liste aller Optionen kann hier eingesehen werden.\nPlot-Gitter Plots können mit dem row und/ oder col Argument auch anhand einer oder mehrerer Variablen in Form von Spalten und/ oder Zeilen angeordnet werden:\nsns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, size=\u0026#39;Year Patron Registered\u0026#39;, row=\u0026#39;Provided Email Address\u0026#39;, col=\u0026#39;Within San Francisco County\u0026#39;, data=df) Diese Form der Visualisierung ist in der Regel einem einzelnen Plot mit einer langen Legende vorzuziehen.\nFarbpaletten Mit dem Argument palette können verschiedene Farbpaletten für den hue Parameter ausgewählt werden. Zu empfehlen sind die Paletten von ColorBrewer:\nsns.relplot(x=\u0026#39;Total Checkouts\u0026#39;, y=\u0026#39;Total Renewals\u0026#39;, hue=\u0026#39;Provided Email Address\u0026#39;, col=\u0026#39;Within San Francisco County\u0026#39;, palette=sns.color_palette(\u0026#39;Accent\u0026#39;, 2), data=df) Mit der Funktion sns.color_palette können verschiedene Paletten anhand ihres Namens ausgewählt werden. Wichtig ist hierbei, die Anzahl der benötigten Farben mit anzugeben.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/visualizations/examples/","title":"Weitere Beispiele","tags":[],"description":"","content":"Im Tutorial hast Du gesehen, wie Du ein Streudiagramm erstellen kannst. Hier werden exemplarisch weitere Möglichkeiten gezeigt, die Daten des Datensatzes zu visualisieren. Die wichtigste Funktion ist hierbei sns.catplot().\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np # matplotlib inline sns.set_theme() df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) Nominale und ordinale Variablen Univariate Häufigkeits- und Bivariate Kreuztatabellen können mit Balkendiagrammen visualisiert werden:\nsns.catplot(y=\u0026#39;Year Patron Registered\u0026#39;, data=df, kind=\u0026#39;count\u0026#39;, color=\u0026#34;steelblue\u0026#34;) sns.catplot(y=\u0026#39;Age Range\u0026#39;, hue=\u0026#39;Provided Email Address\u0026#39;, data=df, kind=\u0026#39;count\u0026#39;) sns.catplot(x=\u0026#39;Patron Type Definition\u0026#39;, data=df, kind=\u0026#39;count\u0026#39;, col=\u0026#39;Year Patron Registered\u0026#39;, col_wrap=4) Metrische Variablen Univariate Verteilungen werden mit Histogrammen oder Kernel-Dichte Schätzern visualisiert:\n# Histogram sns.distplot(df[\u0026#39;Total Renewals\u0026#39;], kde=False) # With density estimation sns.distplot(df[\u0026#39;Total Renewals\u0026#39;], kde=True) Kombination aus metrischen und nominalen/ ordinalen Variablen # Swarmplot sns.catplot(x=\u0026#39;Year Patron Registered\u0026#39;, y = \u0026#39;Total Renewals\u0026#39;, data=df, kind=\u0026#39;swarm\u0026#39;, color=\u0026#34;steelblue\u0026#34;, aspect=4) # Boxplot sns.catplot(col=\u0026#39;Year Patron Registered\u0026#39;, y = \u0026#39;Total Renewals\u0026#39;, data=df, kind=\u0026#39;box\u0026#39;, color=\u0026#34;steelblue\u0026#34;, aspect=4) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/quiz_pandas/","title":"Recap: Quiz","tags":[],"description":"","content":" "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/quiz_statistics/","title":"Recap: Quiz","tags":[],"description":"","content":" "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/solutions/","title":"Musterlösungen","tags":[],"description":"","content":"2. Grundlagen der Datenanalyse in Python 2.1 Grundbegriffe Wie viele Merkmale besitzt der Datensatz? 14. Wie groß ist die Stichprobengröße des Datensatzes? 436290. Wer oder was sind die Merkmalsträger? Bibliothekskunden der SF Public Library. Von wann bis wann wurden die Daten erhoben? Das Bibliothekssystem wurde 2003 installiert. Die Daten reichen bis 2023. Wie lässt sich die Grundgesamtheit beschreiben? Handelt es sich um eine Vollerhebung? Grundgesamtheit sind alle Bibliothekskunden der San Francisco Library. Wahrscheinlich handelt es sich um eine Vollerhebung. Es lässt sich diskutieren, ob die Grundgesamtheit größer gefasst werden kann (z.B. alle Kunden von Bibliotheken in den USA oder alle Kunden von öffentlichen Bibliotheken). Sind die Daten repräsentativ für diese Grundgesamtheiten? Welche Merkmale sind stetig? Welche diskret? Die Variablen Total Checkouts und Total Renewals sind stetig, alle anderen diskret. Welchem Skalenniveau entsprechen die einzelnen Merkmale (Nominal-, Ordinal- oder Metrische Skala)? Metrisch: Total Checkouts, Total Renewals, Circulation Active Year, Year Patron registered, Ordinal: Age Range, Nominal: Der Rest. Enthält der Datensatz fehlende Werte? Ja, z.B. Age Range Handelt es sich um Querschnitts-, Längsschnitss- oder Paneldaten? Querschnittsdaten Lösungen 2.1.solutions_basicterms.ipynb (72 ko) 2.2 Skalenniveau und Datentypen month: nominal (oder ordinal, wenn z.B. Dez \u0026gt; Jan gilt.), diskret, object temp: metrisch, stetig, int below_zero: nominal, diskret, boolean Lösungen 2.2.solutions_dataframe.ipynb (1 ko) 2.3 Exkurs: Datenrundreise Lösungen 2.3.solutions_datenrundreise.ipynb (36 ko) 2.4 Exkurs: Arbeitsspeicher In Linux kann z.B. mit dem free Kommando der freie Speicherplatz ermittelt werden. Bei 8 Gigabyte Arbeitsspeicher werden ca. 3 Gigabyte vom System verbraucht. 5 Gigabyte sind 5 000 000 000 Bytes. Somit können theoretisch 625 000 000 Zahlen vom Typ int64 eingelesen werden. Eine Tabelle mit 100 Variablen kann somit 6.25 Millionen Beobachtungen enthalten. Der Library Datensatz verbraucht ca. 220 Megabyte im Arbeitsspeicher (df.info(memory_usage='deep')). 2.5 Fallstudie: Feature Engineering Lösungen 2.5.solutions_fe.ipynb (19 ko) 2.6 Filtern Lösungen 2.6.solutions_selection.ipynb (4 ko) 2.7 Datentypen None ist ein spezieller Datentyp in Python der fehlende Objekte oder Variablen bezeichnet. 5 ist eine ganzzahlige Zahl vom Typ int True ist eine binäre Zahl vom Typ boolean Die entsprechenden Ausdrücke in \u0026quot;\u0026quot; repräsentieren jeweils einen Text vom Typ str (oder object in pandas) 2.8 Exkurs: Fehlende Werte Lösungen 2.8.solutions_na.ipynb (4 ko) Quiz https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html len(df) oder df.shape[0] df['Age Range'].isna().sum() len(df[(df['Age Range'] == '60 to 64 years') \u0026amp; (df['Circulation Active Year'] == 2016)]) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/quiz_intro/","title":"Recap: Quiz","tags":[],"description":"","content":" "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/solutions/","title":"Musterlösungen","tags":[],"description":"","content":"3. Deskriptive Statistik und Visualisierungen 3.1 Häufigkeiten Lösungen 3.1.solutions_frequency.ipynb (5 ko) 3.2 Mittelwert und Median Der Median ist robust gegenüber Ausreißern, da er nicht auf den absoluten sondern nur auf der relativen Reihung der Beobachtungen basiert. Wird beispielsweise der größte Wert einer Messreihe um den Faktor 1000 tausend vergrößert, so ändert sicht der Median nicht.\nDer Mittelwert hingegen basiert auf den absoluten Werten. Da die Variable Total Checkouts einige wenige sehr große Ausreißer enthält, ist der Mittelwert hier viel größer.\n3.4 Varianz Lösungen 3.4.solutions_variance.ipynb (1 ko) 3.6 Symmetrie und Schiefe Von oben links nach unten rechts:\nBimodal, Symmetrisch Unimodal, Linksschief/ Rechtssteil Unimodal, Linksschief/ Rechtssteil Unimodal, Rechtsschief/ Linkssteil Kein Modus, Symmetrisch, Gleichverteilung Unimodal, Linksschief/ Rechtssteil Unimodal, Rechtsschief/ Linkssteil Unimodal, Rechtsschief/ Linkssteil Unimodal, Symmetrisch Exkurs Ausreißerentfernung (Aufgaben 3.3, 3.5 und 3.7) Lösungen solutions_outlier.ipynb (207 ko) 3.8 Exkurs: Anscombe-Quartett Lösungen solutions_anscombe.ipynb (37 ko) 3.9 Balkendiagramme bei Fox News Die Balkendiagramme beginnen nicht im Nullpunkt. Somit werden die relativen Unterschiede viel größer dargestellt, als sie in Wahrheit sind.\nQuiz rechtsschief df['Provided Email Address'][df['Age Range'] == '0 to 9 years'].sum() (überlege, was dieser Ausdruck liefert und setze ihn in Relation mit der Gesamtzahl der 0-9-jährigen) df['Total Checkouts'].quantile(0.60) df['Age Range'].mode() df['Total Renewals'].quantile([0.25, 0.75]).diff() Projektaufgabe - Beispielfragen Lösungen Musterloesung_Projektaufgabe.ipynb (178 ko) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/organisation/solutions/","title":"Musterlösungen","tags":[],"description":"","content":"1. Kursorganisation und Vorbereitung Quiz Strg+Enter 436290, len(df) siehe (unter sns.set_theme()): https://seaborn.pydata.org/introduction.html "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_basics/quiz_machine_learning_basics/","title":"Recap: Quiz Maschinelles Lernen","tags":[],"description":"","content":" "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/bootstrap/","title":"Das Bootstrapping Verfahren","tags":[],"description":"","content":"\nDas Ziel der Inferenzstatistik ist es, aus einer einzelnen Stichprobe $x_1, \\dots, x_n$ die Stichproben-Verteilung eines Schätzers, wie dem Mittelwert $\\bar{x}$ oder dem Median $x_{0.5}$, herzuleiten. Wenn die Stichproben-Verteilung eines Schätzers vorliegt kann damit der Wert des tatsächlichen unbekannten Populationsparameters eingegrenzt werden.\nFür viele Schätzer kann deren Stichproben-Verteilung theoretisch hergeleitet werden. Neben der theoretischen Herangehensweise, gibt es auch eine intuitive empirische Methode, das Bootstrapping-Verfahren. Es basiert auf der Simulation von vielen Stichproben. Simulation bedeutet, dass die Stichproben nicht real erhoben, sondern alle aus der einzigen vorhanden Stichprobe erstellt werden.\nEine einzelne Bootstrapping-Stichprobe erhält man, indem aus der vorhanden Stichprobe der Größe $n$, genau $n$ Beobachtungen mit Zurücklegen zufällig gezogen werden. Das bedeutet, dass Beobachtungen mehrmals in der simulierten Stichprobe vorkommen können.\nBeispiel Nimm an, dass die Stichprobe die folgenden $n=7$ Werte enthält:\nimport pandas as pd x = pd.Series([21, 13, 8, 14, 10, 12, 5]) x.mean() Eine simulierte Bootstrapping-Stichprobe erhältst Du, indem Du aus der vorhandenen Stichprobe genau $n=7$ Werte mit Zurücklegen (replace=True) zufällig auswählst:\nx.sample(n=len(x), replace=True) Für jede simulierte Stichprobe wird daraufhin der zu interessierende Schätzwert berechnet. Um möglichst exakte Ergebnisse zu erhalten sollten mindestens $S \\geq 5000$ Simulationen durchgeführt werden. Man erhält damit eine Annäherung an die tatsächliche Stichprobenverteilung des Schätzwerts:\nBeispiel (Fortsetzung) Wir erstellen eine Bootstrapping-Verteilung für den Stichproben-Mittelwert. Die Anzahl der Simulationen wird auf $S=10000$ festgelegt. Mit einer for Schleife wird die Simulation wiederholt. In jeder Simulation wird eine Bootstrapping-Stichprobe erstellt und deren Mittelwert berechnet.\nx_means = [] S=10000 for i in range(S): x_mean = x.sample(n=len(x), replace=True).mean() x_means.append(x_mean) Die Mittelwerte jeder Simulation werden in der Liste x_means abgespeichert. Die Liste enthält nun eine empirische Stichprobenverteilung des Mittelwerts. Nun kannst Du Dir die Verteilung des Stichproben-Mittelwertes beispielsweise in einem Histogramm ansehen:\n#matplotlib inline import seaborn as sns sns.set_theme() sns.distplot(x_means, kde=False, bins=35) Wie viele Mittelwerte liegen zwischen 9 und 11?\nx_means = pd.Series(x_means) x_means.between(9,11).mean() 4.1 Bootstrapping-Verfahren Lies Sie den Datensatz ein. Um eine homogene Stichprobe zu erhalten filtere nach Bibliothekskunden die sich im Jahr 2010 registriert haben und auch noch im Jahr 2016 (als der Datensatz erstellt wurde) aktiv waren. [Achtung: Die Spalte 'Circulation Active Year' wird standardmäßig als Text eingelesen] Betrachte die Variable 'Total Renewals'. Wie viele Verlängerungen wurden im Mittel durchgeführt? Erstelle, wie oben beschrieben, eine Stichprobenverteilung für den Mittelwert. Wie viel Prozent der Stichproben-Mittelwerte liegen zwischen 89 und 92 Verlängerungen? Wie groß musst Du das Intervall wählen, so dass 90% aller Bootstrapping-Mittelwerte darin liegen? [Tipp: Nutze die Funktion pandas.Series.quantile] "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/significance/","title":"Konfidenzintervalle und Signifikanz","tags":[],"description":"","content":"\nIm vorherigen Beispiel hast Du mit Hilfe des Bootstrapping-Verfahrens die Stichprobenverteilung geschätzt. Wenn die Stichprobenverteilung bekannt ist, können damit Aussagen über den tatsächlichen Parameter in der Population (im Bild mit $\\mu$ bezeichnet) getroffen werden.\nEine häufig angewandte Methode sind Konfidenzintervalle (KI). Man gibt einen Bereich aus der Stichprobenverteilung des Schätzwertes an, der den wahren Wert in der Population mit hoher Wahrscheinlichkeit überdeckt. Die Wahrscheinlichkeit wird mit $1-\\alpha$ angegeben. Der Wert $\\alpha$ wird Signifikanzniveau genannt und vor der Bestimmung des Intervalls festgelegt. Üblicherweise wird $\\alpha=0.10$, $\\alpha=0.05$, oder $\\alpha=0.01$ gesetzt.\nEin breites Konfidenzintervall zeigt auf, dass die Schätzwerte stark schwanken und dass der wahre Populationsparameter deswegen nur sehr ungenau bestimmt werden kann. Mit größerer Stichprobengröße verkleinert sich in der Regel der Stichprobenfehler und damit auch das Intervall: Es können präzisere Aussagen über die Population getroffen werden.\nMit einem niedrigeren Signifikanzniveau $\\alpha$ kann sichergestellt werden, dass das KI den wahren Wert mit höherer Wahrscheinlichkeit überdeckt. Diese geringere Fehlertoleranz hat jedoch ein breiteres und damit weniger präzises Intervall zur Folge.\nDas Konfidenzintervall kann aus der mit dem Bootstraping-Verfahren angenäherten Stichprobenverteilung geschätzt werden: Es entspricht genau den entsprechenden Quantilen der geschätzten Stichprobenverteilung: Soll beispielsweise ein 90%-KI zum Signifikanzniveau von $\\alpha=0.10$ erstellt werden, so lässt sich die untere Grenze aus der Verteilung als $\\bar{x}_\\frac{\\alpha}{2} = \\bar{x}_{0.05}$ ablesen. Die obere Grenze als $\\bar{x}_{1-\\frac{\\alpha}{2}} = \\bar{x}_{0.95}$. Somit ist sichergestellt, dass 90% aller mit dem Bootstrapping Verfahren ermittelter Stichprobenmittelwerte innerhalb dieses Intervalls liegen. Damit überdeckt das KI mit 90% Wahrscheinlichkeit den wahren Populationsparameter $\\mu$.\nMit dem Bootstrapping Verfahren kann man nicht nur Konfidenzintervalle für den Mittelwert angeben, sondern auch für viele weitere Statistiken, wie den Median oder den Korrelationskoeffizienten zwischen zwei Variablen.\nBeispiel (Fortsetzung) Die Stichprobenmittelwerte aus der Bootstraping-Simulation werden zuerst in eine Series umgewandelt. Mit der Funktion quantile können die Quantile der Stichprobenverteilung bestimmt werden. Sie entsprechen dem geschätzten Konfidenzintervall zum Signifikanzniveau $\\alpha$.\nx_means = pd.Series(x_means) alpha = 0.10 x_means.quantile((alpha/2,1-alpha/2)) 4.2 Signifikanzniveau Berechne ein 90%-Konfidenzintervall jeweils für den Mittelwert und den Median der Variablen Total Renewals und Total Checkouts.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/two-sample-test/","title":"Mittelwertvergleiche","tags":[],"description":"","content":"Ein häufiges Problem bei der statistischen Datenanalyse ist die Frage, ob signifikante Unterschiede in den Mittelwerten zweier Subpopulationen bestehen: Leihen Frauen beispielsweise im Mittel signifikant mehr aus als männliche Bibliothekskunden? Tätigen Kunden im Ruhestand im Mittel weniger Verlängerungen als junge Kunden?\nVon einem signifikanten Unterschied spricht man, wenn die Differenz zwischen den Mittelwerten zweier Stichproben so groß ist, dass es sehr Unwahrscheinlich ist, dass dieser Unterschied alleine aufgrund der rein zufälligen Schwankungen durch die Stichprobenziehung entstanden ist.\nWenn wir mit $\\mu_x$ und $\\mu_y$ die wahren aber unbekannten Mittelwerte in der Population bezeichnen und mit $\\Delta_{xy}$ die Differenz dieser Mittelwerte, dann lautet unsere Hypothese:\n$$ H_0: \\mu_x = \\mu_y \\iff \\Delta_{xy} = \\mu_x - \\mu_y = 0. $$\nWenn diese Hypothese zutrifft, dann gibt es keine Unterschiede in den Mittelwerten der beiden Populationen.\nWie lässt sich diese Hypothese nun anhand von zwei Stichproben $x = x_1, \\dots, x_{n_x}$ und $y = y_1, \\dots, y_{n_y}$ und deren Mittelwerten $\\bar{x}$ und $\\bar{y}$ statistisch überprüfen?\nWir können das bisherige Bootstrapping-Verfahren auch hier anwenden, um die Stichprobenverteilung von $d_{xy} = \\bar{x}-\\bar{y}$ zu schätzen. In jeder Simulation $s = 1, \\dots, S$ erstellen wir eine Bootstrapping-Stichprobe von $x$ und $y$ und berechnen darauf die Differenz $d_{xy}$ über die Mittelwerte.\nBeachte, dass sich das grundlegende Verfahren und die Bestimmung der Konfidenzintervalle im Vergleich zur vorherigen Lektion nicht ändert. Einzig der zu interessierende Schätzwert wird ausgetauscht: Wo vorher nur die Stichprobenverteilung von $\\bar{x}$ bestimmt wurde, ist es nun die Verteilung von $d_{xy} = \\bar{x}-\\bar{y}$.\nZum Signifikanzniveau $\\alpha=0.05$ erhalten wir mit den Quantilen $d_{\\frac{\\alpha}{2}}$ und $d_{1-\\frac{\\alpha}{2}}$ das 95%-Konfidenzintervall für die Mittelwertdifferenz. Mit einer Wahrscheinlichkeit von $1-\\alpha = 95\\%$ überdeckt das Konfidenzintervall damit den unbekannten Populationsparameter $\\Delta_{xy} = \\mu_x - \\mu_y$.\nLiegt das Konfidenz-Intervall für die Differenz zweier Mittelwerte ausschließlich im positiven oder negativem Bereich (also außerhalb des Nullpunktes), können wir daraus schließen, dass die beiden Stichproben mit hoher Evidenz unterschiedlichen Populationen mit verschiedenen Mittelwerten entstammen. Die aufgestellte Hypothese kann deswegen abgelehnt werden.\nÜberdeckt das KI hingegen die Null, so kann nicht ausgeschlossen werden, dass die wahre Mittelwertdifferenz Null ist. In diesem Fall können wir die Hypothese nicht ablehnen.\nimport pandas as pd alpha = 0.05 S=10000 x = pd.Series([3, 3, 5, 8, 7, 3, 2, 5, 8, 1]) y = pd.Series([3, 10, 9, 8, 2, 3, 6, 7, 11, 6]) print((x.mean(), y.mean())) dxy = [] for i in range(S): x_mean = x.sample(n=len(x), replace=True).mean() y_mean = y.sample(n=len(y), replace=True).mean() dxy.append(x_mean - y_mean ) pd.Series(dxy).quantile((alpha/2, 1-alpha/2)) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/solutions/","title":"Musterlösungen","tags":[],"description":"","content":" 4. Exkurs: Inferenzstatistik Lösungen solutions_inference.ipynb (12 ko) "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/classification/","title":"Klassifikation","tags":[],"description":"","content":"Bitte erstell ein neues Jupyter-Notebook und nennen es Klassifikation.\nWir nutzen ein Datenset, das handschriftliche Ziffern in Form von 8x8 Feldern mit Werten der Farbstärkte darstellt. Eine Beschreibung des Datensets gib es bei scikit-learn und im UC Irvine Machine Learning Repository. Dieses Datenset bringt scikit-learn selber mit.\nWir importieren eine Funktion zum Laden des Datensets und rufen dieses auf.\nfrom sklearn.datasets import load_digits digits = load_digits() Die Daten und Metadaten sind in einem sogenannten Bunch-Objekt organisiert\ntype(digits) Dieser Bunch hat folgende Attribute.\ndir(digits) Schauen wir uns mal die Beschreibung an\nprint(digits.DESCR) Die eigentlichen Daten sind in einem numpy-Array abgelegt.\ntype(digits.data) Schauen wir es uns mal an.\ndigits.data Schauen wir uns die Dimension der Matrix an - es handelt sich um eine zweidimentionsale Matrix mit 1797 Zeilen und 64 Spalten. Es sind 1797 Bilder (also weniger als die originalen 5620) und 64 Features (eine lineare Darstellung der 8x8 Felder-Farbintensitätswerte) .\ndigits.data.shape Das Target-Attribute ist ebenfalls ein numpy-array \u0026hellip;\ntype(digits.target) \u0026hellip; allerding mit nur einer Dimension.\ndigits.target.shape Jeder Wert entspricht der geschriebenen Nummer\ndigits.target Das Bunch-Objekt hat noch das Attribute target_names. Normalerweise wird jeder Zahl in targent hier ein Name zugeordnen. Da es sich aber tatsächlich um Ziffern von 0 - 9 handelt, ist das in diesem nicht nötig.\ndigits.target_names In diesem Datenset gibt es zusätzlich noch ein Attribute images. Es enthält für jede geschrieben Ziffer die Farbwerte in ein 8x8-Matrix.\nlen(digits.images) Schauen wir uns zum Beispiel das erste Bild an \u0026hellip;\ndigits.images[0] \u0026hellip; oder das zehnte Bild\ndigits.images[9] Wir können die in dieser Form gespeicherten Farbintensitäten auch mit matplotlib anzeigen lassen. Hier zum Beispiel für die ersten 30 Bilder (wenn man mehr haben möchte, muss man in subplot mehr als 3 Zeilen angeben).\nimport matplotlib.pyplot as plt %matplotlib inline fig, axes = plt.subplots(3, 10, figsize=(10, 5)) for ax, img in zip(axes.ravel(), digits.images): ax.imshow(img, cmap=plt.cm.gray_r) Um einen Klassifikator für ein Klassifikation zu trainieren und dann später seine Güte zu bewerten, wird das Datenset (genauer gesagt die Attribute data und target) in ein Trainingsset (75%) und Testset (25%) aufgeteilt. Die Konvention ist hier eine großes X für den Variablen der Datenmatrix und ein kleines y für den Target-Vektor zu nutzen.\nAnmerkung: Bei einigen der folgenden Schritte wird von zufälligen Zuständen ausgegagen. Um diese fest zu setzen und somit die Analyse reproduzierbar zu machen, kann man den Parameter random_state nutzen und mit einer Zahl versehen.\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( digits[\u0026#39;data\u0026#39;], digits[\u0026#39;target\u0026#39;], random_state=1) Die Maße der zweidimensionalen Trainigs-Daten-Matrix:\nX_train.shape Die Maße der zweidimensionalen Test-Daten-Matrix:\nX_test.shape Die Länge des Trainingsvektor entspricht der Anzahl an Zeilen der Trianingsmatrix.\ny_train.shape Die Länge des Testsvektors entspricht der Anzahl an Zeilen der Testsmatrix.\ny_test.shape Wir werden zuerst mit einem k-Nearest-Neighbor-Klassifizierer Arbeiten und laden dazu die Klasse \u0026hellip;\nfrom sklearn.neighbors import KNeighborsClassifier \u0026hellip; und erzeugen ein Objekt davon. Hierbei können wird die Anzahl an zu betrachteten Nachbarn angeben:\nknn_clf = KNeighborsClassifier(n_neighbors=1) Jetzt trainieren wir den Klassifikator mit den Trainingsdaten. Dafür wird in scikit-learn unabhängig von Klassifikator die Methode fit genutzt.\nknn_clf.fit(X_train, y_train) Herzlichen Glückwunsch - wir haben unser aller erstes Klassifikator-Modell gebaut und trainiert. Jetzt kann mit diesem neue Daten (also Vektoren der Länger 64, die die 8x8 Bilder darstellen) klassifizieren - in diesem Fall also um vorauszusagen, welche Ziffer dargestellt wurde.\nWir haben unsere Testdaten noch verfügbar und können die Methode predict des trainierten Klassifiers nutzen und erhalten die Voraussagen.\nknn_clf.predict(X_test) Da wir für das Testset wissen welche Ziffern tatsächlich herauskommen sollte, können wir die Methode score des Klassifiers nutzen. Diese führt die Voraussage durch und vergleicht sie mit den tatsächlichen Target-Werten. Am Ende bekommen wir einen Wert zwischen 0 (schlecht) und 1 (gut).\nknn_clf.score(X_test, y_test) Führen sie das gleich Verfahren mit einen k-Nearest-Neighbor-Klassifizierer selbstständig durch, der 3 Nachbar betrachtet (Code hier nicht angezeigt).\nDas schöne an scikit-learn ist, dass alle Klassifikatoren die gleichen Methoden besitzten. Sprich anderen Klassifikatoren nutzen auch fit, predict und score.\nMachen wir nun eine Klassifikation mit einem Random-Forest-Klassifikator ganz äquivalent zu der vorherigen Herangehensweise:\nfrom sklearn.ensemble import RandomForestClassifier random_forest_cfl = RandomForestClassifier(random_state=1) random_forest_cfl.fit(X_train, y_train) random_forest_cfl.score(X_test, y_test) Das gleiche machen wir nun für eine Klassifikation mit einem künstlichen, neuralen Netz (Multi-Layer-Perceptron). Standardmäßig hat das Netz ein eine Hidden-Layer mit 100 Nodes.\nfrom sklearn.neural_network import MLPClassifier mlpc = MLPClassifier(random_state=1) mlpc.fit(X_train, y_train) mlpc.score(X_test, y_test) Wir können die Anzahl an Hidden-Layer und Anzahl an Nodes in diesen als Parameter setzen (hier 3 Schichten mit mit 200, 100 und 20 Nodes). Man kann die Schritte kondenensiert schreiben, indem man die Methodenaufrufe direkt verknüpft.\nMLPClassifier(random_state=1, hidden_layer_sizes=(200, 100, 20)).fit( X_train, y_train).score(X_test, y_test) Es gibt noch viele weitere Klassifikatoren in scikit-learn. Für einen Einführung sollten dies 3 Bespiele aber reichen. Wir konnten hier aber sehen, wie einfach sklearn es uns auf Grund der kosistenten Methoden macht, verschiedene Klassifikationsmethode zu nutzen.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/basics/","title":"Grundbegriffe der Statistik und Einführung in Pandas","tags":[],"description":"","content":"19.02.2024 – 24.02.2024 Grundbegriffe der Statistik und Einführung in Pandas Diese Einheit gibt in einem ersten Teil eine Einführung in die Aufgaben und grundlegenden Begriffe der angewandten Statistik. Im zweiten (mehr praktischen) Teil wird das pandas Paket vorgestellt und gezeigt, wie Datensätze eingelesen und bearbeitet werden können. Manche Lektionen und Aufgaben sind als Exkurs markiert. Diese Aufgaben sind für Interessierte und dauern meist etwas länger oder sind schwieriger zu lösen.\nDie Projektaufgabe fasst die in dieser Einheit gelernten Inhalte in Form einer konkreten Aufgabenstellung zusammen, wie sie in der Praxis vorkommen könnte. Nach Bearbeitung aller einzelnen Aufgaben dieser Einheit ist die Projektaufgabe einfach zu bearbeiten.\nZiele Kenntnisse des statistischen Grundvokabulars und Anwendung auf die Beschreibung eines Datensatzes Ein- und Auslesen von Datensätzen als DataFrames in Python Filtern von DataFrames nach Spalten oder Zeilen Erstellung neuer Variablen Projektaufgabe: Grundlagen der Datenanalyse in Python Die Pressestelle der San Francisco Public Library möchte einen Online-Artikel zum Kundenstamm der Bibliothek erstellen. Dazu hat sie Dir einen Datensatz geschickt, den Du auswerten sollst.\nErstelle eine Beschreibung des Datensatzes unter Verwendung des statistischen Grundvokabulars. Lies den Datensatz ein. Bereinige den Datensatz von fehlenden Werten und berechne die neue Variable Membership Duration. Die bearbeitete Projektaufgabe kannst Du in Deinem Git-Repositorium als Notebook bzw. Markdown-Datei speichern.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/regression/","title":"Regression","tags":[],"description":"","content":"Bitte erstell ein neues Jupyter-Notebook und nennen esRegression.\nHier führen wir eine einfache Regression von zwei Features (also in zwei Dimensionen durch), da dies sich leichter visualisieren lässt. Tatsächlich ist man aber in der Anzahl an Features nicht eingeschränkt. Ziel ist es ein Regression-Model zu erstellen, in dem man einen numerischen Eingabe-Wert (x) eingibt und einen numerischen Ausgabe-Wert (y) erhält.\nWir erzeugen und ein künstlichen Datenset von 500 Datenpunkten mit Hilfe der Funktion make_regression. Mit dem Parameter noise können wir angeben, wie verauscht die Daten sein sollen.\nfrom sklearn.datasets import make_regression X_reg, y_reg = make_regression(n_samples=500, n_features=1, noise=20, random_state=1) Kurzer Blick auf die Daten:\nX_reg und die Dimensionen der Matrix:\nX_reg.shape und Länge des Ziel-Vektors:\ny_reg.shape Wir können die Daten zweidimensional plotten. Die x-Achse stellt die Eingangswerte dar, die y-Achse die davon abhängigen Werte. Mit X_reg[:, 0] wird die erste und einzige Spalte, der prinzipiel n-dimensionalen Input-Werte-Matrix genutzt.\nimport matplotlib.pyplot as plt %matplotlib inline plt.plot(X_reg[:, 0], y_reg, \u0026#34;.\u0026#34;) Wir möchten für diese Daten eine lineares Regression-Modell erstellen, laden dafür die nötige Klasse und erstellen eine Instanz davon. Noch einmal zur Erinnerung - ein lineare Modell wir durch folgende Formel beschrieben:\ny = w_1 * x_1 + w_2 * x_2 + \u0026hellip; + w_n * x_n + b\nBeim Fitten werden die w-Parameter und der b-Parameter mit Werten versehen.\nfrom sklearn.linear_model import LinearRegression linear_regression = LinearRegression() Wir teilen unsere Daten für eine spätere Bewertung der Güte in ein Trainings- und ein Testset auf.\nfrom sklearn.model_selection import train_test_split X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg) Das Fitting der Parameter erfolg mit der Methode fit.\nlinear_regression.fit(X_reg_train, y_reg_train) Wir können uns jetzt die Koeffizienten also in diesem nut Fall w_1 anschauen. Die Gerade hat also die Steigung 21.4.\nlinear_regression.coef_ Zudem gibt es noch den y-Achsen-Abschnit bei 0 (Intercept).\nlinear_regression.intercept_ Wir können nun die Trainingsdaten und die Gerade, die das lineare Modell repräsentiert, in einem Plot darstellen.\nplt.plot(X_reg_train, y_reg_train, \u0026#34;.\u0026#34;) y_lm_predicted = linear_regression.predict(X_reg_train) plt.plot(X_reg_train, y_lm_predicted, \u0026#34;.\u0026#34;) Dieses trainierte Modell kann nun auf neue Werte angewandt werden. Zum Beipiel unserer Test-Datenset:\nlinear_regression.predict(X_reg_test) Für das Testdatenset kennen wir die tatsächlichen y-Werte und können mit der Methode score eine Vorraussage durchführen und diese mit den Werten vergleichen um die Güte der Voraussage zu bewerten.\nlinear_regression.score(X_reg_test, y_reg_test) Das gleiche Vorgehene können wir nun mit anderen Regression-Methoden nutzen. Zum Beispiel mit der SVM-Methode\nfrom sklearn.svm import SVR svm_regression = SVR() svm_regression.fit(X_reg_train, y_reg_train) svm_regression.score(X_reg_test, y_reg_test) Auch hier gibt es wieder viele weitere Regressionsmethoden, wir belassen es aber bei diesen Beispielen.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/text_classification/","title":"Text-Klassifikation","tags":[],"description":"","content":"Text-Klassifikations-Beispiel Das Beispiel basiert auf einem offenen Datensat von Newsgroup-Nachtrichten und orientiert sich an diesem offiziellen Tutorial von scikit-learn zur Textanalyse.\nWir nutzen Dokumente von mehreren Newsgroups und trainieren damit einen Classifier, der dann eine Zudordnung von neuen Texten auf eine dieser Gruppen durchführen kann. Sprich die Newsgroups stellen die Klassen/Tags dar, mit denen wir neue Texte klassifizieren. Wie nutzen eine einfachen Bag-of-Word-Ansatz in dem wir (normalisierte) Häufigkeit von Wörtern als Features nutzen.\nIn diesem Fall liegen die Daten noch nicht als Teil von scikit-learn vor, es wird aber eine Funktion angeboten, mit die Daten bezogen werden können.\nfrom sklearn.datasets import fetch_20newsgroups Wir legen vier Newsgroups, die wir nutzen wollen, fest.\nselected_categories = [\u0026#34;sci.crypt\u0026#34;, \u0026#34;sci.electronics\u0026#34;, \u0026#34;sci.med\u0026#34;, \u0026#34;sci.space\u0026#34;] Wir beziehen die Trainingset- und Testsets-Dokumente.\nnewsgroup_posts_train = fetch_20newsgroups( data_home=\u0026#34;newsgroup_data\u0026#34;, subset=\u0026#39;train\u0026#39;, categories=selected_categories, shuffle=True, random_state=1) newsgroup_posts_test = fetch_20newsgroups( data_home=\u0026#34;newsgroup_data\u0026#34;, subset=\u0026#39;test\u0026#39;, categories=selected_categories, shuffle=True, random_state=1) Die Objekte, die wir erhalten, sind scikit-learn-Bunches \u0026hellip; type(newsgroup_posts_train) \u0026hellip; und haben die üblichen Attribute von Bunches. dir(newsgroup_posts_train) U.a. exitiert die übliche Beschreibung des Datensets im Attribute DESCR, die wir uns ansehen können.\nprint(newsgroup_posts_train.DESCR) Das Attribut data enthält in diesem Fall keine Matrix, sondern Newsgroup-Message-Texte. Ein Beispiel schauen wir uns an:\nprint(newsgroup_posts_train.data[6]) Die Targets sind die Newsgroup-Namen. Diese Klassen sind wie üblich für scikit-learn als Zahlen kodiert, die wir mittels target_names auflösen können.\nprint(newsgroup_posts_train.target_names) Für unsere Beispiel-Message: newsgroup_posts_train.target_names[newsgroup_posts_train.target[6]] Um die Wörter zu zählen, aber auch um Stopwörte zu entfernen und zu Tokenisieren nutzen wir ein Objekt der CountVectorizer-Klasse bzw. dessen fit-Methode\nfrom sklearn.feature_extraction.text import CountVectorizer count_vect = CountVectorizer() count_vect.fit(newsgroup_posts_train.data) Über alle Dokumente bekommen wir die folgende Zusammenstellung der Wörter und ihre Indices (positionen im Array): len(count_vect.get_feature_names_out()) Wir können uns ein paar Beispiele ansehen \u0026hellip; count_vect.get_feature_names_out()[10000:10050] \u0026hellip; oder sogar das counting-Dictionary mit den Wörtern und ihre Vorkommen-Anzahl betrachten (Achtung: groß!).\nprint(count_vect.vocabulary_) Diese Countings müssen wir für den Klassifikator in eine Matrix transformieren:\nX_train_counts = count_vect.transform(newsgroup_posts_train.data) Die Matrix, die wir erhalten, hat folgende Maße: X_train_counts.shape Wir normalisieren die Wörtercoutings auf die Anzahl an Wörter im Text (Term Frequency - TF). Dazu nutzen wir eine Objekt der Klasse TfidfTransformer (schalten die idf-Normalisierung (Inverse Document Frequency) dabei ab.)\nfrom sklearn.feature_extraction.text import TfidfTransformer tf_transformer = TfidfTransformer(use_idf=False) Die Normalisierung erfolgt mit den Methoden fit und transform. tf_transformer.fit(X_train_counts) X_train_tf = tf_transformer.transform(X_train_counts) Die Matrix, die wir erhalten, hat folgende Maße: X_train_tf.shape Jetzt können wir einen Klassifikator erstellen. Wir Nutzen hier eine Random-Forest-Klassifikator, könnten aber auch eine andere Methode wählen.\nfrom sklearn.ensemble import RandomForestClassifier tf_random_forest_classifier = RandomForestClassifier() Wie bei allen Supervised-Learning-Verfahren trainieren wir den Klassikator mit der Trainingsmatrix.\ntf_random_forest_classifier.fit(X_train_tf, newsgroup_posts_train.target) Um zu testen wie gut der Klassifikator funktioniert, prozessieren wir das Test-Set mit dem CountVectorizer-Objekt und führen die gleiche TF-Transformation durch.\nX_test_counts = count_vect.transform(newsgroup_posts_test.data) X_test_tf = tf_transformer.transform(X_test_counts) Ein kurze Blick auf die Maße der Matrix, zeigt uns, dass die Anzahl an Spalten (Features) gleich ist wie bei der Trainingsmatrix.\nX_test_counts.shape Jetzt können wir mit der score-Methods die Güte des Klassikators auf dem Test-Set prüfen.\ntf_random_forest_classifier.score(X_test_tf, newsgroup_posts_test.target) Der Klassifikator scheint gut genug zu funktionieren. Wir können jetzt Listen von Dokumenten klassifizieren. Wir nehmen zwei Dokumete aus unserem Test-Set und erstellen zusätzlich ein sehr kleines eigene Dokument, das nur aus einem Satz bestehent.\ndocs_to_classify = [ newsgroup_posts_test.data[1], newsgroup_posts_test.data[7], \u0026#34;The sun send a lot of radiation to the planets including earth\u0026#34;] Werfen wir einen kurzen Blick auf die zwei Dokumente aus dem Testset. print(newsgroup_posts_test.data[1]) print(newsgroup_posts_test.data[7]) Auch diese neu zu klassifizierenden Dokumente müssen wir wie die Traininsdokumente in Matrizen transformieren:\nX_to_classify_counts = count_vect.transform(docs_to_classify) X_to_classify_tfidf = tf_transformer.transform(X_to_classify_counts) Jetzt können wir mit dieser Matrix die Klassifikation durchführen \u0026hellip;\npredicted_classes = tf_random_forest_classifier.predict(X_to_classify_tfidf) \u0026hellip; und uns die Klassen anschauen, mit denen die Dokumente versehen wurden.\nfor predicted_class in predicted_classes: print(newsgroup_posts_train.target_names[predicted_class]) Um den Klassifikator zu verbessern, testen wir statt der Term-Frequenz nun die TFIDF (Term Frequency times Inverse Document Frequency) und erstellen damit unsere Matrizen.\ntfidf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts) Mach mit diesem TFIDF-Ansatz äquivalent zu der Klassifikation mit dem TF-Ansatz weiter. D.h. führe alle nötigen Schritte wie Training, Scoring und Prediction durch. Ist das Ergebnis besser? Gerne kannst Du zusätzlich mit anderen Klassifikator-Typen anstelle von Randeom Forest experiment werden (z.B. SVMs oder neuronale Netzen), um zu testen, ob dies zu einer besseren Klassifikation führt.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/descriptive_statistics/","title":"Deskriptive Statistik und Visualisierungen","tags":[],"description":"","content":"24.02.2024 – 04.03.2024 Deskriptive Statistik und Visualisierungen Diese Einheit gibt eine Einführung in die deskriptive Statistik mit pandas und zeigt, wie statistische Visualisierungen in Python erstellt werden können.\nVorausgesetzt wird statistisches Grundvokabular, wie in der letzten Einheit behandelt.\nZiele Berechnung und Interpretation von grundlegenden Lage- und Streuungsmaßen Beschreibung von univariaten stetigen und diskreten Verteilungen Beschreibung und Berechnung von Statistiken für stetige und diskrete bivariate Verteilungen Erstellung von einfachen Visualisierungen Projektaufgabe: Deskriptive Statistik und Visualisierungen Für den Online-Artikel zum Kundenstamm der Bibliothek braucht die Pressestelle einige interessanten Zahlen zum Thema Alter und Bibliotheksnutzung. Außerdem möchte sie die Daten in einer Info-Graphik zusammenstellen.\nFür eine erste Demo bist Du verantwortlich:\nBerechne 2-3 Statistiken und erstelle 2-3 Visualisierungen basierend auf den Informationen im Datensatz. Nutze pandas zur Berechnung der Statistiken und seaborn für die Visualisierungen. Lade bis spätestens 24.03.23 Deinen Report in Form eines Jupyter Notebooks in der Dateiablage in Moodle hoch.\nBeispielfragen, die Du mit dem Datensatz beantworten und visualisieren kannst:\nWie viele Senioren und Kinder sind Kunden der San Francisco Public Library? Wie viele Nutzer möchten per Mail informiert werden? Wie alt sind diese Nutzer durchschnittlich im Vergleich zu Nutzern, die per Post informiert werden möchten? Wie viele Ausleihen werden im Mittel pro Altersgruppe und pro Jahr getätigt? Ist die Streuung zwischen den Gruppen gleich? "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/dimension_reduction/","title":"Dimensionsreduktion","tags":[],"description":"","content":"Bitte erstelle ein neues Jupyter-Notebook und nennen es Dimensionsreduktion.\nWir laden das Datenset:\nfrom sklearn.datasets import load_digits digits = load_digits() Wir wollen mit einer Pricinple Component Analysis (PCA) die Dimensionreduktion durchführen. Hierzu laden wir zuerst die Klasse und erzeugen eine Instanz davon. Wir geben an, dass wir danach nur zwei Dimensionen (\u0026ldquo;components\u0026rdquo;) erhalten möchten. Dadurch lassen sie die Daten später auf in einem Plot mit zwei Achsen abbilden.\nfrom sklearn.decomposition import PCA pca = PCA(random_state=1, n_components=2) Wir können die Tranformation mit der Funktion fit_transform durchführen.\npca_result = pca.fit_transform(digits.data) Wir erhalten eine Matrix mit der gleichen Anzahl an Datenpunkte aber nur zwei Spalten.\npca_result.shape Wir können nun die beiden Spalten der resultierenden Matrix plotten.\nimport matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots() plt.scatter(pca_result[:, 0], pca_result[:, 1], c=digits.target, cmap=\u0026#34;Set1\u0026#34;) plt.colorbar() Wir können das gleich Vorgehen mit dem t-SNE-Verfahren durchführen\nfrom sklearn.manifold import TSNE tsne = TSNE(random_state=1, n_components=2) tsne_result = tsne.fit_transform(digits.data) Und stellen wir Ergebnis graphisch dar:\nfig, ax = plt.subplots() plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=digits.target, cmap=\u0026#34;Set1\u0026#34;) plt.colorbar() Vergleich die Ergebnisse beider Methode. Velches Verfahren zeigt eine höhere Trennschärfe?\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/inference/","title":"Exkurs: Inferenzstatistik","tags":[],"description":"","content":"Exkurs: Grundlagen der Inferenzstatistik mit dem Bootstrapping Verfahren Diese Einheit gibt einen ersten Einblick in die angewandte Inferenzstatistik mit dem Bootstrapping-Verfahren und stellt die Berechnung von Konfidenzintervallen für den Mittelwert und Median in Python vor. Die Inhalte und Aufgaben dieser Einheit sind anspruchsvoller als bisher und deswegen optional als Herausforderung für interessierte KursteilnehmerInnen konzipiert.\nZiele Durchführung eines Zwei-Stichproben Mittelwerttests Erkennung von signifikanten Unterschieden zwischen den Mittelwerten zweier Sub-Populationen. Projektaufgabe: Inferenzstatistik (optional) Unterscheidet sich das durchschnittliche Ausleihverhalten von jungen und älteren Bibliotheksnutzern signifikant voneinander?\nGehe zur Beantwortung der Fragestellung die folgenden Schritte durch:\nLies den Datensatz ein. Um eine homogene Stichprobe zu erhalten, filtere nach Bibliothekskunden die sich im Jahr 2010 registriert haben und auch noch im Jahr 2016 (als der Datensatz erstellt wurde) aktiv waren. Erstelle jeweils eine Series der Total Checkouts für zwei Sub-Populationen: Betrachte jeweils YOUNG ADULTs und SENIORs aus der Variable Patron Type Definition. Wie viele Beobachtungen sind jeweils in den beiden Populationen? Was sind deren Mittelwerte (Median, arithmetisches Mittel)? Was ist Deine Vermutung? Bestehen signifikante Unterschiede in den Mittelwerten dieser beiden Gruppen? Berechne das Konfidenzinterval für die Differenz der Mittelwerte/ Mediane mit dem Bootstrapping Verfahren! Setze das Signifikanznievau $\\alpha=0.01$. Wie interpretierst Du das Ergebnis? Unterscheiden sich die Mittelwerte signifikant voneinander? Wie Ändert sich das Ergebniss, wenn Du das Signifikanzniveau $\\alpha$ änderst? "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_basics/","title":"Machinelles Lernen - Grundlagen","tags":[],"description":"","content":"05.03.2024 – 10.03.2024 Einführung in das Maschinelle Lernen Ziele Von autonom fahrenden Autos bis automatische Verschlagwortung von Artikeln \u0026ndash; maschinelle Lernen findet immer weiteres Anwendung. In dieser Einheit werden die Grundideen des maschinellen Lernens vermittelt. Dabei versuchen wir die mathematischen Grundlagen weitestgehend auszuklammern und ein grundsätzliches Verständnis von Machine Learning zu entwickeln. Es werden in dem zugehörigen Video verschiedene Machine-Learning-Methoden präsentiert. Dabei wird nicht erwartet erwartet, dass Du die Details dieser Methode in Gänze verstehst. Einige dieser Methoden werden wir in der nächsten Wochen genauer betrachten und praktisch mittels Python sowie des Packetes scikit-learn anwenden und dabei vertiefen.\nReflexion \u0026ndash; Machine Learning in meinem Alltag Sieh Dir da das Video \u0026ldquo;Eine Einführung in das Maschinelle Lernen\u0026rdquo; an. Im Quiz wird das Verständnis einiger Punkte abgefragt. Mit diesem Hintergrundwissen überlege, an welcher Stelle Deines beruflichen aber auch privaten Alltages maschinelles Lernen auf Daten von Dir angewandt wird. Um welche Typen von Features handelt es sich? Welche maschinelle Verfahren werden möglicherweise angewandt? Mach Dir diese Überlegungen für Dich oder erstell in Deinem Git-Repositorium eine Markdown-Datei Beispiele_von_ML.md im Unterordner Module_3 und notiere Deine Vermutungen.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/quiz/","title":"Quiz","tags":[],"description":"","content":" "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/quiz/","title":"Quiz","tags":[],"description":"","content":" "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/solutions/","title":"Musterlösungen","tags":[],"description":"","content":"Hier die Jupyter Notebooks mit Musterlösungen (ablegen mit \u0026ldquo;Speichern unter\u0026rdquo;):\nKlassifikation Regression Dimensionsreduktion "},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/solutions/","title":"Musterlösungen","tags":[],"description":"","content":"Hier ist das Jupyter Notebooks mit der Text-Klassifikation-Musterlösung (ablegen mit \u0026ldquo;Speichern unter\u0026rdquo;).\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/machine_learning_methods/","title":"Machinelles Lernen - Praxis mit scikit-learn","tags":[],"description":"","content":"05.03.2024 – 10.03.2024 Machinelles Lernen - Praxis mit scikit-learn Zielsetzung Nachdem wir uns letzte Woche ein theoretisches Fundament zum maschinellen Lernen erarbeitet haben, möchten wir in dieser Woche praktisch mit der Python-Library scikit-learn maschinelle Lernverfahren anwenden. Auch hier ist das Ziel ein grundsätzliches Verständnis ohne viele Detail. Die Library scikit-learn biete ein sehr schönes konsistentes Interface zu verschiedenen Verfahren und Hauptziel ist es hierfür ein Gefühl zu erlangen. Die in der letzte Wochen angesprochenen Themen wie Feature-Skalierung und Cross-Validation werden zum Beispiel nicht durchführen.\nFür die Durchführung erstell bitte einen Ordner machine_learning in Deinem Modul-Ordner. In diesem soll für jedes der drei Themen (Klassifikation, Regression, Dimensionreduktion) ein Jupyter-Notebook erstellt werden. Die Klassifikation-Übung ist obligatorisch, Regression und Dimensionreduktion-Übungen sind fakultativ. Die Einreichung der Ergebnisse erfolgt über einen Commit bis zum Präsenztag. Musterlösungen sind hier zu finden.\nJetzt wünschen wir viel Spaß mit scikit-learn.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/text_analysis/","title":"Machinelles Lernen - Automatische Textanalyse","tags":[],"description":"","content":"12.03.2024 - 17.03.2024 Machinelles Lernen - Automatische Textanalyse Zielsetzung Final möchte wir mittels maschinellen Lernens Texte automatisch analysieren. Auch hier wollen wir an einem kleinen Beispiel das Verständnis für die Grundlagen und mögliche Anwendungen entwickelt. In dem Beispiel werden wir Texte in Kategorien zuordnen und somit ein kleines automatisches Verschlagwortungswerkzeug erstellen. Dazu bauen wir auf das in der letzte Woche Erlernte zu Klassifizierugsverfahren, aber auch auf einige Punkte aus Modul 2 namentlich Term-Frequency (TF) und und Term frequency inverse document frequency (TFIDF), auf. Für die Implementation greifen wir wieder auf scikit-learn zurück.\nBitte nutz den Ordner machine_learning in Deinem Modul-Ordner und erstelle darin einen ein Jupyter-Notebook names Text-Analyse. Die Einreichung der Ergebnisse erfolgt über einen Commit bis zum Präsenztag. Musterlösungen sind hier zu finden.\n"},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://zbmed.github.io/2023-2024-ZK_Data_Librarian_Modul_3/tags/","title":"Tags","tags":[],"description":"","content":""}]